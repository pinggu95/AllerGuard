{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‹í’ˆ ë¼ë²¨ ë¶„ì„ íŒŒì´í”„ë¼ì¸: OCR í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì•Œë ˆë¥´ê² ë¶„ì„\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ì´ë¯¸ì§€ í˜•íƒœì˜ ì‹í’ˆ ë¼ë²¨ì„ ë¶„ì„í•˜ëŠ” ë‘ ê°€ì§€ ì£¼ìš” íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.\n",
    "\n",
    "1.  **ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR â†’ .txt)**: ì´ë¯¸ì§€ì—ì„œ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ `.txt` íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "2.  **ì•Œë ˆë¥´ê² ë¶„ì„ (OCR â†’ LLM â†’ .json)**: ì´ë¯¸ì§€ì˜ í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œêµ­ ì‹ì•½ì²˜(MFDS) ê¸°ì¤€ ì•Œë ˆë¥´ê² ì •ë³´ë¥¼ ë¶„ì„í•˜ê³ , êµ¬ì¡°í™”ëœ `.json` íŒŒì¼ë¡œ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ì‚¬ì „ ì¤€ë¹„ (Prerequisites)\n",
    "\n",
    "ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì—¬ íŒŒì´í”„ë¼ì¸ì— í•„ìš”í•œ ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-vision\n",
      "  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 (from google-cloud-vision)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-cloud-vision)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 (from google-cloud-vision)\n",
      "  Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.3)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision)\n",
      "  Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision)\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2025.1.31)\n",
      "Downloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading protobuf-6.32.0-cp39-abi3-manylinux2014_x86_64.whl (322 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading grpcio-1.74.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m370.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.74.0-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-api-core, google-cloud-vision\n",
      "Successfully installed cachetools-5.5.2 google-api-core-2.25.1 google-auth-2.40.3 google-cloud-vision-3.10.2 googleapis-common-protos-1.70.0 grpcio-1.74.0 grpcio-status-1.74.0 proto-plus-1.26.1 protobuf-6.32.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.56.1-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading transformers-4.56.1-py3-none-any.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m398.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m275.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m187.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.9.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (798 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m799.0/799.0 kB\u001b[0m \u001b[31m311.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tokenizers-0.22.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m359.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading hf_xet-1.1.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m282.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, hf-xet, tiktoken, huggingface-hub, tokenizers, transformers, accelerate\n",
      "Successfully installed accelerate-1.10.1 hf-xet-1.1.9 huggingface-hub-0.34.4 regex-2025.9.1 safetensors-0.6.2 tiktoken-0.11.0 tokenizers-0.22.0 tqdm-4.67.1 transformers-4.56.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-vision\n",
    "!pip install transformers accelerate torch tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Cloud Platform (GCP) ì¸ì¦ ì„¤ì •\n",
    "\n",
    "GCP Vision APIë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼(`.json`)ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
    "ì•„ë˜ ì½”ë“œì—ì„œ `YOUR_SERVICE_ACCOUNT_KEY.json` ë¶€ë¶„ì„ ì‹¤ì œ í‚¤ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… GCP ì¸ì¦ ì„¤ì • ì™„ë£Œ: ocr-project-470906-7ffeebabeb09.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# ğŸš¨ ì¤‘ìš”: ì´ ë¶€ë¶„ì„ ì‹¤ì œ GCP ì„œë¹„ìŠ¤ ê³„ì • í‚¤ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.\n",
    "key_path = \"ocr-project-470906-7ffeebabeb09.json\"\n",
    "\n",
    "if os.path.exists(key_path):\n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key_path\n",
    "    print(f\"âœ… GCP ì¸ì¦ ì„¤ì • ì™„ë£Œ: {key_path}\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ê²½ê³ : GCP ì¸ì¦ í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”: {key_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë° ì „ì—­ ì„¤ì •\n",
    "\n",
    "íŒŒì´í”„ë¼ì¸ ì „ì²´ì—ì„œ ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ì£¼ìš” ì„¤ì •ê°’ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "import textwrap\n",
    "import argparse\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# Google Cloud Vision API\n",
    "from google.cloud import vision\n",
    "\n",
    "# Hugging Face Transformers (LLM)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# --- ì „ì—­ ì„¤ì • ---\n",
    "\n",
    "# ì‚¬ìš©í•  LLM ëª¨ë¸ ì´ë¦„ (VRAMì´ ë¶€ì¡±í•˜ë©´ ë” ì‘ì€ ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥)\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# í•œêµ­ ì‹ì•½ì²˜(MFDS) ê³ ì‹œ ì•Œë ˆë¥´ê² í‘œì¤€ ëª…ì¹­\n",
    "MFDS_CANON = [\n",
    "    \"ê³„ë€(ë‚œë¥˜)\",\"ìš°ìœ \",\"ë©”ë°€\",\"ë•…ì½©\",\"ëŒ€ë‘\",\"ë°€\",\"ê³ ë“±ì–´\",\"ê²Œ\",\"ìƒˆìš°\",\n",
    "    \"ë¼ì§€ê³ ê¸°\",\"ë³µìˆ­ì•„\",\"í† ë§ˆí† \",\"ì•„í™©ì‚°ë¥˜\",\"í˜¸ë‘\",\"ë‹­ê³ ê¸°\",\"ì‡ ê³ ê¸°\",\n",
    "    \"ì˜¤ì§•ì–´\",\"ì¡°ê°œë¥˜\",\"ì£\"\n",
    "]\n",
    "\n",
    "# LLM ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜ (ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œí•˜ê¸° ìœ„í•¨)\n",
    "_tokenizer = None\n",
    "_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. í•µì‹¬ ê¸°ëŠ¥: ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (GCP Vision OCR)\n",
    "\n",
    "ì´ í•¨ìˆ˜ëŠ” ë‘ íŒŒì´í”„ë¼ì¸ ëª¨ë‘ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ë°›ì•„ GCP Vision APIë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_by_gcp(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    ì´ë¯¸ì§€ íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°ì§€í•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ë¡œ.\n",
    "\n",
    "    Returns:\n",
    "        str: ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸. í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ì„ ë°˜í™˜.\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œê°€ ì˜ëª»ëœ ê²½ìš° ë°œìƒ.\n",
    "        RuntimeError: GCP API í˜¸ì¶œ ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš°.\n",
    "    \"\"\"\n",
    "    # 1. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸\n",
    "    if not os.path.exists(image_path):\n",
    "        raise FileNotFoundError(f\"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}\")\n",
    "\n",
    "    print(f\"'{os.path.basename(image_path)}' íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\")\n",
    "    \n",
    "    try:\n",
    "        # 2. GCP Vision í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "        client = vision.ImageAnnotatorClient()\n",
    "        \n",
    "        # 3. ì´ë¯¸ì§€ íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬(binary) ëª¨ë“œë¡œ ì½ê¸°\n",
    "        with io.open(image_path, \"rb\") as image_file:\n",
    "            content = image_file.read()\n",
    "\n",
    "        # 4. GCP APIê°€ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "        image = vision.Image(content=content)\n",
    "        \n",
    "        # 5. text_detection API í˜¸ì¶œ\n",
    "        response = client.text_detection(image=image)\n",
    "\n",
    "        # API ì‘ë‹µì— ì—ëŸ¬ ë©”ì‹œì§€ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if response.error.message:\n",
    "            raise RuntimeError(\n",
    "                f\"GCP OCR ì˜¤ë¥˜: {response.error.message}\\n\"\n",
    "                \"ìì„¸í•œ ë‚´ìš©: https://cloud.google.com/apis/design/errors\"\n",
    "            )\n",
    "\n",
    "        # 6. ê²°ê³¼ íŒŒì‹±\n",
    "        texts = response.text_annotations\n",
    "        if not texts:\n",
    "            print(\"ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "            return \"\"\n",
    "        \n",
    "        print(\"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.\")\n",
    "        # response.text_annotationsì˜ ì²« ë²ˆì§¸ ìš”ì†Œ(index 0)ì— ì „ì²´ í…ìŠ¤íŠ¸ê°€ í¬í•¨ë¨\n",
    "        return texts[0].description\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\")\n",
    "        print(\"Google Cloud ì¸ì¦(GOOGLE_APPLICATION_CREDENTIALS)ì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. íŒŒì´í”„ë¼ì¸ 1: ì•Œë ˆë¥´ê² ë¶„ì„ (OCR â†’ LLM â†’ JSON)\n",
    "\n",
    "í…ìŠ¤íŠ¸ ì¶”ì¶œ í›„, LLMì„ í†µí•´ ì•Œë ˆë¥´ê² ì •ë³´ë¥¼ ë¶„ì„í•˜ê³  JSONìœ¼ë¡œ ê²°ê³¼ë¥¼ ì •ì œí•˜ëŠ” ì „ì²´ ê³¼ì •ì…ë‹ˆë‹¤.\n",
    "\n",
    "### 4-1. LLM ë¡œë”© ë° í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "\n",
    "LLMì„ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ê³ , LLMì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    \"\"\"Hugging Face LLMê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì´ë¯¸ ë¡œë“œëœ ê²½ìš° ê±´ë„ˆëœë‹ˆë‹¤.\"\"\"\n",
    "    global _tokenizer, _model\n",
    "    if _model is None:\n",
    "        print(f\"'{MODEL_NAME}' ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...\")\n",
    "        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        _model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_NAME,\n",
    "            device_map=\"auto\",  # ì‚¬ìš© ê°€ëŠ¥í•œ GPU/CPUì— ìë™ìœ¼ë¡œ ëª¨ë¸ì„ í• ë‹¹\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=\"auto\", # ì‚¬ìš© ê°€ëŠ¥í•œ í•˜ë“œì›¨ì–´ì— ë§ì¶° ë°ì´í„° íƒ€ì… ìë™ ì„¤ì •\n",
    "        )\n",
    "        print(\"ëª¨ë¸ ë¡œë”© ì™„ë£Œ.\")\n",
    "\n",
    "# LLMì—ê²Œ ì—­í• ì„ ë¶€ì—¬í•˜ê³ , ì¶œë ¥ í˜•ì‹ì„ ì§€ì •í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "SYSTEM_PROMPT = textwrap.dedent(\"\"\"\n",
    "ë„ˆëŠ” 'í•œêµ­ ì‹í’ˆ ë¼ë²¨ ë¶„ì„ê°€'ì•¼. ì…ë ¥ì€ GCP OCRë¡œ ì¶”ì¶œí•œ ë¼ë²¨ í…ìŠ¤íŠ¸ë‹¤.\n",
    "ëª©í‘œ: í•œêµ­ ì‹ì•½ì²˜(MFDS) ì•Œë ˆë¥´ê¸° í‘œì‹œ ê¸°ì¤€ ì•ˆì—ì„œ í¬í•¨ ì—¬ë¶€ë¥¼ íŒë³„í•´ JSONë§Œ ì¶œë ¥í•œë‹¤.\n",
    "\n",
    "ê·œì¹™:\n",
    "- \"ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆ\", \"í•¨ìœ \" ë“± ëª…ì‹œ ë¼ì¸ì„ ìµœìš°ì„ ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤.\n",
    "- ëª…ì‹œ ë¼ì¸ì´ ì—†ìœ¼ë©´ ì›ì¬ë£Œëª…ì—ì„œ ë™ì˜ì–´ë¥¼ ê·¼ê±°ë¡œ ì¶”ë¡ í•œë‹¤.\n",
    "- ê²°ê³¼ëŠ” ë°˜ë“œì‹œ ì•„ë˜ ìŠ¤í‚¤ë§ˆì˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ë©°, ë‹¤ë¥¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ì•ŠëŠ”ë‹¤.\n",
    "- evidenceëŠ” ì›ë¬¸ì—ì„œ ì°¾ì€ 'ìµœì†Œí•œì˜ ë¬¸ìì—´'ë§Œ ì‚¬ìš©í•œë‹¤. ì˜ˆ: \"ìš°ìœ \", \"ì¹˜ì¦ˆë¶„ë§\".\n",
    "- \"â€¦ ëŒ€ë‘, ë°€, ìš°ìœ  â€¦ í•¨ìœ \" ë¬¸ì¥ì€ ê° ì•Œë ˆë¥´ê²ìœ¼ë¡œ ë¶„í• í•˜ì—¬ í•´ë‹¹ ë‹¨ì–´ë§Œ evidenceë¡œ ì“´ë‹¤.\n",
    "\n",
    "JSON ìŠ¤í‚¤ë§ˆ:\n",
    "{\n",
    "  \"allergens_found\": [\n",
    "    {\"canonical\": \"<MFDS í‘œì¤€ëª…>\", \"evidence\": [\"<ê·¼ê±°1>\", \"<ê·¼ê±°2>\"]}\n",
    "  ],\n",
    "  \"uncertain_mentions\": [\"<ì• ë§¤í•˜ê±°ë‚˜ êµì°¨ì˜¤ì—¼ ê°€ëŠ¥ì„± í‘œê¸°>\"],\n",
    "  \"notes\": \"<íŒë³„ ê·¼ê±° ìš”ì•½(í•œ ì¤„)>\"\n",
    "}\n",
    "\"\"\").strip()\n",
    "\n",
    "def build_user_prompt(ocr_text: str) -> str:\n",
    "    \"\"\"OCR í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ LLMì—ê²Œ ì „ë‹¬í•  ìµœì¢… ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return textwrap.dedent(f\"\"\"\n",
    "    ë‹¤ìŒì€ GCP Vision OCRë¡œ ì¶”ì¶œí•œ ì‹í’ˆ ë¼ë²¨ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.\n",
    "    ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì˜ ê·œì¹™ì„ ì ìš©í•´ JSONë§Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n",
    "\n",
    "    [OCR_TEXT_START]\n",
    "    {ocr_text.strip()}\n",
    "    [OCR_TEXT_END]\n",
    "    \"\"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. LLM ì¶”ë¡  ë° ê²°ê³¼ íŒŒì‹±\n",
    "\n",
    "LLMì„ ì‹¤í–‰í•˜ê³ , ëª¨ë¸ì´ ìƒì„±í•œ í…ìŠ¤íŠ¸ ì‘ë‹µì—ì„œ ê¹¨ë—í•œ JSON ë°ì´í„°ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_infer_to_json(ocr_text: str) -> Dict:\n",
    "    \"\"\"OCR í…ìŠ¤íŠ¸ë¡œ LLM ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³ , ê²°ê³¼ í…ìŠ¤íŠ¸ë¥¼ JSON(Dict)ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    load_llm() # ëª¨ë¸ì´ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸\n",
    "    \n",
    "    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ LLMì´ ì´í•´í•˜ëŠ” ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì¡°í•©\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": build_user_prompt(ocr_text)}\n",
    "    ]\n",
    "    prompt = _tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ì„œ(Tensor)ë¡œ ë³€í™˜\n",
    "    inputs = _tokenizer(prompt, return_tensors=\"pt\").to(_model.device)\n",
    "    \n",
    "    # LLM í…ìŠ¤íŠ¸ ìƒì„± ì‹¤í–‰\n",
    "    outputs = _model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,   # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
    "        do_sample=False,      # í•­ìƒ ê°€ì¥ í™•ë¥  ë†’ì€ ë‹¨ì–´ë§Œ ì„ íƒ (ê²°ê³¼ì˜ ì¼ê´€ì„± í™•ë³´)\n",
    "        temperature=0.0,      # ì°½ì˜ì„± 0 (ê²°ê³¼ì˜ ì¼ê´€ì„± í™•ë³´)\n",
    "        eos_token_id=_tokenizer.eos_token_id,\n",
    "    )\n",
    "    \n",
    "    # ìƒì„±ëœ í…ì„œë¥¼ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "    full_response = _tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # ëª¨ë¸ ì‘ë‹µ ë¶€ë¶„ë§Œ ì˜ë¼ë‚´ê¸°\n",
    "    assistant_response = full_response.split(\"assistant\\n\")[-1].strip()\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ì—ì„œ JSON ë¶€ë¶„ë§Œ ì•ˆì „í•˜ê²Œ íŒŒì‹±\n",
    "    return safe_json_parse(assistant_response)\n",
    "\n",
    "\n",
    "def safe_json_parse(model_text: str) -> Dict:\n",
    "    \"\"\"ëª¨ë¸ì´ ì¶œë ¥í•œ í…ìŠ¤íŠ¸ì—ì„œ JSON ë¸”ë¡ë§Œ ì¶”ì¶œí•´ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # ëª¨ë¸ì´ ```json ... ``` ì½”ë“œ ë¸”ë¡ìœ¼ë¡œ ì‘ë‹µí•˜ëŠ” ê²½ìš°ë¥¼ ë¨¼ì € ì²˜ë¦¬\n",
    "    match = re.search(r\"```json\\s*(\\{.*?\\})\\s*```\", model_text, re.DOTALL)\n",
    "    if match:\n",
    "        chunk = match.group(1)\n",
    "    else:\n",
    "        # ì½”ë“œ ë¸”ë¡ì´ ì—†ë‹¤ë©´, ê°€ì¥ ë°”ê¹¥ìª½ì˜ ì¤‘ê´„í˜¸({})ë¥¼ ê¸°ì¤€ìœ¼ë¡œ JSONì„ ì°¾ìŒ\n",
    "        start = model_text.find(\"{\")\n",
    "        end = model_text.rfind(\"}\")\n",
    "        if start != -1 and end != -1 and end > start:\n",
    "            chunk = model_text[start:end+1]\n",
    "        else:\n",
    "            raise ValueError(\"ëª¨ë¸ ì‘ë‹µì—ì„œ JSON ë¸”ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤:\\n\" + model_text)\n",
    "    \n",
    "    try:\n",
    "        # JSON ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜\n",
    "        return json.loads(chunk)\n",
    "    except json.JSONDecodeError:\n",
    "        # ë§ˆì§€ë§‰ í•­ëª© ë’¤ì— ì‰¼í‘œê°€ ë¶™ëŠ” ë“± í”í•œ JSON ì˜¤ë¥˜ë¥¼ ë³´ì •í•˜ì—¬ ì¬ì‹œë„\n",
    "        chunk = re.sub(r\",\\s*]\", \"]\", chunk)\n",
    "        chunk = re.sub(r\",\\s*}\", \"}\", chunk)\n",
    "        try:\n",
    "            return json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"JSON íŒŒì‹±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤ (ì˜¤ë¥˜: {e}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. ê²°ê³¼ ì •ì œ ë° í‘œì¤€í™” (Normalization)\n",
    "\n",
    "LLMì´ ìƒì„±í•œ JSONì„ MFDS í‘œì¤€ì— ë§ê²Œ ë‹¤ë“¬ê³ , 'evidence'(ì¦ê±°) í•­ëª©ì„ ë” ì •í™•í•˜ê²Œ ë³´ê°•í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_fix_evidence(model_json: Dict, ocr_text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    LLMì´ ìƒì„±í•œ JSONì„ MFDS í‘œì¤€ì— ë§ê²Œ ì •ê·œí™”í•˜ê³ ,\n",
    "    OCR ì›ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì¦ê±°(evidence)ë¥¼ ë³´ê°• ë° ìˆ˜ì •í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. í‘œì¤€ ëª…ì¹­ìœ¼ë¡œ ì •ê·œí™” (ì˜ˆ: 'ê³„ë€', 'ë‚œë¥˜' -> 'ê³„ë€(ë‚œë¥˜)')\n",
    "    normalized_allergens = []\n",
    "    seen = set()\n",
    "    for item in model_json.get(\"allergens_found\", []):\n",
    "        c = (item.get(\"canonical\") or \"\").strip()\n",
    "        if c in [\"ê³„ë€\", \"ë‚œë¥˜\"]: c = \"ê³„ë€(ë‚œë¥˜)\"\n",
    "        if \"ì¡°ê°œ\" in c: c = \"ì¡°ê°œë¥˜\"\n",
    "        \n",
    "        if c in MFDS_CANON:\n",
    "            # ì¤‘ë³µ ì œê±°\n",
    "            key = (c, tuple(sorted(item.get(\"evidence\", []))))\n",
    "            if key not in seen:\n",
    "                normalized_allergens.append(item)\n",
    "                seen.add(key)\n",
    "    \n",
    "    # 2. OCR ì›ë¬¸ì—ì„œ ë” ì •í™•í•œ ì¦ê±°(evidence) ì°¾ê¸°\n",
    "    \n",
    "    # '...í•¨ìœ ' ë¼ì¸ì—ì„œ ì•Œë ˆë¥´ê² ë‹¨ì–´ë“¤ ì¶”ì¶œ\n",
    "    contains_items = re.findall(r'([ê°€-í£]+)\\s*í•¨ìœ ', ocr_text)\n",
    "    \n",
    "    # ìµœì¢… ê²°ê³¼ë¬¼ êµ¬ì¡° ì´ˆê¸°í™”\n",
    "    final_result = {\n",
    "        \"allergens_found\": [],\n",
    "        \"uncertain_mentions\": model_json.get(\"uncertain_mentions\", []),\n",
    "        \"notes\": model_json.get(\"notes\", \"\")\n",
    "    }\n",
    "    \n",
    "    for item in normalized_allergens:\n",
    "        canonical_name = item['canonical']\n",
    "        evidence = set(item.get('evidence', []))\n",
    "        \n",
    "        # '...í•¨ìœ ' ë¼ì¸ì— í‘œì¤€ëª…ì´ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ ì¦ê±°ë¡œ ì¶”ê°€\n",
    "        if canonical_name in contains_items:\n",
    "            evidence.add(canonical_name)\n",
    "        \n",
    "        # ì›ì¬ë£Œëª… ì „ì²´ì—ì„œ í‘œì¤€ëª…ê³¼ ê´€ë ¨ëœ ë‹¨ì–´(ë™ì˜ì–´)ë“¤ì„ ì°¾ì•„ ì¦ê±°ë¡œ ì¶”ê°€\n",
    "        synonyms = {\"ìš°ìœ \": [\"ì¹˜ì¦ˆ\", \"ë²„í„°\", \"í¬ë¦¼\", \"ìœ ì²­\"], \"ë°€\": [\"ì†Œë§¥\"], \"ëŒ€ë‘\": [\"ê°„ì¥\", \"ëœì¥\", \"ë ˆì‹œí‹´\"]}\n",
    "        for syn in synonyms.get(canonical_name, []):\n",
    "            if syn in ocr_text:\n",
    "                evidence.add(syn)\n",
    "\n",
    "        if evidence:\n",
    "            final_result[\"allergens_found\"].append({\n",
    "                \"canonical\": canonical_name,\n",
    "                \"evidence\": sorted(list(evidence), key=len)\n",
    "            })\n",
    "    \n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-4. ì•Œë ˆë¥´ê² ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "\n",
    "ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ìˆœì„œëŒ€ë¡œ í˜¸ì¶œí•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_allergen_pipeline(image_path: str = None, ocr_text: str = None, save_path: str = \"allergen_result.json\") -> Dict:\n",
    "    \"\"\"ì•Œë ˆë¥´ê² ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì „ì²´ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # OCR í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì§€ ì•Šì•˜ë‹¤ë©´, ì´ë¯¸ì§€ ê²½ë¡œë¡œ OCR ì‹¤í–‰\n",
    "    if not ocr_text:\n",
    "        if not image_path:\n",
    "            raise ValueError(\"image_path ë˜ëŠ” ocr_text ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "        ocr_text = detect_text_by_gcp(image_path)\n",
    "        if not ocr_text:\n",
    "            print(\"OCR ê²°ê³¼, í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            return {}\n",
    "\n",
    "    # LLM ì¶”ë¡  ì‹¤í–‰\n",
    "    print(\"LLMì„ í†µí•´ ì•Œë ˆë¥´ê² ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤...\")\n",
    "    raw_json = llm_infer_to_json(ocr_text)\n",
    "\n",
    "    # ê²°ê³¼ ì •ì œ ë° í‘œì¤€í™”\n",
    "    print(\"ê²°ê³¼ë¥¼ í‘œì¤€í™”í•˜ê³  ì¦ê±°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...\")\n",
    "    final_result = normalize_and_fix_evidence(raw_json, ocr_text)\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "    # save_pathì˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ì„ ê²½ìš° ìƒì„±\n",
    "    output_dir = os.path.dirname(save_path)\n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_result, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… ì•Œë ˆë¥´ê² ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ë¥¼ '{save_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. íŒŒì´í”„ë¼ì¸ 2: ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (OCR â†’ TXT)\n",
    "\n",
    "ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ `.txt` íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ê°„ë‹¨í•œ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text_extraction_pipeline(image_path: str, save_path: str = None):\n",
    "    \"\"\"ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    # 1. OCR ì‹¤í–‰\n",
    "    extracted_text = detect_text_by_gcp(image_path)\n",
    "\n",
    "    # 2. ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°ì—ë§Œ íŒŒì¼ ì €ì¥\n",
    "    if extracted_text:\n",
    "        output_path = save_path\n",
    "        \n",
    "        # 3. ì €ì¥ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ì´ë¯¸ì§€ íŒŒì¼ëª…ìœ¼ë¡œ ìë™ ìƒì„±\n",
    "        if output_path is None:\n",
    "            # ì˜ˆ: 'sample.jpg' -> 'sample.txt'\n",
    "            base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "            output_path = f\"{base_name}.txt\"\n",
    "        \n",
    "        # 4. UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì €ì¥ (í•œê¸€ ê¹¨ì§ ë°©ì§€)\n",
    "        output_dir = os.path.dirname(output_path)\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            \n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(extracted_text)\n",
    "        print(f\"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ! ê²°ê³¼ë¥¼ '{output_path}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ë…¸íŠ¸ë¶ì—ì„œ ì§ì ‘ ì‹¤í–‰í•˜ê¸°\n",
    "\n",
    "ì´ì œ ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
    "ì•„ë˜ ì˜ˆì‹œ ì½”ë“œì˜ íŒŒì¼ ê²½ë¡œë¥¼ ì‹¤ì œ ë¶„ì„í•˜ê³  ì‹¶ì€ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½í•œ í›„ ì‹¤í–‰í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ì•Œë ˆë¥´ê² ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\n",
      "'ì°¨ì§€ì˜ˆ_009.jpg' íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.\n",
      "LLMì„ í†µí•´ ì•Œë ˆë¥´ê² ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤...\n",
      "'Qwen/Qwen2.5-7B-Instruct' ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤. ì ì‹œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06462a18aa1430297933bfd017be844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c94c42cdc54e14b566af0b71efc3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef090686762441ccadbea7f27808f1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b5857612fa4e1788be4de3c2ec4a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbeb4ab2abf242ee827af4d6a59f34db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63bb7dcbb1cb44c1a72c75e2ae72a10b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a3e806121164c2ab814f50466492e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16de3e91fff4ab5b27f3401e623d49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba23cffa29d14360a472f1f770be6977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433092559e6d4f47adc74ecc414960e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c98fd8f9fd041d4bb63998ca48246c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d922317befb94eaab98f89ec20a2920f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a269fed876a749d59dfbbf4c6c7829e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ë¡œë”© ì™„ë£Œ.\n",
      "ê²°ê³¼ë¥¼ í‘œì¤€í™”í•˜ê³  ì¦ê±°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...\n",
      "âœ… ì•Œë ˆë¥´ê² ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ë¥¼ 'my_allergen_result.json'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[ìµœì¢… ë¶„ì„ ê²°ê³¼]\n",
      "{\n",
      "  \"allergens_found\": [\n",
      "    {\n",
      "      \"canonical\": \"ëŒ€ë‘\",\n",
      "      \"evidence\": [\n",
      "        \"ê°„ì¥\",\n",
      "        \"ëŒ€ë‘\",\n",
      "        \"ëŒ€ë‘:ì™¸êµ­ì‚°\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"canonical\": \"ë°€\",\n",
      "      \"evidence\": [\n",
      "        \"ë°€\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"uncertain_mentions\": [],\n",
      "  \"notes\": \"ì›ì¬ë£Œëª…ì—ì„œ ëŒ€ë‘ì™€ ë°€ì´ í•¨ìœ ë¨ì„ í™•ì¸.\"\n",
      "}\n",
      "-----------------------------------\n",
      "\n",
      "--- ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\n",
      "'ì°¨ì§€ì˜ˆ_009.jpg' íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.\n",
      "âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ! ê²°ê³¼ë¥¼ 'my_ocr_text.txt'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- ì‹¤í–‰ ì˜ˆì‹œ ---\n",
    "\n",
    "# ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ (ğŸš¨ ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
    "my_image_file = \"ì°¨ì§€ì˜ˆ_009.jpg\"\n",
    "\n",
    "# --- ì˜ˆì‹œ 1: ì•Œë ˆë¥´ê² ë¶„ì„ ì‹¤í–‰ ---\n",
    "if os.path.exists(my_image_file):\n",
    "    print(\"--- ì•Œë ˆë¥´ê² ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\")\n",
    "    \n",
    "    allergen_result = run_allergen_pipeline(\n",
    "        image_path=my_image_file, \n",
    "        save_path=\"my_allergen_result.json\"\n",
    "    )\n",
    "    print(\"\\n[ìµœì¢… ë¶„ì„ ê²°ê³¼]\")\n",
    "    print(json.dumps(allergen_result, ensure_ascii=False, indent=2))\n",
    "    print(\"-\" * 35)\n",
    "else:\n",
    "    print(f\"'{my_image_file}' ê²½ë¡œì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. my_image_file ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# --- ì˜ˆì‹œ 2: ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰ ---\n",
    "if os.path.exists(my_image_file):\n",
    "    print(\"\\n--- ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\")\n",
    "    run_text_extraction_pipeline(\n",
    "        image_path=my_image_file, \n",
    "        save_path=\"my_ocr_text.txt\"\n",
    "    )\n",
    "    print(\"-\" * 35)\n",
    "else:\n",
    "    print(f\"'{my_image_file}' ê²½ë¡œì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. my_image_file ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ì•Œë ˆë¥´ê² ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\n",
      "'ê¹€ê´‘ë¬´_147.jpg' íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.\n",
      "LLMì„ í†µí•´ ì•Œë ˆë¥´ê² ì •ë³´ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤...\n",
      "ê²°ê³¼ë¥¼ í‘œì¤€í™”í•˜ê³  ì¦ê±°ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤...\n",
      "âœ… ì•Œë ˆë¥´ê² ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ë¥¼ 'my_allergen_result.json'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "[ìµœì¢… ë¶„ì„ ê²°ê³¼]\n",
      "{\n",
      "  \"allergens_found\": [\n",
      "    {\n",
      "      \"canonical\": \"ë°€\",\n",
      "      \"evidence\": [\n",
      "        \"ë°€\",\n",
      "        \"ë°€ í•¨ìœ \"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"canonical\": \"ëŒ€ë‘\",\n",
      "      \"evidence\": [\n",
      "        \"ëŒ€ë‘, ë°€, ìš°ìœ , ë‹­ê³ ê¸°, ì‡ ê³ ê¸°, ì¡°ê°œë¥˜(êµ´) í•¨ìœ \"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"canonical\": \"ìš°ìœ \",\n",
      "      \"evidence\": [\n",
      "        \"í¬ë¦¼\",\n",
      "        \"ë²„í„°\",\n",
      "        \"ì¹˜ì¦ˆ\",\n",
      "        \"ëŒ€ë‘, ë°€, ìš°ìœ , ë‹­ê³ ê¸°, ì‡ ê³ ê¸°, ì¡°ê°œë¥˜(êµ´) í•¨ìœ \"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"canonical\": \"ë‹­ê³ ê¸°\",\n",
      "      \"evidence\": [\n",
      "        \"ëŒ€ë‘, ë°€, ìš°ìœ , ë‹­ê³ ê¸°, ì‡ ê³ ê¸°, ì¡°ê°œë¥˜(êµ´) í•¨ìœ \"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"uncertain_mentions\": [],\n",
      "  \"notes\": \"ì›ì¬ë£Œëª…ì—ì„œ ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆì„ ì§ì ‘ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.\"\n",
      "}\n",
      "-----------------------------------\n",
      "\n",
      "--- ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\n",
      "'ê¹€ê´‘ë¬´_147.jpg' íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤...\n",
      "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ.\n",
      "âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ! ê²°ê³¼ë¥¼ 'my_ocr_text.txt'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- ì‹¤í–‰ ì˜ˆì‹œ ---\n",
    "\n",
    "# ë¶„ì„í•  ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ\n",
    "my_image_file = \"ê¹€ê´‘ë¬´_147.jpg\"\n",
    "\n",
    "# --- ì˜ˆì‹œ 1: ì•Œë ˆë¥´ê² ë¶„ì„ ì‹¤í–‰ ---\n",
    "if os.path.exists(my_image_file):\n",
    "    print(\"--- ì•Œë ˆë¥´ê² ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\")\n",
    "    \n",
    "    allergen_result = run_allergen_pipeline(\n",
    "        image_path=my_image_file, \n",
    "        save_path=\"my_allergen_result.json\"\n",
    "    )\n",
    "    print(\"\\n[ìµœì¢… ë¶„ì„ ê²°ê³¼]\")\n",
    "    print(json.dumps(allergen_result, ensure_ascii=False, indent=2))\n",
    "    print(\"-\" * 35)\n",
    "else:\n",
    "    print(f\"'{my_image_file}' ê²½ë¡œì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. my_image_file ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "\n",
    "# --- ì˜ˆì‹œ 2: ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤í–‰ ---\n",
    "if os.path.exists(my_image_file):\n",
    "    print(\"\\n--- ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---\")\n",
    "    run_text_extraction_pipeline(\n",
    "        image_path=my_image_file, \n",
    "        save_path=\"my_ocr_text.txt\"\n",
    "    )\n",
    "    print(\"-\" * 35)\n",
    "else:\n",
    "    print(f\"'{my_image_file}' ê²½ë¡œì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. my_image_file ë³€ìˆ˜ë¥¼ ìˆ˜ì •í•´ì£¼ì„¸ìš”.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
