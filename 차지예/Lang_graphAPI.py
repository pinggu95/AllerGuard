# -*- coding: utf-8 -*-

import os
import io
import json
import re
from typing import List, Set, TypedDict

import numpy as np
import torch
import pandas as pd

from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity  # (í˜¸í™˜ì„± ìœ ì§€ìš©; ì§ì ‘ dot ì‚¬ìš©)
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from transformers import logging as hf_logging

# GCP Vision OCR
from google.cloud import vision
from google.oauth2 import service_account

# (ì„ íƒ) Gemini Structured Output
try:
    import google.generativeai as genai
    _HAS_GEMINI = True
except Exception:
    _HAS_GEMINI = False

# (ì„ íƒ) Document AI
try:
    from google.cloud import documentai
    _HAS_DOCAI = True
except Exception:
    _HAS_DOCAI = False

# LangGraph
from langgraph.graph import StateGraph, END

print("--- ğŸš€ ì•Œë ˆë¥´ê¸° ë¶„ì„ ì„œë¹„ìŠ¤ (GCP Vision API + RAG + LLM Fallback) ì‹œì‘ ---")

# =====================
# 0. ì „ì—­ ì„¤ì •/ìƒìˆ˜
# =====================
ALLERGENS_STD_SET = set([
    "ì•Œë¥˜", "ìš°ìœ ", "ë©”ë°€", "ë•…ì½©", "ëŒ€ë‘", "ë°€", "ì£", "í˜¸ë‘",
    "ê²Œ", "ìƒˆìš°", "ì˜¤ì§•ì–´", "ê³ ë“±ì–´", "ì¡°ê°œë¥˜", "ë³µìˆ­ì•„", "í† ë§ˆí† ",
    "ë‹­ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì‡ ê³ ê¸°", "ì•„í™©ì‚°ë¥˜"
])
print(f"âœ… í‘œì¤€ ì•Œë ˆë¥´ê¸° ì¹´í…Œê³ ë¦¬ {len(ALLERGENS_STD_SET)}ê°œ ë¡œë“œ ì™„ë£Œ.")

IGNORE_KEYWORDS = set([
    "ì—´ëŸ‰", "íƒ„ìˆ˜í™”ë¬¼", "ë‹¨ë°±ì§ˆ", "ì§€ë°©", "ë‹¹ë¥˜", "ë‚˜íŠ¸ë¥¨", "ì½œë ˆìŠ¤í…Œë¡¤",
    "í¬í™”ì§€ë°©", "íŠ¸ëœìŠ¤ì§€ë°©", "ë‚´ìš©ëŸ‰", "I", "II"
])
print(f"âœ… ë¹„-ì„±ë¶„ í•„í„° í‚¤ì›Œë“œ {len(IGNORE_KEYWORDS)}ê°œ ë¡œë“œ ì™„ë£Œ.")

# ë™ì˜ì–´â†’í‘œì¤€ ë§¤í•‘
ALIAS2STD = {
    # ì•Œë¥˜(ë‚œë¥˜)
    "ë‚œë¥˜": "ì•Œë¥˜", "ê³„ë€": "ì•Œë¥˜", "ë‹¬ê±€": "ì•Œë¥˜", "ë‚œë°±": "ì•Œë¥˜", "ë‚œí™©": "ì•Œë¥˜",
    # ìš°ìœ  ê³„ì—´
    "ìœ ì²­": "ìš°ìœ ", "ìœ ì²­ë‹¨ë°±": "ìš°ìœ ", "ìœ ì²­ë‹¨ë°±ë¶„ë§": "ìš°ìœ ", "ì¹´ì œì¸": "ìš°ìœ ", "ì¹´ì œì¸ë‚˜íŠ¸ë¥¨": "ìš°ìœ ",
    "ì¹˜ì¦ˆ": "ìš°ìœ ", "ì¹˜ì¦ˆë¶„ë§": "ìš°ìœ ", "íƒˆì§€ë¶„ìœ ": "ìš°ìœ ", "ë¶„ìœ ": "ìš°ìœ ",
    # ëŒ€ë‘/ë°€/ê²¬ê³¼
    "ëŒ€ë‘ë ˆì‹œí‹´": "ëŒ€ë‘", "ë ˆì‹œí‹´(ëŒ€ë‘)": "ëŒ€ë‘", "ë°€ê°€ë£¨": "ë°€", "ë•…ì½©ë²„í„°": "ë•…ì½©",
    "í˜¸ë‘ë¶„íƒœ": "í˜¸ë‘", "ì£ê°€ë£¨": "ì£",
    # ìˆ˜ì‚°ë¬¼/ì¡°ê°œë¥˜
    "í™í•©": "ì¡°ê°œë¥˜", "êµ´": "ì¡°ê°œë¥˜", "ì „ë³µ": "ì¡°ê°œë¥˜",
    "ê³ ë“±ì–´ì¶”ì¶œë¬¼": "ê³ ë“±ì–´", "ìƒˆìš°ì¶”ì¶œë¬¼": "ìƒˆìš°", "ì˜¤ì§•ì–´ë¨¹ë¬¼": "ì˜¤ì§•ì–´",
    # ê³¼ì±„, ì²¨ê°€ë¬¼
    "ë³µìˆ­ì•„ë†ì¶•ì•¡": "ë³µìˆ­ì•„", "í† ë§ˆí† í˜ì´ìŠ¤íŠ¸": "í† ë§ˆí† ",
    "ì•„í™©ì‚°ë‚˜íŠ¸ë¥¨": "ì•„í™©ì‚°ë¥˜",
}

# ê²½ë¡œ/ëª¨ë¸ ì„¤ì •(í™˜ê²½ë³€ìˆ˜ ìš°ì„ , ì—†ìœ¼ë©´ ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ ìë™ ì„ íƒ)
KB_EMB_PATH = r"C:\\Users\\MYNOTE\\AllerGuard\\ì°¨ì§€ì˜ˆ\\kb_embeddings.npy"
KB_CAT_PATH = r"C:\\Users\\MYNOTE\\AllerGuard\\ì°¨ì§€ì˜ˆ\\kb_categories.json"
KB_CSV_PATH = r"C:\\Users\\MYNOTE\\AllerGuard\\domestic_allergy_rag_knowledge_1000.csv"


# GCP Vision Key
KEY_JSON_PATH = os.environ.get("GCP_VISION_KEY_PATH", r"D:\key folder\ocr-project-470906-7ffeebabeb09.json")

EMBEDDING_MODEL_NAME = "distiluse-base-multilingual-cased-v1"
NLI_MODEL_NAME = "joeddav/xlm-roberta-large-xnli"

# íŒŒì„œ ì„ íƒ: "gemini"(ê¸°ë³¸) ë˜ëŠ” "docai"
USE_API_PARSER = os.environ.get("ALLER_GUARD_API_PARSER", "gemini").lower()

# ì„ê³„ê°’
RAG_CONFIDENCE_THRESHOLD = float(os.environ.get("RAG_CONF_THRESH", 0.85))
NLI_FALLBACK_THRESHOLD   = float(os.environ.get("NLI_FALLBACK_THRESH", 0.5))

print(f"â„¹ï¸ RAG ì„ê³„ê°’={RAG_CONFIDENCE_THRESHOLD}, NLI ì„ê³„ê°’={NLI_FALLBACK_THRESHOLD}")
print(f"â„¹ï¸ API íŒŒì„œ ëª¨ë“œ: {USE_API_PARSER}")

# --- Gemini API í‚¤ íƒìƒ‰ ë„ìš°ë¯¸ ---
HARDCODED_GEMINI_API_KEY = "AIzaSyDMTVeVGPU374hlJWEGhxB902f-RxkRVSU"  # â— ë³´ì•ˆìƒ ë¹ˆ ë¬¸ìì—´ ìœ ì§€. í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ë„£ìœ¼ì„¸ìš”.

def _get_gemini_api_key():
    # 0) ì½”ë“œì— ì§ì ‘ ì…ë ¥ëœ í‚¤ (ê¶Œì¥X)
    if HARDCODED_GEMINI_API_KEY:
        return HARDCODED_GEMINI_API_KEY.strip()
    # 1) ëŒ€í‘œ í™˜ê²½ë³€ìˆ˜ ì´ë¦„ë“¤ ì‹œë„
    for var in ("GEMINI_API_KEY", "GOOGLE_API_KEY", "GENAI_API_KEY"):
        v = os.environ.get(var)
        if v:
            return v
    # 2) .env ì§€ì› (ì„ íƒ)
    try:
        from dotenv import load_dotenv  # type: ignore
        load_dotenv()
        for var in ("GEMINI_API_KEY", "GOOGLE_API_KEY", "GENAI_API_KEY"):
            v = os.environ.get(var)
            if v:
                return v
    except Exception:
        pass
    # 3) ë¡œì»¬ í‚¤ íŒŒì¼ (ì„ íƒ)
    for fname in ("gemini_api_key.txt", ".gemini_api_key"):
        if os.path.exists(fname):
            try:
                with open(fname, "r", encoding="utf-8") as f:
                    key = f.read().strip()
                    if key:
                        return key
            except Exception:
                pass
    return None

# =====================
# 1. ìœ í‹¸ (ì •ê·œí™”/ìš©ì–´-í•µì‹¬ ê°€ë“œ)
# =====================
GENERIC_SUFFIXES = (
    "ê°€ë£¨","ë¶„ë§","ì¶”ì¶œë¬¼","ë†ì¶•ì•¡","ë†ì¶•ë¶„ë§","ìœ ë˜","ë‹¨ë°±ì§ˆ","ë†ì¶•",
    "í˜ì´ìŠ¤íŠ¸","ì—‘ê¸°ìŠ¤","ë¶„íƒœ","ì‹œëŸ½","ì˜¤ì¼","í˜¼í•©","ì•¡","ë¶„","ì •ì œ","ê°€ìˆ˜ë¶„í•´ë¬¼"
)

def l2_normalize(x: np.ndarray) -> np.ndarray:
    return x / (np.linalg.norm(x, axis=-1, keepdims=True) + 1e-12)


def normalize_to_std(name: str) -> str:
    n = re.sub(r"\s+", "", str(name))
    n = n.split("(")[0]
    return ALIAS2STD.get(n, n)


def core_token(s: str) -> str:
    s = re.sub(r"\s+", "", str(s))
    s = s.split("(")[0]
    # ë’¤ì—ì„œë¶€í„° í•œ ë²ˆë§Œ ì œê±° (ê³¼ì‰ ì œê±° ë°©ì§€)
    for suf in GENERIC_SUFFIXES:
        if s.endswith(suf) and len(s) > len(suf) + 1:
            s = s[:-len(suf)]
            break
    return s


def lexical_consistent(query: str, cand_term: str) -> bool:
    q = core_token(query)
    c = core_token(cand_term)
    if not q or not c:
        return False
    if q == c:
        return True
    # 2ê¸€ì ì´ìƒ í•µì‹¬ì–´ì˜ í¬í•¨ ê´€ê³„ë©´ ìœ ì‚¬í•˜ë‹¤ê³  ê°„ì£¼
    if len(q) >= 2 and len(c) >= 2 and (q in c or c in q):
        return True
    return False

# =====================
# 2. ê¸€ë¡œë²Œ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”
# =====================
try:
    print(f"'{EMBEDDING_MODEL_NAME}' ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print("âœ… ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    print("Zero-Shot NLI ëª¨ë¸ ë¡œë“œ ì¤‘ (Fallback ì „ìš©)...")
    hf_logging.set_verbosity_error()
    try:
        import sentencepiece  # noqa: F401
    except Exception:
        print("âš ï¸ 'sentencepiece' íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. 'pip install sentencepiece' ê¶Œì¥(ë©€í‹°ë§êµ¬ì–¼ ëª¨ë¸ì— í•„ìš”)")

    # ì•ˆì „í•œ NLI ë¡œë”(ìˆœì°¨ í´ë°±)
    candidates = [
        ("joeddav/xlm-roberta-large-xnli", False),
        ("MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7", False),
        ("facebook/bart-large-mnli", True),  # ì˜ì–´ ì „ìš©(ê¸´ê¸‰ í´ë°±)
    ]
    last_err = None
    nli_pipeline = None
    for mid, english_only in candidates:
        try:
            nli_tokenizer = AutoTokenizer.from_pretrained(mid, use_fast=False)
            nli_model = AutoModelForSequenceClassification.from_pretrained(mid)
            nli_pipeline = pipeline(
                "zero-shot-classification",
                model=nli_model,
                tokenizer=nli_tokenizer,
                device=(0 if torch.cuda.is_available() else -1),
                hypothesis_template=(
                    "ì´ ì„±ë¶„ì€ {} ì•Œë ˆë¥´ê²(ê³¼)ì— í•´ë‹¹í•œë‹¤." if not english_only else "This ingredient belongs to {} allergen."
                ),
            )
            NLI_MODEL_NAME = mid
            print(f"âœ… NLI ëª¨ë¸ ë¡œë“œ: {mid}")
            break
        except Exception as e:
            print(f"âš ï¸ NLI í›„ë³´ ë¡œë“œ ì‹¤íŒ¨({mid}): {e}")
            last_err = e
    if nli_pipeline is None:
        raise RuntimeError(f"NLI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨(ëª¨ë“  í›„ë³´ ì‹¤íŒ¨): {last_err}")

    # NLI í›„ë³´ ë ˆì´ë¸”
    ALLERGEN_CANDIDATES = list(ALLERGENS_STD_SET) + ["ê´€ë ¨ ì—†ìŒ"]

    # GCP Vision í´ë¼ì´ì–¸íŠ¸
    print("GCP Vision API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
    credentials = service_account.Credentials.from_service_account_file(KEY_JSON_PATH)
    vision_client = vision.ImageAnnotatorClient(credentials=credentials)
    print("âœ… GCP Vision í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ.")

    # KB ë¡œë“œ + L2 ì •ê·œí™” + ì¤‘ë³µ ì œê±° + í…ìŠ¤íŠ¸/ìš©ì–´ ë§¤í•‘
    print("ì‚¬ì „ ê³„ì‚°ëœ RAG ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...")

    if not os.path.exists(KB_EMB_PATH) or not os.path.exists(KB_CAT_PATH):
        raise FileNotFoundError(f"KB íŒŒì¼ ëˆ„ë½: {KB_EMB_PATH} ë˜ëŠ” {KB_CAT_PATH}")

    kb_embeddings = np.load(KB_EMB_PATH).astype(np.float32)
    kb_embeddings = kb_embeddings / (np.linalg.norm(kb_embeddings, axis=1, keepdims=True) + 1e-12)

    with open(KB_CAT_PATH, "r", encoding="utf-8") as f:
        kb_categories = json.load(f)  # ê¸¸ì´ N

    # KB terms/texts í™•ë³´ (ê°€ëŠ¥í•˜ë©´ CSVì—ì„œ)
    kb_terms, kb_texts = None, None
    if os.path.exists(KB_CSV_PATH):
        df_kb = pd.read_csv(KB_CSV_PATH)
        term_col = "term" if "term" in df_kb.columns else df_kb.columns[0]
        kb_terms = df_kb[term_col].astype(str).tolist()
        if "description" in df_kb.columns:
            kb_texts = (df_kb[term_col].astype(str) + " | " + df_kb["description"].astype(str)).tolist()
        else:
            kb_texts = kb_terms[:]
    else:
        kb_terms = [f"item_{i}" for i in range(len(kb_categories))]
        kb_texts = [str(c) for c in kb_categories]

    # ì„ë² ë”© ì¤‘ë³µ ì œê±° (í•´ì‹œ ê¸°ë°˜) â†’ ê²€ìƒ‰ ì™œê³¡ ë°©ì§€
    def _dedup_embs(embs: np.ndarray, terms: list, cats: list, texts: list):
        import hashlib
        seen, keep = {}, []
        arr = np.ascontiguousarray(embs)
        for i, row in enumerate(arr):
            h = hashlib.sha256(row.view(np.uint8)).hexdigest()
            if h not in seen:
                seen[h] = True
                keep.append(i)
        return arr[keep], [terms[i] for i in keep], [cats[i] for i in keep], [texts[i] for i in keep]

    kb_embeddings, kb_terms, kb_categories, kb_texts = _dedup_embs(
        kb_embeddings, kb_terms, kb_categories, kb_texts
    )

    print(f"âœ… KB ë¡œë“œ ì™„ë£Œ (í•­ëª©: {len(kb_categories)}ê°œ, terms:{len(kb_terms)}ê°œ)")

except Exception as e:
    print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ê¸€ë¡œë²Œ ì„¤ì • ì‹¤íŒ¨: {e}")
    raise

# =====================
# 3. ìƒíƒœ ë° ë…¸ë“œ íƒ€ì…
# =====================
class AllergyGraphState(TypedDict):
    image_path: str
    raw_ocr_text: str
    ingredients_to_check: List[str]
    current_ingredient: str
    rag_result: dict
    final_allergens: Set[str]
    final_output_json: str

# =====================
# 4. ë…¸ë“œ êµ¬í˜„
# =====================
# --- Node 1: OCR ---

def call_gcp_vision_api(state: AllergyGraphState) -> AllergyGraphState:
    print("\n--- (Node 1: call_gcp_vision_api) ---")
    img_path = state.get("image_path", "")
    print(f"GCP Vision OCR í˜¸ì¶œ... (ì´ë¯¸ì§€: {img_path})")
    if not img_path or not os.path.exists(img_path):
        print("âš ï¸ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return {**state, "raw_ocr_text": ""}
    try:
        with io.open(img_path, 'rb') as image_file:
            content = image_file.read()
        image = vision.Image(content=content)
        response = vision_client.text_detection(image=image)
        if response.error.message:
            raise RuntimeError(f"GCP API Error: {response.error.message}")
        raw_text = response.full_text_annotation.text
        print(f"âœ… OCR ì„±ê³µ. í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)}")
        return {**state, "raw_ocr_text": raw_text}
    except Exception as e:
        print(f"âŒ GCP Vision ì‹¤íŒ¨: {e}")
        return {**state, "raw_ocr_text": ""}


# --- API íŒŒì„œ A: Gemini Structured Output ---

def parse_with_gemini_structured(state: AllergyGraphState) -> AllergyGraphState:
    raw_text = state.get("raw_ocr_text", "")
    if not raw_text.strip():
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    if not _HAS_GEMINI:
        print("âš ï¸ google-generativeai ë¯¸ì„¤ì¹˜. ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    api_key = _get_gemini_api_key()
    if not api_key:
        print("âš ï¸ Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”. ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-1.5-flash")

    schema = {
        "type": "object",
        "properties": {
            "ingredients_block": {"type": "string"},
            "ingredients_list":  {"type": "array", "items": {"type": "string"}},
            "contains_list":     {"type": "array", "items": {"type": "string"}},
            "cross_contamination_lines": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["ingredients_block", "ingredients_list", "contains_list", "cross_contamination_lines"]
    }

    prompt = f"""
[ì—­í• ] ë„ˆëŠ” í•œêµ­ ì‹í’ˆí‘œì‹œ ì „ë¬¸ ê°ë¦¬ì›.
[ëª©í‘œ] ì•„ë˜ OCR ì›ë¬¸ì—ì„œë§Œ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜.

[ì§€ì‹œ]
- 'ì›ì¬ë£Œëª…' ë¸”ë¡ì„ í•œ ë©ì–´ë¦¬ ë¬¸ìì—´ë¡œ ê·¸ëŒ€ë¡œ ingredients_blockì— ë„£ì–´ë¼.
- ì‰¼í‘œ/êµ¬ë‘ì  ê¸°ì¤€ìœ¼ë¡œ ì¬ë£Œë¥¼ í† í°í™”í•œ ëª©ë¡ì„ ingredients_listì— ë„£ì–´ë¼.
- 'ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆ', '...í•¨ìœ ', '...í¬í•¨' ë“± í‘œì‹œ ë¼ì¸ì— ë“±ì¥í•˜ëŠ” í•­ëª©ë“¤ì„ contains_listì— ë„£ì–´ë¼.
- 'ê°™ì€ ì œì¡°ì‹œì„¤/êµì°¨ì˜¤ì—¼/í˜¼ì… ê°€ëŠ¥' ë“± ë¬¸ì¥ì„ cross_contamination_linesì— ì›ë¬¸ ê·¸ëŒ€ë¡œ ë„£ì–´ë¼.
- ì›ë¬¸ì— ì—†ìœ¼ë©´ ë¹ˆ ê°’/ë¹ˆ ë°°ì—´ì„ ë„£ì–´ë¼. ì¶”ì¸¡ ê¸ˆì§€.

[OCR ì›ë¬¸]
```text
{raw_text}
```
"""
    try:
        resp = model.generate_content(
            prompt,
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": schema,
                "temperature": 0,
            },
        )
        data = json.loads(resp.text)
    except Exception as e:
        print(f"âŒ Gemini íŒŒì„œ ì˜¤ë¥˜: {e}")
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    def _clean(x: str) -> str:
        x = re.sub(r"\s+", "", x)
        x = x.split("(")[0]
        return normalize_to_std(x)

    ing_list   = [_clean(s) for s in data.get("ingredients_list", []) if s]
    contain_ls = [_clean(s) for s in data.get("contains_list", []) if s]

    filtered_ing = [i for i in ing_list if i and not any(i.startswith(k) for k in IGNORE_KEYWORDS)]
    filtered_con = [c for c in contain_ls if c and not any(c.startswith(k) for k in IGNORE_KEYWORDS)]

    found = set([s for s in filtered_con if s in ALLERGENS_STD_SET])
    queue = sorted(set([*filtered_ing, *filtered_con]))

    print(f"âœ… Gemini íŒŒì‹± ì™„ë£Œ: queue={len(queue)} / pre_found={sorted(found)}")
    return {**state, "ingredients_to_check": queue, "final_allergens": found}


# --- API íŒŒì„œ B: Document AI Custom Extractor ---

def parse_with_docai(state: AllergyGraphState,
                     project_id: str,
                     location: str,
                     processor_id: str) -> AllergyGraphState:
    if not _HAS_DOCAI:
        print("âš ï¸ google-cloud-documentai ë¯¸ì„¤ì¹˜. ë¹ˆ ê²°ê³¼ ë°˜í™˜")
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    img_path = state.get("image_path", "")
    try:
        client = documentai.DocumentProcessorServiceClient()
        name = client.processor_path(project=project_id, location=location, processor=processor_id)
        with open(img_path, "rb") as f:
            raw_doc = documentai.RawDocument(content=f.read(), mime_type="image/jpeg")
        req = documentai.ProcessRequest(name=name, raw_document=raw_doc)
        result = client.process_document(request=req)
        doc = result.document
    except Exception as e:
        print(f"âŒ Document AI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    ingredients_block = ""
    ingredients_list, contains_list, cross_lines = [], [], []

    for ent in doc.entities:
        t = ent.type_
        val = (ent.mention_text or "").strip()
        if   t == "ingredients_block":        ingredients_block = val
        elif t == "ingredients_item":         ingredients_list.append(val)
        elif t == "allergens_contains_item":  contains_list.append(val)
        elif t == "cross_contamination_line": cross_lines.append(val)

    def _clean(x: str) -> str:
        x = re.sub(r"\s+", "", x)
        x = x.split("(")[0]
        return normalize_to_std(x)

    ing_list   = [_clean(s) for s in ingredients_list if s]
    contain_ls = [_clean(s) for s in contains_list if s]

    filtered_ing = [i for i in ing_list if i and not any(i.startswith(k) for k in IGNORE_KEYWORDS)]
    filtered_con = [c for c in contain_ls if c and not any(c.startswith(k) for k in IGNORE_KEYWORDS)]

    found = set([s for s in filtered_con if s in ALLERGENS_STD_SET])
    queue = sorted(set([*filtered_ing, *filtered_con]))

    print(f"âœ… Document AI íŒŒì‹± ì™„ë£Œ: queue={len(queue)} / pre_found={sorted(found)}")
    return {**state, "ingredients_to_check": queue, "final_allergens": found}


# --- Node 2: API íŒŒì„œ ë¼ìš°í„° ---

def parse_text_via_api(state: AllergyGraphState) -> AllergyGraphState:
    print("\n--- (Node 2: parse_text_via_api) [API Parser] ---")
    if USE_API_PARSER == "docai":
        project_id = os.environ.get("DOCAI_PROJECT", "YOUR_GCP_PROJECT")
        location   = os.environ.get("DOCAI_LOCATION", "asia-northeast1")
        processor  = os.environ.get("DOCAI_PROCESSOR_ID", "your-processor-id")
        return parse_with_docai(state, project_id, location, processor)
    else:
        return parse_with_gemini_structured(state)


# --- Node 3: ë£¨í”„ ì»¨íŠ¸ë¡¤ëŸ¬ ---

def prepare_next_ingredient(state: AllergyGraphState) -> AllergyGraphState:
    print("\n--- (Node 3: prepare_next_ingredient) ---")
    queue = list(state.get("ingredients_to_check", []))
    if not queue:
        print("â„¹ï¸ ë‚¨ì€ í•­ëª© ì—†ìŒ")
        return state
    nxt = queue.pop(0)
    print(f"ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: '{nxt}' (ë‚¨ì€ {len(queue)}ê°œ)")
    return {**state, "current_ingredient": nxt, "ingredients_to_check": queue}


# --- RAG ì•ˆì „ ê²€ìƒ‰ (top-k + ê°€ë“œë£°) ---

def rag_search_topk(query_text: str, k: int = 5, thresh: float = 0.65):
    # 0) ë™ì˜ì–´â†’í‘œì¤€: ì§ˆì˜ ìì²´ê°€ í‘œì¤€ ì•Œë ˆë¥´ê²ì´ë©´ ë°”ë¡œ í™•ì •
    std = normalize_to_std(query_text)
    if std in ALLERGENS_STD_SET:
        return [{"term": std, "category": std, "text": std, "sim": 1.0, "found_by": "alias"}]

    # 1) ì¿¼ë¦¬ ì„ë² ë”©ì€ í•­ìƒ ìƒˆë¡œ ê³„ì‚° (ë¶€ë¶„ ì¼ì¹˜ ìºì‹œ ê¸ˆì§€)
    q = embedding_model.encode([query_text], normalize_embeddings=True)
    q = np.asarray(q, dtype=np.float32)[0]

    # 2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì •ê·œí™” ê°€ì •)
    sims = kb_embeddings @ q  # (N,)

    # 3) top-k
    k = max(1, min(k, len(sims)))
    top_idx = np.argpartition(-sims, kth=k-1)[:k]
    top_idx = top_idx[np.argsort(-sims[top_idx])]

    results = []
    for i in top_idx:
        results.append({
            "term": kb_terms[i],
            "category": kb_categories[i],
            "text": kb_texts[i],
            "sim": float(sims[i]),
            "found_by": "rag"
        })

    if not results:
        return [{"term": None, "category": "ì—†ìŒ", "text": "", "sim": 0.0, "found_by": "none"}]

    # 4) ê·¹ë‹¨ê°’ ë³´ì •: 0.99 ì´ìƒì¸ë°ë„ ìš©ì–´ê°€ ë‹¤ë¥´ë©´ ì‚´ì§ ê°•ë“±
    r0 = results[0]
    if r0["sim"] >= 0.99:
        if normalize_to_std(r0["term"]) != std and r0["term"] != query_text:
            r0["sim"] = r0["sim"] - 0.05
            results = sorted(results, key=lambda x: -x["sim"])
            r0 = results[0]

    # 5) **ìš©ì–´ ì¼ì¹˜ì„± ê°€ë“œ**: í•µì‹¬ì–´ê°€ ë‹¤ë¥´ë©´ 'ì—†ìŒ'ìœ¼ë¡œ ì°¨ë‹¨
    if not lexical_consistent(query_text, r0["term"]):
        return [{"term": None, "category": "ì—†ìŒ", "text": "", "sim": float(r0["sim"]), "found_by": "lex_guard"}]

    # 6) ì„ê³„ì¹˜ ë¯¸ë‹¬ì´ë©´ 'ì—†ìŒ'
    if r0["sim"] < thresh:
        return [{"term": None, "category": "ì—†ìŒ", "text": "", "sim": float(r0["sim"]), "found_by": "below_thresh"}]

    return results[:k]


# --- Node 4: RAG ê²€ìƒ‰ ---

def rag_search(state: AllergyGraphState) -> AllergyGraphState:
    print("--- (Node 4: rag_search) ---")
    ingredient = state.get("current_ingredient", "")

    cand_list = rag_search_topk(ingredient, k=5, thresh=0.65)
    top = cand_list[0]

    found = top["category"]
    conf  = float(top["sim"])
    by    = top.get("found_by")
    print(f"RAG ê²€ìƒ‰: '{ingredient}' â†’ '{found}' (ìœ ì‚¬ë„ {conf:.4f}, by={by})")

    return {**state, "rag_result": {"confidence": conf, "found_allergen": found}}


# --- Node 5: LLM Fallback (Zero-Shot) ---

def llm_fallback(state: AllergyGraphState) -> AllergyGraphState:
    print("--- (Node 5: llm_fallback) [NLI Zero-Shot] ---")
    ingredient = state.get("current_ingredient", "")
    try:
        resp = nli_pipeline(ingredient, list(ALLERGENS_STD_SET) + ["ê´€ë ¨ ì—†ìŒ"])
        top_label, top_score = resp['labels'][0], float(resp['scores'][0])
        print(f"NLI ì‘ë‹µ: Label='{top_label}', Score={top_score:.4f}")
        if top_label in ALLERGENS_STD_SET and top_score >= NLI_FALLBACK_THRESHOLD:
            return {**state, "rag_result": {"confidence": top_score, "found_allergen": top_label}}
        return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì—†ìŒ"}}
    except Exception as e:
        print(f"âŒ NLI Fallback ì˜¤ë¥˜: {e}")
        return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì˜¤ë¥˜"}}


# --- Node 6: ê²°ê³¼ ì·¨í•© ---

def update_final_list(state: AllergyGraphState) -> AllergyGraphState:
    print("--- (Node 6: update_final_list) ---")
    result_allergen = state.get("rag_result", {}).get("found_allergen", "")
    if result_allergen in ALLERGENS_STD_SET:
        s = set(state.get("final_allergens", set()))
        s.add(result_allergen)
        print(f"âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: '{result_allergen}' â†’ {sorted(s)}")
        return {**state, "final_allergens": s}
    print(f"â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': '{result_allergen}' (ë¬´ì‹œ)")
    return state


# --- Node 7: ì¢…ë£Œ ---

def finalize_processing(state: AllergyGraphState) -> AllergyGraphState:
    print("\n--- (Node 7: finalize_processing) ---")
    final_set = set(state.get("final_allergens", set()))
    final_list = sorted(list(final_set))
    final_json = json.dumps(final_list, ensure_ascii=False)
    print(f"ğŸ‰ ìµœì¢… ê²°ê³¼: {final_json}")
    return {**state, "final_output_json": final_json}


# =====================
# 5. ì—£ì§€(Edge) ë¼ìš°í„°
# =====================

def route_after_parse(state: AllergyGraphState) -> str:
    if state.get("ingredients_to_check"):
        return "has_ingredients"
    return "no_ingredients"


def route_rag_result(state: AllergyGraphState) -> str:
    conf = state.get("rag_result", {}).get("confidence", 0.0)
    allergen = state.get("rag_result", {}).get("found_allergen", "")

    # 'ì—†ìŒ'ì´ë©´ í´ë°± ë¶ˆí•„ìš” â†’ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ(ì¶”ê°€ ì•ˆ ë˜ê³  ë„˜ì–´ê°)
    if allergen == "ì—†ìŒ":
        print("  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëµ)")
        return "rag_success"

    if conf >= RAG_CONFIDENCE_THRESHOLD and allergen in ALLERGENS_STD_SET:
        print("  -> [RAG ì„±ê³µ] update_final_list")
        return "rag_success"

    print("  -> [RAG ë¶ˆí™•ì‹¤] llm_fallback")
    return "needs_llm_fallback"


def check_remaining_ingredients(state: AllergyGraphState) -> str:
    if state.get("ingredients_to_check"):
        print("  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient")
        return "has_more_ingredients"
    print("  -> [í•­ëª© ì—†ìŒ] finalize_processing")
    return "all_ingredients_done"


# =====================
# 6. ê·¸ë˜í”„ ë¹Œë“œ
# =====================
print("\n--- LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘ ---")
workflow = StateGraph(AllergyGraphState)

# ë…¸ë“œ ë“±ë¡
workflow.add_node("call_gcp_vision_api", call_gcp_vision_api)
workflow.add_node("parse_text_via_api", parse_text_via_api)
workflow.add_node("prepare_next_ingredient", prepare_next_ingredient)
workflow.add_node("rag_search", rag_search)
workflow.add_node("llm_fallback", llm_fallback)
workflow.add_node("update_final_list", update_final_list)
workflow.add_node("finalize_processing", finalize_processing)

# ì—£ì§€ ì—°ê²°
workflow.set_entry_point("call_gcp_vision_api")
workflow.add_edge("call_gcp_vision_api", "parse_text_via_api")

# parse â†’ ì¡°ê±´ë¶€ ë¶„ê¸°
workflow.add_conditional_edges(
    "parse_text_via_api",
    route_after_parse,
    {"has_ingredients": "prepare_next_ingredient", "no_ingredients": "finalize_processing"}
)

# ë£¨í”„ ë³¸ì²´
workflow.add_edge("prepare_next_ingredient", "rag_search")
workflow.add_conditional_edges(
    "rag_search",
    route_rag_result,
    {"rag_success": "update_final_list", "needs_llm_fallback": "llm_fallback"}
)
workflow.add_edge("llm_fallback", "update_final_list")
workflow.add_conditional_edges(
    "update_final_list",
    check_remaining_ingredients,
    {"has_more_ingredients": "prepare_next_ingredient", "all_ingredients_done": "finalize_processing"}
)
workflow.add_edge("finalize_processing", END)

# ì»´íŒŒì¼
app = workflow.compile()
print("--- âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ ---")


# =====================
# 7. ë””ë²„ê·¸/ê²€ì¦ ìœ í‹¸ (ì„ íƒ)
# =====================

def kb_self_check(max_show: int = 5):
    """ì¤‘ë³µ ì„ë² ë”© ê·¸ë£¹/ìƒ˜í”Œ í‘œì‹œ"""
    import hashlib
    groups = {}
    arr = np.ascontiguousarray(kb_embeddings)
    for i, row in enumerate(arr):
        h = hashlib.sha256(row.view(np.uint8)).hexdigest()
        groups.setdefault(h, []).append(i)
    dup_groups = {h:idxs for h,idxs in groups.items() if len(idxs) > 1}
    print(f"[SELF-CHECK] ì¤‘ë³µ ì„ë² ë”© ê·¸ë£¹ ìˆ˜: {len(dup_groups)}")
    for h, idxs in list(dup_groups.items())[:max_show]:
        names = [kb_terms[i] for i in idxs]
        cats  = [kb_categories[i] for i in idxs]
        print(f"  - size={len(idxs)} | terms={names[:5]} | cats={cats[:5]}")


# =====================
# 8. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì˜ˆì‹œ)
# =====================
if __name__ == "__main__":
    print("\n--- [Test Run: GCP OCR + API Parser + RAG + NLI] ---")

    # (ì„ íƒ) KB ì¤‘ë³µ ì²´í¬
    try:
        kb_self_check()
    except Exception as e:
        print(f"[SELF-CHECK] ì‹¤íŒ¨: {e}")

    # ì˜ˆì‹œ ì´ë¯¸ì§€ ê²½ë¡œ
    test_image = os.environ.get("ALLER_GUARD_TEST_IMAGE", r"C:\\Users\\MYNOTE\\AllerGuard\\Data\\ê¹€ê´‘ë¬´_118.jpg")
    if not os.path.exists(test_image):
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {test_image}")
    test_input = {"image_path": test_image}

    try:
        final_state = app.invoke(test_input, {"recursion_limit": 1000})
        print("\nìµœì¢… ë°˜í™˜ JSON:")
        print(final_state.get('final_output_json', ''))
    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")

