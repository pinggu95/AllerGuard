# --- 0. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import os
import pandas as pd
import numpy as np  # .npy íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•´ í•„ìš”
import io
import json
import re  # ğŸ‘ˆ ì •ê·œì‹(Regex) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from typing import List, Set, TypedDict

# ë¨¸ì‹ ëŸ¬ë‹ ë° ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline # (ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ìœ ì§€)
from google.cloud import vision
from google.oauth2 import service_account  # ğŸ‘ˆ ì§ì ‘ ì¸ì¦ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬
from langgraph.graph import StateGraph, END

print("--- ğŸš€ ì•Œë ˆë¥´ê¸° ë¶„ì„ ì„œë¹„ìŠ¤ (GCP Vision API + RAG + LLM Fallback) ì‹œì‘ ---")
print("ì‚¬ì „ ë¹Œë“œëœ ì„ë² ë”© ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

# --- 0a. í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡ ì •ì˜ ---
ALLERGENS_STD_SET = set([
    "ì•Œë¥˜", "ìš°ìœ ", "ë©”ë°€", "ë•…ì½©", "ëŒ€ë‘", "ë°€", "ì£", "í˜¸ë‘",
    "ê²Œ", "ìƒˆìš°", "ì˜¤ì§•ì–´", "ê³ ë“±ì–´", "ì¡°ê°œë¥˜", "ë³µìˆ­ì•„", "í† ë§ˆí† ",
    "ë‹­ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì‡ ê³ ê¸°", "ì•„í™©ì‚°ë¥˜"
])
print(f"âœ… í‘œì¤€ ì•Œë ˆë¥´ê¸° ì¹´í…Œê³ ë¦¬ {len(ALLERGENS_STD_SET)}ê°œ ë¡œë“œ ì™„ë£Œ.")

# --- 0b. ë¹„-ì„±ë¶„ í‚¤ì›Œë“œ í•„í„° ëª©ë¡ (Node 2 ìˆ˜ì •ìš©) ---
IGNORE_KEYWORDS = set([
    'ì—´ëŸ‰', 'íƒ„ìˆ˜í™”ë¬¼', 'ë‹¨ë°±ì§ˆ', 'ì§€ë°©', 'ë‹¹ë¥˜', 'ë‚˜íŠ¸ë¥¨', 'ì½œë ˆìŠ¤í…Œë¡¤',
    'í¬í™”ì§€ë°©', 'íŠ¸ëœìŠ¤ì§€ë°©', 'ë‚´ìš©ëŸ‰', 'I', 'II' # (ë¹ˆ ë¬¸ìì—´ '' ì œê±°ëœ ìƒíƒœ)
])
print(f"âœ… ë¹„-ì„±ë¶„ í•„í„° í‚¤ì›Œë“œ {len(IGNORE_KEYWORDS)}ê°œ ë¡œë“œ ì™„ë£Œ.")


# --- 1. ê¸€ë¡œë²Œ ì„¤ì •: ëª¨ë¸ ë¡œë“œ ë° RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ---
try:
    # 1a. RAG ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì¿¼ë¦¬ ì„ë² ë”©ìš©)
    EMBEDDING_MODEL_NAME = 'distiluse-base-multilingual-cased-v1'
    print(f"'{EMBEDDING_MODEL_NAME}' ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print("âœ… ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    # 1b. Zero-Shot NLI ëª¨ë¸ ë¡œë“œ (Fallback ì „ìš©) - [T5ì—ì„œ êµì²´ë¨]
    print("Zero-Shot NLI ëª¨ë¸ ë¡œë“œ ì¤‘ (Fallback ì „ìš©)...")
    NLI_MODEL_NAME = "klue/roberta-base"
    
    # ì´ì „ ë¡œê·¸ì—ì„œ CUDA ì‚¬ìš©ì´ í™•ì¸ë˜ì—ˆìœ¼ë¯€ë¡œ device=0 (GPU) ì„¤ì •
    nli_pipeline = pipeline("zero-shot-classification", model=NLI_MODEL_NAME, device=0) 
    print(f"âœ… Zero-Shot NLI ëª¨ë¸ ({NLI_MODEL_NAME}) ë¡œë“œ ì™„ë£Œ.")
    
    # NLI Fallbackì´ ì‚¬ìš©í•  í›„ë³´ ë ˆì´ë¸” ëª©ë¡ (ê¸€ë¡œë²Œ ìºì‹œ)
    ALLERGEN_CANDIDATES = list(ALLERGENS_STD_SET) + ["ê´€ë ¨ ì—†ìŒ"]
    print(f"âœ… NLI Fallback í›„ë³´ ë ˆì´ë¸” {len(ALLERGEN_CANDIDATES)}ê°œ ì¤€ë¹„ ì™„ë£Œ.")

    # 1c. GCP Vision API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì§ì ‘ ê²½ë¡œ ì§€ì • ë°©ì‹)
    print("GCP Vision API í´ë¼ì´ì–¸íŠ¸ (ì§ì ‘ ê²½ë¡œ ì§€ì •) ì´ˆê¸°í™” ì¤‘...")
    
    KEY_JSON_PATH = r"C:\Users\ì •ì£¼í™˜\Desktop\keyfolder\nlp-study-467306-563e76afdbca.json"

    try:
        credentials = service_account.Credentials.from_service_account_file(KEY_JSON_PATH)
        vision_client = vision.ImageAnnotatorClient(credentials=credentials)
        print(f"âœ… GCP Vision í´ë¼ì´ì–¸íŠ¸ (ì§ì ‘ ê²½ë¡œ) ì¤€ë¹„ ì™„ë£Œ.")
        
    except FileNotFoundError:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì§€ì •ëœ í‚¤ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   ì‹œë„í•œ ê²½ë¡œ: {KEY_JSON_PATH}")
        raise 
    
    # 1d. ì‚¬ì „ ê³„ì‚°ëœ ë²¡í„° ìºì‹œ ë¡œë“œ
    print("ì‚¬ì „ ê³„ì‚°ëœ RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ì¤‘...")
    kb_embeddings = np.load("kb_embeddings.npy") 
    
    with open("kb_categories.json", "r", encoding="utf-8") as f:
            kb_categories = json.load(f) 
    
    print(f"âœ… RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ì™„ë£Œ ({len(kb_categories)}ê°œ í•­ëª©)")

except Exception as e:
    print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ê¸€ë¡œë²Œ ì„¤ì • ì‹¤íŒ¨: {e}")
    exit()


# RAG ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  ìœ ì‚¬ë„ ì„ê³„ê°’ (íŠœë‹ëœ ê°’ìœ¼ë¡œ ê°€ì •, ì˜ˆ: 0.85)
# (ë§Œì•½ 0.9ë¡œ ìœ ì§€í•˜ê³  NLIê°€ 'ìœ ì²­ë‹¨ë°±ë¶„ë§'ì„ ì¡ë„ë¡ í•˜ë ¤ë©´ 0.9ë¡œ ì„¤ì •í•˜ì„¸ìš”)
RAG_CONFIDENCE_THRESHOLD = 0.85
print(f"â„¹ï¸ RAG ì‹ ë¢°ë„ ì„ê³„ê°’: {RAG_CONFIDENCE_THRESHOLD}")

# NLI Fallback ê²°ê³¼ê°€ ìœ íš¨í•˜ë‹¤ê³  ì¸ì •í•  ìµœì†Œ ì ìˆ˜
NLI_FALLBACK_THRESHOLD = 0.5  
print(f"â„¹ï¸ NLI Fallback ì‹ ë¢°ë„ ì„ê³„ê°’: {NLI_FALLBACK_THRESHOLD}")

# --- 2. LangGraph ìƒíƒœ ì •ì˜ ---
class AllergyGraphState(TypedDict):
    image_path: str
    raw_ocr_text: str
    ingredients_to_check: List[str]
    current_ingredient: str
    rag_result: dict
    final_allergens: Set[str]
    final_output_json: str


# --- 3. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---

def call_gcp_vision_api(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 1 (Entry Point): GCP Vision API í˜¸ì¶œ
    """
    print(f"\n--- (Node 1: call_gcp_vision_api) ---")
    img_path = state['image_path']
    print(f"GCP Vision API í˜¸ì¶œ... (ì´ë¯¸ì§€: {img_path})")
    try:
        with io.open(img_path, 'rb') as image_file:
            content = image_file.read()
        image = vision.Image(content=content)
        
        response = vision_client.text_detection(image=image)
        if response.error.message:
            raise Exception(f"GCP API Error: {response.error.message}")

        raw_text = response.full_text_annotation.text
        print(f"âœ… GCP OCR ì„±ê³µ. (ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)})")
        return {**state, "raw_ocr_text": raw_text}
    
    except Exception as e:
        print(f"âŒ GCP Vision API ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {**state, "raw_ocr_text": ""}


def parse_text_from_raw(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 2 (Regex íŒŒì„œ ë…¸ë“œ)
    (startswith í•„í„° ë¡œì§ì´ ì ìš©ëœ ìµœì¢… ìˆ˜ì • ë²„ì „)
    """
    print(f"\n--- (Node 2: parse_text_from_raw) [Regex Parser] ---")
    raw_text = state['raw_ocr_text']
    if not raw_text or not raw_text.strip():
        print("â„¹ï¸ OCR í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ íŒŒì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    clean_text = raw_text.replace("\n", " ")

    ingredient_queue = []
    found_allergens_set = set()

    match1 = re.search(r"ì›ì¬ë£Œëª…[ :](.*?)(â€¢|\||ì˜ì–‘ì •ë³´|ì˜ì–‘ì„±ë¶„|$)", clean_text)
    
    if match1:
        ingredient_blob = match1.group(1).strip()
        raw_ingredients_list = [item.strip() for item in ingredient_blob.split(',') if item.strip()]
        cleaned_ingredients_raw = [name.split('(')[0].strip() for name in raw_ingredients_list if name.strip()]
        
        cleaned_ingredients_filtered = []
        for item in cleaned_ingredients_raw:
            is_noise = False
            for keyword in IGNORE_KEYWORDS:
                if item.startswith(keyword):
                    is_noise = True
                    print(f"  -> í•„í„°ë§ë¨: '{item}' (ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ '{keyword}'ë¡œ ì‹œì‘í•˜ë¯€ë¡œ ì œì™¸)")
                    break 
            
            if not is_noise:
                cleaned_ingredients_filtered.append(item)
        
        ingredient_queue.extend(cleaned_ingredients_filtered)
        print(f"âœ… Regex íŒŒì„œ: 'ì›ì¬ë£Œëª…' ì„¹ì…˜ì—ì„œ {len(cleaned_ingredients_filtered)}ê°œ ì„±ë¶„ ì¶”ì¶œ: {cleaned_ingredients_filtered}")
    
    else:
        print("â„¹ï¸ Regex íŒŒì„œ: 'ì›ì¬ë£Œëª…' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨.")

    match2 = re.search(r"â€¢?\s*([\w,]+)\s+í•¨ìœ ", clean_text)
    if match2:
        contains_blob = match2.group(1) 
        contains_list = [item.strip() for item in contains_blob.split(',') if item.strip()]
        print(f"âœ… Regex íŒŒì„œ: '...í•¨ìœ ' ì„¹ì…˜ì—ì„œ {len(contains_list)}ê°œ í•­ëª© ì¶”ì¶œ: {contains_list}")
        
        for item in contains_list:
            if item not in IGNORE_KEYWORDS:
                ingredient_queue.append(item) 
            
            if item in ALLERGENS_STD_SET:
                print(f"  -> '{item}'ì€(ëŠ”) í‘œì¤€ ì•Œë ˆë¥´ê¸°ì´ë¯€ë¡œ final_setì— ì§ì ‘ ì¶”ê°€.")
                found_allergens_set.add(item) 
    else:
        print("â„¹ï¸ Regex íŒŒì„œ: '...í•¨ìœ ' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨.")

    final_queue = sorted(list(set(ingredient_queue)))
    print(f"==> ìµœì¢… RAG ê²€ì‚¬ í (ì¤‘ë³µì œê±°, {len(final_queue)}ê°œ): {final_queue}")
    
    return {
        **state,
        "ingredients_to_check": final_queue,      
        "final_allergens": found_allergens_set 
    }


def prepare_next_ingredient(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 3 (ë£¨í”„ ì»¨íŠ¸ë¡¤ëŸ¬)
    """
    print(f"\n--- (Node 3: prepare_next_ingredient) ---")
    queue = state['ingredients_to_check']
    next_ingredient = queue.pop(0) 
    print(f"ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: '{next_ingredient}' (ë‚¨ì€ í•­ëª©: {len(queue)}ê°œ)")
    return {
        **state,
        "current_ingredient": next_ingredient,
        "ingredients_to_check": queue
    }

def rag_search(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 4 (í•µì‹¬ RAG ê²€ìƒ‰ ë…¸ë“œ)
    """
    print(f"--- (Node 4: rag_search) ---")
    ingredient = state['current_ingredient']
    
    query_embedding = embedding_model.encode([ingredient])
    similarities = cosine_similarity(query_embedding, kb_embeddings)
    
    best_match_index = np.argmax(similarities[0])
    confidence_score = float(similarities[0][best_match_index])
    
    found_allergen = kb_categories[best_match_index] 
    
    print(f"RAG ê²€ìƒ‰: '{ingredient}' (ìœ ì‚¬ë„: {confidence_score:.4f}) -> ë§¤í•‘: '{found_allergen}'")
    
    rag_result_data = {
        "confidence": confidence_score,
        "found_allergen": found_allergen
    }
    return {**state, "rag_result": rag_result_data}


# ==============================================================================
# === ğŸ’¥ [êµì²´ëœ ë…¸ë“œ 5] (Zero-Shot NLI íŒŒì´í”„ë¼ì¸ ë²„ì „) ğŸ’¥ ===
# ==============================================================================
def llm_fallback(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 5 (LLM Fallback ë…¸ë“œ) - [NLI Zero-Shot ë²„ì „]
    
    RAGê°€ ì‹¤íŒ¨í•œ í•­ëª©ì„ Zero-Shot Classification íŒŒì´í”„ë¼ì¸(NLI ëª¨ë¸)ìœ¼ë¡œ ë„˜ê¹ë‹ˆë‹¤.
    ì…ë ¥ ì„±ë¶„ì„ ëª¨ë“  ì•Œë ˆë¥´ê¸° í›„ë³´ ë ˆì´ë¸”ê³¼ ë¹„êµí•˜ì—¬ ìµœê³  ì ìˆ˜(entailment)ë¥¼ ë°›ì€ í•­ëª©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"--- (Node 5: llm_fallback) [NLI Zero-Shot] ---")
    ingredient = state['current_ingredient']
    print(f"NLI Fallback: '{ingredient}' ë¶„ë¥˜ ìš”ì²­... (í›„ë³´: {len(ALLERGEN_CANDIDATES)}ê°œ)")

    try:
        # NLI íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (ê¸€ë¡œë²Œ íŒŒì´í”„ë¼ì¸ 'nli_pipeline' ë° í›„ë³´ ë¦¬ìŠ¤íŠ¸ 'ALLERGEN_CANDIDATES' ì¬ì‚¬ìš©)
        response = nli_pipeline(ingredient, ALLERGEN_CANDIDATES) 
        
        # ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ë ˆì´ë¸”ê³¼ ì ìˆ˜ë¥¼ ì¶”ì¶œ
        top_label = response['labels'][0]
        top_score = response['scores'][0]
        
        print(f"NLI ì‘ë‹µ: Label='{top_label}', Score={top_score:.4f}")

        # ìµœê³  ì ìˆ˜ ë ˆì´ë¸”ì´ í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡(SET)ì— ìˆëŠ”ì§€ í™•ì¸
        if top_label in ALLERGENS_STD_SET: 
            # í•´ë‹¹ ì ìˆ˜ê°€ ìš°ë¦¬ê°€ ì„¤ì •í•œ NLI ì„ê³„ê°’(ì˜ˆ: 0.5)ë³´ë‹¤ ë†’ì€ì§€ í™•ì¸
            if top_score >= NLI_FALLBACK_THRESHOLD:
                 print(f"  -> ìœ íš¨í•œ ë¶„ë¥˜: '{top_label}' (Score: {top_score}, ì„ê³„ê°’ {NLI_FALLBACK_THRESHOLD} í†µê³¼).")
                 return {**state, "rag_result": {"confidence": top_score, "found_allergen": top_label}}
            else:
                 # ì•Œë ˆë¥´ê¸°ì´ê¸´ í•˜ì§€ë§Œ, ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ì•„ì„œ ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ
                 print(f"  -> ì ìˆ˜ê°€ ë‚®ìŒ ({top_score} < {NLI_FALLBACK_THRESHOLD}). 'ì—†ìŒ'ìœ¼ë¡œ ì²˜ë¦¬.")
                 return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì—†ìŒ"}}
        else:
            # ìµœê³  ì ìˆ˜ ë ˆì´ë¸”ì´ "ê´€ë ¨ ì—†ìŒ"ì´ê±°ë‚˜, (í˜¹ì‹œ ëª¨ë¥¼) ë‹¤ë¥¸ ì“°ë ˆê¸° ê°’ì¸ ê²½ìš°
            print(f"  -> ìµœê³  ì ìˆ˜ ë ˆì´ë¸”ì´ '{top_label}'ì´ë¯€ë¡œ 'ì—†ìŒ'ìœ¼ë¡œ ì²˜ë¦¬.")
            return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì—†ìŒ"}}
            
    except Exception as e:
        print(f"âŒ NLI Fallback ì¤‘ ì˜¤ë¥˜: {e}")
        return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì˜¤ë¥˜"}}
# ==============================================================================
# === [êµì²´ëœ ë…¸ë“œ 5] ë ===
# ==============================================================================


def update_final_list(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 6 (ê²°ê³¼ ì·¨í•© ë…¸ë“œ)
    """
    print(f"--- (Node 6: update_final_list) ---")
    result_allergen = state['rag_result']['found_allergen']
    
    if result_allergen in ALLERGENS_STD_SET:
        current_set = state['final_allergens']
        print(f"âœ… ìœ íš¨í•œ ì•Œë ˆë¥´ê¸° ë°œê²¬: '{result_allergen}'. ìµœì¢… ëª©ë¡ì— ì¶”ê°€.")
        current_set.add(result_allergen)
        return {**state, "final_allergens": current_set}
    else:
        print(f"â„¹ï¸ '{result_allergen}'ì€(ëŠ”) í‘œì¤€ ì•Œë ˆë¥´ê¸° í•­ëª©ì´ ì•„ë‹ˆë¯€ë¡œ ë¬´ì‹œí•©ë‹ˆë‹¤.")
        return state 

def finalize_processing(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 7 (ì¢…ë£Œ ë…¸ë“œ)
    """
    print(f"\n--- (Node 7: finalize_processing) ---")
    final_set = state['final_allergens']
    
    final_list = sorted(list(final_set))
    final_json = json.dumps(final_list, ensure_ascii=False)
    
    print(f"ğŸ‰ ëª¨ë“  ì„±ë¶„ ê²€ì‚¬ ì™„ë£Œ. ìµœì¢… ê²°ê³¼: {final_json}")
    return {**state, "final_output_json": final_json}


# --- 4. LangGraph ì—£ì§€(Edge) í•¨ìˆ˜ ì •ì˜ ---

def route_rag_result(state: AllergyGraphState) -> str:
    """(ì¡°ê±´ë¶€ ì—£ì§€ 1: RAG ë¼ìš°í„°)
    """
    print(f"--- (Edge: route_rag_result?) ---")
    confidence = state['rag_result']['confidence']
    allergen = state['rag_result']['found_allergen']
    
    if confidence >= RAG_CONFIDENCE_THRESHOLD and allergen in ALLERGENS_STD_SET:
        print(f"  -> [RAG ì„±ê³µ]. 'update_final_list'ë¡œ ì´ë™.")
        return "rag_success"
    else:
        print(f"  -> [RAG ì‹¤íŒ¨/ë¶ˆí™•ì‹¤]. 'llm_fallback'ìœ¼ë¡œ ì´ë™.")
        return "needs_llm_fallback"

def check_remaining_ingredients(state: AllergyGraphState) -> str:
    """(ì¡°ê±´ë¶€ ì—£ì§€ 2: ë£¨í”„ ì œì–´)
    """
    print(f"--- (Edge: check_remaining_ingredients?) ---")
    
    if state["ingredients_to_check"] and len(state["ingredients_to_check"]) > 0:
        print(f"  -> [í•­ëª© ë‚¨ìŒ]. 'prepare_next_ingredient'ë¡œ ë£¨í”„.")
        return "has_more_ingredients"
    else:
        print("  -> [í•­ëª© ì—†ìŒ]. 'finalize_processing'ë¡œ ì´ë™.")
        return "all_ingredients_done"

# --- 5. ê·¸ë˜í”„ ë¹Œë“œ ë° ì»´íŒŒì¼ ---

print("\n--- LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘ ---")

workflow = StateGraph(AllergyGraphState)

# 1. ëª¨ë“  ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€
workflow.add_node("call_gcp_vision_api", call_gcp_vision_api)
workflow.add_node("parse_text_from_raw", parse_text_from_raw)
workflow.add_node("prepare_next_ingredient", prepare_next_ingredient)
workflow.add_node("rag_search", rag_search)
workflow.add_node("llm_fallback", llm_fallback) # <--- NLI ë²„ì „ í•¨ìˆ˜ê°€ ë“±ë¡ë¨
workflow.add_node("update_final_list", update_final_list)
workflow.add_node("finalize_processing", finalize_processing)

# 2. ì—£ì§€ ì—°ê²° (íë¦„ ì •ì˜)
workflow.set_entry_point("call_gcp_vision_api")                
workflow.add_edge("call_gcp_vision_api", "parse_text_from_raw")    
workflow.add_edge("parse_text_from_raw", "prepare_next_ingredient") 
workflow.add_edge("prepare_next_ingredient", "rag_search")          

# 3. RAG ë¼ìš°íŒ… (ì¡°ê±´ë¶€ ì—£ì§€ 1)
workflow.add_conditional_edges(
    "rag_search",
    route_rag_result,
    {"rag_success": "update_final_list", "needs_llm_fallback": "llm_fallback"}
)

# 4. Fallback ê²°ê³¼ë„ ì·¨í•© ë…¸ë“œë¡œ ì—°ê²° (NLI Fallbackì´ ì—°ê²°ë¨)
workflow.add_edge("llm_fallback", "update_final_list") 

# 5. ë©”ì¸ ë£¨í”„ (ì¡°ê±´ë¶€ ì—£ì§€ 2)
workflow.add_conditional_edges(
    "update_final_list",
    check_remaining_ingredients,
    {"has_more_ingredients": "prepare_next_ingredient", "all_ingredients_done": "finalize_processing"}
)

# 6. ì¢…ë£Œ ë…¸ë“œ ì—°ê²°
workflow.add_edge("finalize_processing", END)

# 7. ì»´íŒŒì¼
app = workflow.compile()
print("--- âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ ---")


# --- 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---
print("\n\n--- [Test Run: GCP API + Regex íŒŒì„œ + NLI Fallback ê¸°ë°˜ ì‹¤í–‰] ---")

# (í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ íŒŒì¼ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤)
my_test_image_file = "image.jpg" # ğŸ‘ˆ 'image.jpg'ëŠ” OCR ë¡œê·¸ë¥¼ ì œê³µí•œ ê·¸ ì´ë¯¸ì§€ íŒŒì¼ ê°€ì •

if my_test_image_file:
    test_input = {"image_path": my_test_image_file}
    print(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘: {my_test_image_file}\n")

    print("\n--- [Test Run: ìµœì¢… ê²°ê³¼ (invoke)] ---")
    final_state = app.invoke(test_input, {"recursion_limit": 100}) 
    print("\nìµœì¢… ë°˜í™˜ JSON:")
    print(final_state['final_output_json'])

else:
    print("\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê±´ë„ˆëœ€: 'my_test_image_file' ë³€ìˆ˜ì— ì´ë¯¸ì§€ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")