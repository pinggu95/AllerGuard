{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fed031ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.56.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.10.1)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.22.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from accelerate) (2.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\MYNOTE\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-vision in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.10.2)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.8.5)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.34.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-vision) (2.40.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-vision) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-cloud-vision) (3.20.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (2.181.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (2.10.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-generativeai) (4.14.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.4)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai) (0.30.2)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-api-python-client->google-generativeai) (4.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic->google-generativeai) (2.27.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision) (0.6.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mynote\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2025.8.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\MYNOTE\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate tokenizers sentencepiece\n",
    "!pip install -U google-cloud-vision google-generativeai \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c69aeeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸš€ ì•Œë ˆë¥´ê¸° ë¶„ì„ ì„œë¹„ìŠ¤ (GCP Vision API + RAG + LLM Fallback) ì‹œìž‘ ---\n",
      "âœ… í‘œì¤€ ì•Œë ˆë¥´ê¸° ì¹´í…Œê³ ë¦¬ 19ê°œ ë¡œë“œ ì™„ë£Œ.\n",
      "âœ… ë¹„-ì„±ë¶„ í•„í„° í‚¤ì›Œë“œ 12ê°œ ë¡œë“œ ì™„ë£Œ.\n",
      "'distiluse-base-multilingual-cased-v1' ì¿¼ë¦¬ ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "âœ… ì¿¼ë¦¬ ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\n",
      "Zero-Shot NLI ëª¨ë¸ ë¡œë“œ ì¤‘ (Fallback ì „ìš©)...\n",
      "âœ… NLI ëª¨ë¸ ë¡œë“œ: joeddav/xlm-roberta-large-xnli\n",
      "GCP Vision API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...\n",
      "âœ… GCP Vision í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ.\n",
      "ì‚¬ì „ ê³„ì‚°ëœ RAG ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...\n",
      "âœ… KB ë¡œë“œ ì™„ë£Œ (í•­ëª©: 689ê°œ, terms:689ê°œ)\n",
      "\n",
      "--- LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œìž‘ ---\n",
      "--- âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ ---\n",
      "\n",
      "--- [Test Run: GCP OCR + API Parser + RAG + NLI] ---\n",
      "[SELF-CHECK] ì¤‘ë³µ ìž„ë² ë”© ê·¸ë£¹ ìˆ˜: 0\n",
      "\n",
      "--- (Node 1: call_gcp_vision_api) ---\n",
      "GCP Vision OCR í˜¸ì¶œ... (ì´ë¯¸ì§€: C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\Data\\\\ê¹€ê´‘ë¬´_118.jpg)\n",
      "âœ… OCR ì„±ê³µ. í…ìŠ¤íŠ¸ ê¸¸ì´: 606\n",
      "\n",
      "--- (Node 2: parse_text_via_api) [API Parser] ---\n",
      "âœ… Gemini íŒŒì‹± ì™„ë£Œ: queue=26 / pre_found=['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ê±´ë‹¹ê·¼' (ë‚¨ì€ 25ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ê±´ë‹¹ê·¼' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7632, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ê±´ì–‘íŒŒ' (ë‚¨ì€ 24ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ê±´ì–‘íŒŒ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7799, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë†ì¶•í† ë§ˆí† íŽ˜ì´ìŠ¤íŠ¸' (ë‚¨ì€ 23ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë†ì¶•í† ë§ˆí† íŽ˜ì´ìŠ¤íŠ¸' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7639, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë†ì¶•íŒŒì¸ì• í”Œì¦™' (ë‚¨ì€ 22ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë†ì¶•íŒŒì¸ì• í”Œì¦™' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 1.0000, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë†ì¶•í“¨ë ˆ' (ë‚¨ì€ 21ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë†ì¶•í“¨ë ˆ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7408, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë‹­ê³ ê¸°' (ë‚¨ì€ 20ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë‹­ê³ ê¸°' â†’ 'ë‹­ê³ ê¸°' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'ë‹­ê³ ê¸°' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë‹¹ê·¼' (ë‚¨ì€ 19ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë‹¹ê·¼' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7150, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ëŒ€ë‘' (ë‚¨ì€ 18ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ëŒ€ë‘' â†’ 'ëŒ€ë‘' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'ëŒ€ë‘' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë¬¼ì—¿' (ë‚¨ì€ 17ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë¬¼ì—¿' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.8205, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë°€' (ë‚¨ì€ 16ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë°€' â†’ 'ë°€' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'ë°€' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ë°œíš¨ì‹ì´ˆ' (ë‚¨ì€ 15ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ë°œíš¨ì‹ì´ˆ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7093, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì‚¬ê³¼' (ë‚¨ì€ 14ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì‚¬ê³¼' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.6084, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì‚¬ê³¼ì¦™' (ë‚¨ì€ 13ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì‚¬ê³¼ì¦™' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 1.0000, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì„¤íƒ•' (ë‚¨ì€ 12ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì„¤íƒ•' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.6724, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì‡ ê³ ê¸°' (ë‚¨ì€ 11ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì‡ ê³ ê¸°' â†’ 'ì‡ ê³ ê¸°' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'ì‡ ê³ ê¸°' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì•Œë¥˜' (ë‚¨ì€ 10ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì•Œë¥˜' â†’ 'ì•Œë¥˜' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'ì•Œë¥˜' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì–‘íŒŒ' (ë‚¨ì€ 9ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì–‘íŒŒ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.6768, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ìš°ìœ ' (ë‚¨ì€ 8ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ìš°ìœ ' â†’ 'ìš°ìœ ' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'ìš°ìœ ' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì „ë¶„' (ë‚¨ì€ 7ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì „ë¶„' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.8101, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì •ì œì†Œê¸ˆ' (ë‚¨ì€ 6ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì •ì œì†Œê¸ˆ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7714, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì •ì œìˆ˜' (ë‚¨ì€ 5ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì •ì œìˆ˜' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7713, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì¡°ê°œë¥˜' (ë‚¨ì€ 4ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì¡°ê°œë¥˜' â†’ 'ì¡°ê°œë¥˜' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'ì¡°ê°œë¥˜' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'ì²œì—°í–¥ì‹ ë£Œ' (ë‚¨ì€ 3ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'ì²œì—°í–¥ì‹ ë£Œ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.5284, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'í† ë§ˆí† ' (ë‚¨ì€ 2ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'í† ë§ˆí† ' â†’ 'í† ë§ˆí† ' (ìœ ì‚¬ë„ 1.0000, by=alias)\n",
      "  -> [RAG ì„±ê³µ] update_final_list\n",
      "--- (Node 6: update_final_list) ---\n",
      "âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: 'í† ë§ˆí† ' â†’ ['ë‹­ê³ ê¸°', 'ëŒ€ë‘', 'ë°€', 'ì‡ ê³ ê¸°', 'ì•Œë¥˜', 'ìš°ìœ ', 'ì¡°ê°œë¥˜', 'í† ë§ˆí† ']\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'í† ë§ˆí† í•¨ìœ ' (ë‚¨ì€ 1ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'í† ë§ˆí† í•¨ìœ ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.9672, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\n",
      "\n",
      "--- (Node 3: prepare_next_ingredient) ---\n",
      "ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: 'íŒŒì¸ì• í”Œ' (ë‚¨ì€ 0ê°œ)\n",
      "--- (Node 4: rag_search) ---\n",
      "RAG ê²€ìƒ‰: 'íŒŒì¸ì• í”Œ' â†’ 'ì—†ìŒ' (ìœ ì‚¬ë„ 0.7872, by=lex_guard)\n",
      "  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\n",
      "--- (Node 6: update_final_list) ---\n",
      "â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': 'ì—†ìŒ' (ë¬´ì‹œ)\n",
      "  -> [í•­ëª© ì—†ìŒ] finalize_processing\n",
      "\n",
      "--- (Node 7: finalize_processing) ---\n",
      "ðŸŽ‰ ìµœì¢… ê²°ê³¼: [\"ë‹­ê³ ê¸°\", \"ëŒ€ë‘\", \"ë°€\", \"ì‡ ê³ ê¸°\", \"ì•Œë¥˜\", \"ìš°ìœ \", \"ì¡°ê°œë¥˜\", \"í† ë§ˆí† \"]\n",
      "\n",
      "ìµœì¢… ë°˜í™˜ JSON:\n",
      "[\"ë‹­ê³ ê¸°\", \"ëŒ€ë‘\", \"ë°€\", \"ì‡ ê³ ê¸°\", \"ì•Œë¥˜\", \"ìš°ìœ \", \"ì¡°ê°œë¥˜\", \"í† ë§ˆí† \"]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import re\n",
    "from typing import List, Set, TypedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "# GCP Vision OCR\n",
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# (ì„ íƒ) Gemini Structured Output\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    _HAS_GEMINI = True\n",
    "except Exception:\n",
    "    _HAS_GEMINI = False\n",
    "\n",
    "# (ì„ íƒ) Document AI\n",
    "try:\n",
    "    from google.cloud import documentai\n",
    "    _HAS_DOCAI = True\n",
    "except Exception:\n",
    "    _HAS_DOCAI = False\n",
    "\n",
    "# LangGraph\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "print(\"--- ðŸš€ ì•Œë ˆë¥´ê¸° ë¶„ì„ ì„œë¹„ìŠ¤ (GCP Vision API + RAG + LLM Fallback) ì‹œìž‘ ---\")\n",
    "\n",
    "# =====================\n",
    "# 0. ì „ì—­ ì„¤ì •/ìƒìˆ˜\n",
    "# =====================\n",
    "ALLERGENS_STD_SET = set([\n",
    "    \"ì•Œë¥˜\", \"ìš°ìœ \", \"ë©”ë°€\", \"ë•…ì½©\", \"ëŒ€ë‘\", \"ë°€\", \"ìž£\", \"í˜¸ë‘\",\n",
    "    \"ê²Œ\", \"ìƒˆìš°\", \"ì˜¤ì§•ì–´\", \"ê³ ë“±ì–´\", \"ì¡°ê°œë¥˜\", \"ë³µìˆ­ì•„\", \"í† ë§ˆí† \",\n",
    "    \"ë‹­ê³ ê¸°\", \"ë¼ì§€ê³ ê¸°\", \"ì‡ ê³ ê¸°\", \"ì•„í™©ì‚°ë¥˜\"\n",
    "])\n",
    "print(f\"âœ… í‘œì¤€ ì•Œë ˆë¥´ê¸° ì¹´í…Œê³ ë¦¬ {len(ALLERGENS_STD_SET)}ê°œ ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "IGNORE_KEYWORDS = set([\n",
    "    \"ì—´ëŸ‰\", \"íƒ„ìˆ˜í™”ë¬¼\", \"ë‹¨ë°±ì§ˆ\", \"ì§€ë°©\", \"ë‹¹ë¥˜\", \"ë‚˜íŠ¸ë¥¨\", \"ì½œë ˆìŠ¤í…Œë¡¤\",\n",
    "    \"í¬í™”ì§€ë°©\", \"íŠ¸ëžœìŠ¤ì§€ë°©\", \"ë‚´ìš©ëŸ‰\", \"I\", \"II\"\n",
    "])\n",
    "print(f\"âœ… ë¹„-ì„±ë¶„ í•„í„° í‚¤ì›Œë“œ {len(IGNORE_KEYWORDS)}ê°œ ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "# ë™ì˜ì–´â†’í‘œì¤€ ë§¤í•‘\n",
    "ALIAS2STD = {\n",
    "    \"ë‚œë¥˜\": \"ì•Œë¥˜\", \"ê³„ëž€\": \"ì•Œë¥˜\", \"ë‹¬ê±€\": \"ì•Œë¥˜\", \"ë‚œë°±\": \"ì•Œë¥˜\", \"ë‚œí™©\": \"ì•Œë¥˜\",\n",
    "    \"ìœ ì²­\": \"ìš°ìœ \", \"ìœ ì²­ë‹¨ë°±\": \"ìš°ìœ \", \"ìœ ì²­ë‹¨ë°±ë¶„ë§\": \"ìš°ìœ \", \"ì¹´ì œì¸\": \"ìš°ìœ \", \"ì¹´ì œì¸ë‚˜íŠ¸ë¥¨\": \"ìš°ìœ \",\n",
    "    \"ì¹˜ì¦ˆ\": \"ìš°ìœ \", \"ì¹˜ì¦ˆë¶„ë§\": \"ìš°ìœ \", \"íƒˆì§€ë¶„ìœ \": \"ìš°ìœ \", \"ë¶„ìœ \": \"ìš°ìœ \",\n",
    "    \"ëŒ€ë‘ë ˆì‹œí‹´\": \"ëŒ€ë‘\", \"ë ˆì‹œí‹´(ëŒ€ë‘)\": \"ëŒ€ë‘\", \"ë°€ê°€ë£¨\": \"ë°€\", \"ë•…ì½©ë²„í„°\": \"ë•…ì½©\",\n",
    "    \"í˜¸ë‘ë¶„íƒœ\": \"í˜¸ë‘\", \"ìž£ê°€ë£¨\": \"ìž£\",\n",
    "    \"í™í•©\": \"ì¡°ê°œë¥˜\", \"êµ´\": \"ì¡°ê°œë¥˜\", \"ì „ë³µ\": \"ì¡°ê°œë¥˜\",\n",
    "    \"ê³ ë“±ì–´ì¶”ì¶œë¬¼\": \"ê³ ë“±ì–´\", \"ìƒˆìš°ì¶”ì¶œë¬¼\": \"ìƒˆìš°\", \"ì˜¤ì§•ì–´ë¨¹ë¬¼\": \"ì˜¤ì§•ì–´\",\n",
    "    \"ë³µìˆ­ì•„ë†ì¶•ì•¡\": \"ë³µìˆ­ì•„\", \"í† ë§ˆí† íŽ˜ì´ìŠ¤íŠ¸\": \"í† ë§ˆí† \",\n",
    "    \"ì•„í™©ì‚°ë‚˜íŠ¸ë¥¨\": \"ì•„í™©ì‚°ë¥˜\",\n",
    "}\n",
    "\n",
    "KB_EMB_PATH = r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\ì°¨ì§€ì˜ˆ\\\\kb_embeddings.npy\"\n",
    "KB_CAT_PATH = r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\ì°¨ì§€ì˜ˆ\\\\kb_categories.json\"\n",
    "KB_CSV_PATH = r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\domestic_allergy_rag_knowledge_1000.csv\"\n",
    "\n",
    "KEY_JSON_PATH = os.environ.get(\"GCP_VISION_KEY_PATH\", r\"D:\\key folder\\ocr-project-470906-7ffeebabeb09.json\")\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"distiluse-base-multilingual-cased-v1\"\n",
    "NLI_MODEL_NAME = \"joeddav/xlm-roberta-large-xnli\"\n",
    "\n",
    "USE_API_PARSER = os.environ.get(\"ALLER_GUARD_API_PARSER\", \"gemini\").lower()\n",
    "\n",
    "RAG_CONFIDENCE_THRESHOLD = float(os.environ.get(\"RAG_CONF_THRESH\", 0.85))\n",
    "NLI_FALLBACK_THRESHOLD   = float(os.environ.get(\"NLI_FALLBACK_THRESH\", 0.5))\n",
    "\n",
    "HARDCODED_GEMINI_API_KEY = \"AIzaSyDMTVeVGPU374hlJWEGhxB902f-RxkRVSU\"\n",
    "\n",
    "def _get_gemini_api_key():\n",
    "    if HARDCODED_GEMINI_API_KEY:\n",
    "        return HARDCODED_GEMINI_API_KEY.strip()\n",
    "    for var in (\"GEMINI_API_KEY\", \"GOOGLE_API_KEY\", \"GENAI_API_KEY\"):\n",
    "        v = os.environ.get(var)\n",
    "        if v:\n",
    "            return v\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "        load_dotenv()\n",
    "        for var in (\"GEMINI_API_KEY\", \"GOOGLE_API_KEY\", \"GENAI_API_KEY\"):\n",
    "            v = os.environ.get(var)\n",
    "            if v:\n",
    "                return v\n",
    "    except Exception:\n",
    "        pass\n",
    "    for fname in (\"gemini_api_key.txt\", \".gemini_api_key\"):\n",
    "        if os.path.exists(fname):\n",
    "            try:\n",
    "                with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "                    key = f.read().strip()\n",
    "                    if key:\n",
    "                        return key\n",
    "            except Exception:\n",
    "                pass\n",
    "    return None\n",
    "\n",
    "GENERIC_SUFFIXES = (\n",
    "    \"ê°€ë£¨\",\"ë¶„ë§\",\"ì¶”ì¶œë¬¼\",\"ë†ì¶•ì•¡\",\"ë†ì¶•ë¶„ë§\",\"ìœ ëž˜\",\"ë‹¨ë°±ì§ˆ\",\"ë†ì¶•\",\n",
    "    \"íŽ˜ì´ìŠ¤íŠ¸\",\"ì—‘ê¸°ìŠ¤\",\"ë¶„íƒœ\",\"ì‹œëŸ½\",\"ì˜¤ì¼\",\"í˜¼í•©\",\"ì•¡\",\"ë¶„\",\"ì •ì œ\",\"ê°€ìˆ˜ë¶„í•´ë¬¼\"\n",
    ")\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    return x / (np.linalg.norm(x, axis=-1, keepdims=True) + 1e-12)\n",
    "\n",
    "def normalize_to_std(name: str) -> str:\n",
    "    n = re.sub(r\"\\s+\", \"\", str(name))\n",
    "    n = n.split(\"(\")[0]\n",
    "    return ALIAS2STD.get(n, n)\n",
    "\n",
    "def core_token(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \"\", str(s))\n",
    "    s = s.split(\"(\")[0]\n",
    "    for suf in GENERIC_SUFFIXES:\n",
    "        if s.endswith(suf) and len(s) > len(suf) + 1:\n",
    "            s = s[:-len(suf)]\n",
    "            break\n",
    "    return s\n",
    "\n",
    "def lexical_consistent(query: str, cand_term: str) -> bool:\n",
    "    q = core_token(query)\n",
    "    c = core_token(cand_term)\n",
    "    if not q or not c:\n",
    "        return False\n",
    "    if q == c:\n",
    "        return True\n",
    "    if len(q) >= 2 and len(c) >= 2 and (q in c or c in q):\n",
    "        return True\n",
    "    return False\n",
    "# =====================\n",
    "# 2. ê¸€ë¡œë²Œ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™”\n",
    "# =====================\n",
    "try:\n",
    "    print(f\"'{EMBEDDING_MODEL_NAME}' ì¿¼ë¦¬ ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "    print(\"âœ… ì¿¼ë¦¬ ìž„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.\")\n",
    "\n",
    "    print(\"Zero-Shot NLI ëª¨ë¸ ë¡œë“œ ì¤‘ (Fallback ì „ìš©)...\")\n",
    "    hf_logging.set_verbosity_error()\n",
    "    try:\n",
    "        import sentencepiece  # noqa: F401\n",
    "    except Exception:\n",
    "        print(\"âš ï¸ 'sentencepiece' íŒ¨í‚¤ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. 'pip install sentencepiece' ê¶Œìž¥(ë©€í‹°ë§êµ¬ì–¼ ëª¨ë¸ì— í•„ìš”)\")\n",
    "\n",
    "    # ì•ˆì „í•œ NLI ë¡œë”(ìˆœì°¨ í´ë°±)\n",
    "    candidates = [\n",
    "        (\"joeddav/xlm-roberta-large-xnli\", False),\n",
    "        (\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\", False),\n",
    "        (\"facebook/bart-large-mnli\", True),  # ì˜ì–´ ì „ìš©(ê¸´ê¸‰ í´ë°±)\n",
    "    ]\n",
    "    last_err = None\n",
    "    nli_pipeline = None\n",
    "    for mid, english_only in candidates:\n",
    "        try:\n",
    "            nli_tokenizer = AutoTokenizer.from_pretrained(mid, use_fast=False)\n",
    "            nli_model = AutoModelForSequenceClassification.from_pretrained(mid)\n",
    "            nli_pipeline = pipeline(\n",
    "                \"zero-shot-classification\",\n",
    "                model=nli_model,\n",
    "                tokenizer=nli_tokenizer,\n",
    "                device=(0 if torch.cuda.is_available() else -1),\n",
    "                hypothesis_template=(\n",
    "                    \"ì´ ì„±ë¶„ì€ {} ì•Œë ˆë¥´ê²(ê³¼)ì— í•´ë‹¹í•œë‹¤.\" if not english_only else \"This ingredient belongs to {} allergen.\"\n",
    "                ),\n",
    "            )\n",
    "            NLI_MODEL_NAME = mid\n",
    "            print(f\"âœ… NLI ëª¨ë¸ ë¡œë“œ: {mid}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ NLI í›„ë³´ ë¡œë“œ ì‹¤íŒ¨({mid}): {e}\")\n",
    "            last_err = e\n",
    "    if nli_pipeline is None:\n",
    "        raise RuntimeError(f\"NLI ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨(ëª¨ë“  í›„ë³´ ì‹¤íŒ¨): {last_err}\")\n",
    "\n",
    "    # NLI í›„ë³´ ë ˆì´ë¸”\n",
    "    ALLERGEN_CANDIDATES = list(ALLERGENS_STD_SET) + [\"ê´€ë ¨ ì—†ìŒ\"]\n",
    "\n",
    "    # GCP Vision í´ë¼ì´ì–¸íŠ¸\n",
    "    print(\"GCP Vision API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...\")\n",
    "    credentials = service_account.Credentials.from_service_account_file(KEY_JSON_PATH)\n",
    "    vision_client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "    print(\"âœ… GCP Vision í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "\n",
    "    # KB ë¡œë“œ + L2 ì •ê·œí™” + ì¤‘ë³µ ì œê±° + í…ìŠ¤íŠ¸/ìš©ì–´ ë§¤í•‘\n",
    "    print(\"ì‚¬ì „ ê³„ì‚°ëœ RAG ì§€ì‹ ë² ì´ìŠ¤ ë¡œë“œ ì¤‘...\")\n",
    "\n",
    "    if not os.path.exists(KB_EMB_PATH) or not os.path.exists(KB_CAT_PATH):\n",
    "        raise FileNotFoundError(f\"KB íŒŒì¼ ëˆ„ë½: {KB_EMB_PATH} ë˜ëŠ” {KB_CAT_PATH}\")\n",
    "\n",
    "    kb_embeddings = np.load(KB_EMB_PATH).astype(np.float32)\n",
    "    kb_embeddings = kb_embeddings / (np.linalg.norm(kb_embeddings, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    with open(KB_CAT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        kb_categories = json.load(f)  # ê¸¸ì´ N\n",
    "\n",
    "    # KB terms/texts í™•ë³´ (ê°€ëŠ¥í•˜ë©´ CSVì—ì„œ)\n",
    "    kb_terms, kb_texts = None, None\n",
    "    if os.path.exists(KB_CSV_PATH):\n",
    "        df_kb = pd.read_csv(KB_CSV_PATH)\n",
    "        term_col = \"term\" if \"term\" in df_kb.columns else df_kb.columns[0]\n",
    "        kb_terms = df_kb[term_col].astype(str).tolist()\n",
    "        if \"description\" in df_kb.columns:\n",
    "            kb_texts = (df_kb[term_col].astype(str) + \" | \" + df_kb[\"description\"].astype(str)).tolist()\n",
    "        else:\n",
    "            kb_texts = kb_terms[:]\n",
    "    else:\n",
    "        kb_terms = [f\"item_{i}\" for i in range(len(kb_categories))]\n",
    "        kb_texts = [str(c) for c in kb_categories]\n",
    "\n",
    "    # ìž„ë² ë”© ì¤‘ë³µ ì œê±° (í•´ì‹œ ê¸°ë°˜) â†’ ê²€ìƒ‰ ì™œê³¡ ë°©ì§€\n",
    "    def _dedup_embs(embs: np.ndarray, terms: list, cats: list, texts: list):\n",
    "        import hashlib\n",
    "        seen, keep = {}, []\n",
    "        arr = np.ascontiguousarray(embs)\n",
    "        for i, row in enumerate(arr):\n",
    "            h = hashlib.sha256(row.view(np.uint8)).hexdigest()\n",
    "            if h not in seen:\n",
    "                seen[h] = True\n",
    "                keep.append(i)\n",
    "        return arr[keep], [terms[i] for i in keep], [cats[i] for i in keep], [texts[i] for i in keep]\n",
    "\n",
    "    kb_embeddings, kb_terms, kb_categories, kb_texts = _dedup_embs(\n",
    "        kb_embeddings, kb_terms, kb_categories, kb_texts\n",
    "    )\n",
    "\n",
    "    print(f\"âœ… KB ë¡œë“œ ì™„ë£Œ (í•­ëª©: {len(kb_categories)}ê°œ, terms:{len(kb_terms)}ê°œ)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ê¸€ë¡œë²Œ ì„¤ì • ì‹¤íŒ¨: {e}\")\n",
    "    raise\n",
    "\n",
    "# =====================\n",
    "# 3. ìƒíƒœ ë° ë…¸ë“œ íƒ€ìž…\n",
    "# =====================\n",
    "class AllergyGraphState(TypedDict):\n",
    "    image_path: str\n",
    "    raw_ocr_text: str\n",
    "    ingredients_to_check: List[str]\n",
    "    current_ingredient: str\n",
    "    rag_result: dict\n",
    "    final_allergens: Set[str]\n",
    "    final_output_json: str\n",
    "\n",
    "# =====================\n",
    "# 4. ë…¸ë“œ êµ¬í˜„\n",
    "# =====================\n",
    "# --- Node 1: OCR ---\n",
    "\n",
    "def call_gcp_vision_api(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 1: call_gcp_vision_api) ---\")\n",
    "    img_path = state.get(\"image_path\", \"\")\n",
    "    print(f\"GCP Vision OCR í˜¸ì¶œ... (ì´ë¯¸ì§€: {img_path})\")\n",
    "    if not img_path or not os.path.exists(img_path):\n",
    "        print(\"âš ï¸ ì´ë¯¸ì§€ ê²½ë¡œê°€ ì—†ê±°ë‚˜ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        return {**state, \"raw_ocr_text\": \"\"}\n",
    "    try:\n",
    "        with io.open(img_path, 'rb') as image_file:\n",
    "            content = image_file.read()\n",
    "        image = vision.Image(content=content)\n",
    "        response = vision_client.text_detection(image=image)\n",
    "        if response.error.message:\n",
    "            raise RuntimeError(f\"GCP API Error: {response.error.message}\")\n",
    "        raw_text = response.full_text_annotation.text\n",
    "        print(f\"âœ… OCR ì„±ê³µ. í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)}\")\n",
    "        return {**state, \"raw_ocr_text\": raw_text}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ GCP Vision ì‹¤íŒ¨: {e}\")\n",
    "        return {**state, \"raw_ocr_text\": \"\"}\n",
    "\n",
    "\n",
    "# --- API íŒŒì„œ A: Gemini Structured Output ---\n",
    "\n",
    "def parse_with_gemini_structured(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    raw_text = state.get(\"raw_ocr_text\", \"\")\n",
    "    if not raw_text.strip():\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    if not _HAS_GEMINI:\n",
    "        print(\"âš ï¸ google-generativeai ë¯¸ì„¤ì¹˜. ë¹ˆ ê²°ê³¼ ë°˜í™˜\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    api_key = _get_gemini_api_key()\n",
    "    if not api_key:\n",
    "        print(\"âš ï¸ Gemini API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”. ë¹ˆ ê²°ê³¼ ë°˜í™˜\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"ingredients_block\": {\"type\": \"string\"},\n",
    "            \"ingredients_list\":  {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"contains_list\":     {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"cross_contamination_lines\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n",
    "        },\n",
    "        \"required\": [\"ingredients_block\", \"ingredients_list\", \"contains_list\", \"cross_contamination_lines\"]\n",
    "    }\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "[ì—­í• ] ë„ˆëŠ” í•œêµ­ ì‹í’ˆí‘œì‹œ ì „ë¬¸ ê°ë¦¬ì›.\n",
    "[ëª©í‘œ] ì•„ëž˜ OCR ì›ë¬¸ì—ì„œë§Œ ì¶”ì¶œí•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜.\n",
    "\n",
    "[ì§€ì‹œ]\n",
    "- 'ì›ìž¬ë£Œëª…' ë¸”ë¡ì„ í•œ ë©ì–´ë¦¬ ë¬¸ìžì—´ë¡œ ê·¸ëŒ€ë¡œ ingredients_blockì— ë„£ì–´ë¼.\n",
    "- ì‰¼í‘œ/êµ¬ë‘ì  ê¸°ì¤€ìœ¼ë¡œ ìž¬ë£Œë¥¼ í† í°í™”í•œ ëª©ë¡ì„ ingredients_listì— ë„£ì–´ë¼.\n",
    "- 'ì•Œë ˆë¥´ê¸° ìœ ë°œë¬¼ì§ˆ', '...í•¨ìœ ', '...í¬í•¨' ë“± í‘œì‹œ ë¼ì¸ì— ë“±ìž¥í•˜ëŠ” í•­ëª©ë“¤ì„ contains_listì— ë„£ì–´ë¼.\n",
    "- 'ê°™ì€ ì œì¡°ì‹œì„¤/êµì°¨ì˜¤ì—¼/í˜¼ìž… ê°€ëŠ¥' ë“± ë¬¸ìž¥ì„ cross_contamination_linesì— ì›ë¬¸ ê·¸ëŒ€ë¡œ ë„£ì–´ë¼.\n",
    "- ì›ë¬¸ì— ì—†ìœ¼ë©´ ë¹ˆ ê°’/ë¹ˆ ë°°ì—´ì„ ë„£ì–´ë¼. ì¶”ì¸¡ ê¸ˆì§€.\n",
    "\n",
    "[OCR ì›ë¬¸]\n",
    "```text\n",
    "{raw_text}\n",
    "```\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_schema\": schema,\n",
    "                \"temperature\": 0,\n",
    "            },\n",
    "        )\n",
    "        data = json.loads(resp.text)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Gemini íŒŒì„œ ì˜¤ë¥˜: {e}\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    def _clean(x: str) -> str:\n",
    "        x = re.sub(r\"\\s+\", \"\", x)\n",
    "        x = x.split(\"(\")[0]\n",
    "        return normalize_to_std(x)\n",
    "\n",
    "    ing_list   = [_clean(s) for s in data.get(\"ingredients_list\", []) if s]\n",
    "    contain_ls = [_clean(s) for s in data.get(\"contains_list\", []) if s]\n",
    "\n",
    "    filtered_ing = [i for i in ing_list if i and not any(i.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "    filtered_con = [c for c in contain_ls if c and not any(c.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "\n",
    "    found = set([s for s in filtered_con if s in ALLERGENS_STD_SET])\n",
    "    queue = sorted(set([*filtered_ing, *filtered_con]))\n",
    "\n",
    "    print(f\"âœ… Gemini íŒŒì‹± ì™„ë£Œ: queue={len(queue)} / pre_found={sorted(found)}\")\n",
    "    return {**state, \"ingredients_to_check\": queue, \"final_allergens\": found}\n",
    "\n",
    "\n",
    "# --- API íŒŒì„œ B: Document AI Custom Extractor ---\n",
    "\n",
    "def parse_with_docai(state: AllergyGraphState,\n",
    "                     project_id: str,\n",
    "                     location: str,\n",
    "                     processor_id: str) -> AllergyGraphState:\n",
    "    if not _HAS_DOCAI:\n",
    "        print(\"âš ï¸ google-cloud-documentai ë¯¸ì„¤ì¹˜. ë¹ˆ ê²°ê³¼ ë°˜í™˜\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    img_path = state.get(\"image_path\", \"\")\n",
    "    try:\n",
    "        client = documentai.DocumentProcessorServiceClient()\n",
    "        name = client.processor_path(project=project_id, location=location, processor=processor_id)\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            raw_doc = documentai.RawDocument(content=f.read(), mime_type=\"image/jpeg\")\n",
    "        req = documentai.ProcessRequest(name=name, raw_document=raw_doc)\n",
    "        result = client.process_document(request=req)\n",
    "        doc = result.document\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Document AI í˜¸ì¶œ ì‹¤íŒ¨: {e}\")\n",
    "        return {**state, \"ingredients_to_check\": [], \"final_allergens\": set()}\n",
    "\n",
    "    ingredients_block = \"\"\n",
    "    ingredients_list, contains_list, cross_lines = [], [], []\n",
    "\n",
    "    for ent in doc.entities:\n",
    "        t = ent.type_\n",
    "        val = (ent.mention_text or \"\").strip()\n",
    "        if   t == \"ingredients_block\":        ingredients_block = val\n",
    "        elif t == \"ingredients_item\":         ingredients_list.append(val)\n",
    "        elif t == \"allergens_contains_item\":  contains_list.append(val)\n",
    "        elif t == \"cross_contamination_line\": cross_lines.append(val)\n",
    "\n",
    "    def _clean(x: str) -> str:\n",
    "        x = re.sub(r\"\\s+\", \"\", x)\n",
    "        x = x.split(\"(\")[0]\n",
    "        return normalize_to_std(x)\n",
    "\n",
    "    ing_list   = [_clean(s) for s in ingredients_list if s]\n",
    "    contain_ls = [_clean(s) for s in contains_list if s]\n",
    "\n",
    "    filtered_ing = [i for i in ing_list if i and not any(i.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "    filtered_con = [c for c in contain_ls if c and not any(c.startswith(k) for k in IGNORE_KEYWORDS)]\n",
    "\n",
    "    found = set([s for s in filtered_con if s in ALLERGENS_STD_SET])\n",
    "    queue = sorted(set([*filtered_ing, *filtered_con]))\n",
    "\n",
    "    print(f\"âœ… Document AI íŒŒì‹± ì™„ë£Œ: queue={len(queue)} / pre_found={sorted(found)}\")\n",
    "    return {**state, \"ingredients_to_check\": queue, \"final_allergens\": found}\n",
    "\n",
    "\n",
    "# --- Node 2: API íŒŒì„œ ë¼ìš°í„° ---\n",
    "\n",
    "def parse_text_via_api(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 2: parse_text_via_api) [API Parser] ---\")\n",
    "    if USE_API_PARSER == \"docai\":\n",
    "        project_id = os.environ.get(\"DOCAI_PROJECT\", \"YOUR_GCP_PROJECT\")\n",
    "        location   = os.environ.get(\"DOCAI_LOCATION\", \"asia-northeast1\")\n",
    "        processor  = os.environ.get(\"DOCAI_PROCESSOR_ID\", \"your-processor-id\")\n",
    "        return parse_with_docai(state, project_id, location, processor)\n",
    "    else:\n",
    "        return parse_with_gemini_structured(state)\n",
    "\n",
    "\n",
    "# --- Node 3: ë£¨í”„ ì»¨íŠ¸ë¡¤ëŸ¬ ---\n",
    "\n",
    "def prepare_next_ingredient(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 3: prepare_next_ingredient) ---\")\n",
    "    queue = list(state.get(\"ingredients_to_check\", []))\n",
    "    if not queue:\n",
    "        print(\"â„¹ï¸ ë‚¨ì€ í•­ëª© ì—†ìŒ\")\n",
    "        return state\n",
    "    nxt = queue.pop(0)\n",
    "    print(f\"ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: '{nxt}' (ë‚¨ì€ {len(queue)}ê°œ)\")\n",
    "    return {**state, \"current_ingredient\": nxt, \"ingredients_to_check\": queue}\n",
    "\n",
    "\n",
    "# --- RAG ì•ˆì „ ê²€ìƒ‰ (top-k + ê°€ë“œë£°) ---\n",
    "\n",
    "def rag_search_topk(query_text: str, k: int = 5, thresh: float = 0.65):\n",
    "    # 0) ë™ì˜ì–´â†’í‘œì¤€: ì§ˆì˜ ìžì²´ê°€ í‘œì¤€ ì•Œë ˆë¥´ê²ì´ë©´ ë°”ë¡œ í™•ì •\n",
    "    std = normalize_to_std(query_text)\n",
    "    if std in ALLERGENS_STD_SET:\n",
    "        return [{\"term\": std, \"category\": std, \"text\": std, \"sim\": 1.0, \"found_by\": \"alias\"}]\n",
    "\n",
    "    # 1) ì¿¼ë¦¬ ìž„ë² ë”©ì€ í•­ìƒ ìƒˆë¡œ ê³„ì‚° (ë¶€ë¶„ ì¼ì¹˜ ìºì‹œ ê¸ˆì§€)\n",
    "    q = embedding_model.encode([query_text], normalize_embeddings=True)\n",
    "    q = np.asarray(q, dtype=np.float32)[0]\n",
    "\n",
    "    # 2) ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (ì •ê·œí™” ê°€ì •)\n",
    "    sims = kb_embeddings @ q  # (N,)\n",
    "\n",
    "    # 3) top-k\n",
    "    k = max(1, min(k, len(sims)))\n",
    "    top_idx = np.argpartition(-sims, kth=k-1)[:k]\n",
    "    top_idx = top_idx[np.argsort(-sims[top_idx])]\n",
    "\n",
    "    results = []\n",
    "    for i in top_idx:\n",
    "        results.append({\n",
    "            \"term\": kb_terms[i],\n",
    "            \"category\": kb_categories[i],\n",
    "            \"text\": kb_texts[i],\n",
    "            \"sim\": float(sims[i]),\n",
    "            \"found_by\": \"rag\"\n",
    "        })\n",
    "\n",
    "    if not results:\n",
    "        return [{\"term\": None, \"category\": \"ì—†ìŒ\", \"text\": \"\", \"sim\": 0.0, \"found_by\": \"none\"}]\n",
    "\n",
    "    # 4) ê·¹ë‹¨ê°’ ë³´ì •: 0.99 ì´ìƒì¸ë°ë„ ìš©ì–´ê°€ ë‹¤ë¥´ë©´ ì‚´ì§ ê°•ë“±\n",
    "    r0 = results[0]\n",
    "    if r0[\"sim\"] >= 0.99:\n",
    "        if normalize_to_std(r0[\"term\"]) != std and r0[\"term\"] != query_text:\n",
    "            r0[\"sim\"] = r0[\"sim\"] - 0.05\n",
    "            results = sorted(results, key=lambda x: -x[\"sim\"])\n",
    "            r0 = results[0]\n",
    "\n",
    "    # 5) **ìš©ì–´ ì¼ì¹˜ì„± ê°€ë“œ**: í•µì‹¬ì–´ê°€ ë‹¤ë¥´ë©´ 'ì—†ìŒ'ìœ¼ë¡œ ì°¨ë‹¨\n",
    "    if not lexical_consistent(query_text, r0[\"term\"]):\n",
    "        return [{\"term\": None, \"category\": \"ì—†ìŒ\", \"text\": \"\", \"sim\": float(r0[\"sim\"]), \"found_by\": \"lex_guard\"}]\n",
    "\n",
    "    # 6) ìž„ê³„ì¹˜ ë¯¸ë‹¬ì´ë©´ 'ì—†ìŒ'\n",
    "    if r0[\"sim\"] < thresh:\n",
    "        return [{\"term\": None, \"category\": \"ì—†ìŒ\", \"text\": \"\", \"sim\": float(r0[\"sim\"]), \"found_by\": \"below_thresh\"}]\n",
    "\n",
    "    return results[:k]\n",
    "\n",
    "\n",
    "# --- Node 4: RAG ê²€ìƒ‰ ---\n",
    "\n",
    "def rag_search(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"--- (Node 4: rag_search) ---\")\n",
    "    ingredient = state.get(\"current_ingredient\", \"\")\n",
    "\n",
    "    cand_list = rag_search_topk(ingredient, k=5, thresh=0.65)\n",
    "    top = cand_list[0]\n",
    "\n",
    "    found = top[\"category\"]\n",
    "    conf  = float(top[\"sim\"])\n",
    "    by    = top.get(\"found_by\")\n",
    "    print(f\"RAG ê²€ìƒ‰: '{ingredient}' â†’ '{found}' (ìœ ì‚¬ë„ {conf:.4f}, by={by})\")\n",
    "\n",
    "    return {**state, \"rag_result\": {\"confidence\": conf, \"found_allergen\": found}}\n",
    "\n",
    "\n",
    "# --- Node 5: LLM Fallback (Zero-Shot) ---\n",
    "\n",
    "def llm_fallback(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"--- (Node 5: llm_fallback) [NLI Zero-Shot] ---\")\n",
    "    ingredient = state.get(\"current_ingredient\", \"\")\n",
    "    try:\n",
    "        resp = nli_pipeline(ingredient, list(ALLERGENS_STD_SET) + [\"ê´€ë ¨ ì—†ìŒ\"])\n",
    "        top_label, top_score = resp['labels'][0], float(resp['scores'][0])\n",
    "        print(f\"NLI ì‘ë‹µ: Label='{top_label}', Score={top_score:.4f}\")\n",
    "        if top_label in ALLERGENS_STD_SET and top_score >= NLI_FALLBACK_THRESHOLD:\n",
    "            return {**state, \"rag_result\": {\"confidence\": top_score, \"found_allergen\": top_label}}\n",
    "        return {**state, \"rag_result\": {\"confidence\": 1.0, \"found_allergen\": \"ì—†ìŒ\"}}\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ NLI Fallback ì˜¤ë¥˜: {e}\")\n",
    "        return {**state, \"rag_result\": {\"confidence\": 1.0, \"found_allergen\": \"ì˜¤ë¥˜\"}}\n",
    "\n",
    "\n",
    "# --- Node 6: ê²°ê³¼ ì·¨í•© ---\n",
    "\n",
    "def update_final_list(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"--- (Node 6: update_final_list) ---\")\n",
    "    result_allergen = state.get(\"rag_result\", {}).get(\"found_allergen\", \"\")\n",
    "    if result_allergen in ALLERGENS_STD_SET:\n",
    "        s = set(state.get(\"final_allergens\", set()))\n",
    "        s.add(result_allergen)\n",
    "        print(f\"âœ… ìœ íš¨ ì•Œë ˆë¥´ê¸° ì¶”ê°€: '{result_allergen}' â†’ {sorted(s)}\")\n",
    "        return {**state, \"final_allergens\": s}\n",
    "    print(f\"â„¹ï¸ í‘œì¤€ ì•Œë ˆë¥´ê¸° ì•„ë‹˜ ë˜ëŠ” 'ì—†ìŒ': '{result_allergen}' (ë¬´ì‹œ)\")\n",
    "    return state\n",
    "\n",
    "\n",
    "# --- Node 7: ì¢…ë£Œ ---\n",
    "\n",
    "def finalize_processing(state: AllergyGraphState) -> AllergyGraphState:\n",
    "    print(\"\\n--- (Node 7: finalize_processing) ---\")\n",
    "    final_set = set(state.get(\"final_allergens\", set()))\n",
    "    final_list = sorted(list(final_set))\n",
    "    final_json = json.dumps(final_list, ensure_ascii=False)\n",
    "    print(f\"ðŸŽ‰ ìµœì¢… ê²°ê³¼: {final_json}\")\n",
    "    return {**state, \"final_output_json\": final_json}\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 5. ì—£ì§€(Edge) ë¼ìš°í„°\n",
    "# =====================\n",
    "\n",
    "def route_after_parse(state: AllergyGraphState) -> str:\n",
    "    if state.get(\"ingredients_to_check\"):\n",
    "        return \"has_ingredients\"\n",
    "    return \"no_ingredients\"\n",
    "\n",
    "\n",
    "def route_rag_result(state: AllergyGraphState) -> str:\n",
    "    conf = state.get(\"rag_result\", {}).get(\"confidence\", 0.0)\n",
    "    allergen = state.get(\"rag_result\", {}).get(\"found_allergen\", \"\")\n",
    "\n",
    "    # 'ì—†ìŒ'ì´ë©´ í´ë°± ë¶ˆí•„ìš” â†’ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ(ì¶”ê°€ ì•ˆ ë˜ê³  ë„˜ì–´ê°)\n",
    "    if allergen == \"ì—†ìŒ\":\n",
    "        print(\"  -> [RAG ê²°ê³¼ ì—†ìŒ] update_final_list (í´ë°± ìƒëžµ)\")\n",
    "        return \"rag_success\"\n",
    "\n",
    "    if conf >= RAG_CONFIDENCE_THRESHOLD and allergen in ALLERGENS_STD_SET:\n",
    "        print(\"  -> [RAG ì„±ê³µ] update_final_list\")\n",
    "        return \"rag_success\"\n",
    "\n",
    "    print(\"  -> [RAG ë¶ˆí™•ì‹¤] llm_fallback\")\n",
    "    return \"needs_llm_fallback\"\n",
    "\n",
    "\n",
    "def check_remaining_ingredients(state: AllergyGraphState) -> str:\n",
    "    if state.get(\"ingredients_to_check\"):\n",
    "        print(\"  -> [í•­ëª© ë‚¨ìŒ] prepare_next_ingredient\")\n",
    "        return \"has_more_ingredients\"\n",
    "    print(\"  -> [í•­ëª© ì—†ìŒ] finalize_processing\")\n",
    "    return \"all_ingredients_done\"\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 6. ê·¸ëž˜í”„ ë¹Œë“œ\n",
    "# =====================\n",
    "print(\"\\n--- LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œìž‘ ---\")\n",
    "workflow = StateGraph(AllergyGraphState)\n",
    "\n",
    "# ë…¸ë“œ ë“±ë¡\n",
    "workflow.add_node(\"call_gcp_vision_api\", call_gcp_vision_api)\n",
    "workflow.add_node(\"parse_text_via_api\", parse_text_via_api)\n",
    "workflow.add_node(\"prepare_next_ingredient\", prepare_next_ingredient)\n",
    "workflow.add_node(\"rag_search\", rag_search)\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback)\n",
    "workflow.add_node(\"update_final_list\", update_final_list)\n",
    "workflow.add_node(\"finalize_processing\", finalize_processing)\n",
    "\n",
    "# ì—£ì§€ ì—°ê²°\n",
    "workflow.set_entry_point(\"call_gcp_vision_api\")\n",
    "workflow.add_edge(\"call_gcp_vision_api\", \"parse_text_via_api\")\n",
    "\n",
    "# parse â†’ ì¡°ê±´ë¶€ ë¶„ê¸°\n",
    "workflow.add_conditional_edges(\n",
    "    \"parse_text_via_api\",\n",
    "    route_after_parse,\n",
    "    {\"has_ingredients\": \"prepare_next_ingredient\", \"no_ingredients\": \"finalize_processing\"}\n",
    ")\n",
    "\n",
    "# ë£¨í”„ ë³¸ì²´\n",
    "workflow.add_edge(\"prepare_next_ingredient\", \"rag_search\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"rag_search\",\n",
    "    route_rag_result,\n",
    "    {\"rag_success\": \"update_final_list\", \"needs_llm_fallback\": \"llm_fallback\"}\n",
    ")\n",
    "workflow.add_edge(\"llm_fallback\", \"update_final_list\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"update_final_list\",\n",
    "    check_remaining_ingredients,\n",
    "    {\"has_more_ingredients\": \"prepare_next_ingredient\", \"all_ingredients_done\": \"finalize_processing\"}\n",
    ")\n",
    "workflow.add_edge(\"finalize_processing\", END)\n",
    "\n",
    "# ì»´íŒŒì¼\n",
    "app = workflow.compile()\n",
    "print(\"--- âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ ---\")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 7. ë””ë²„ê·¸/ê²€ì¦ ìœ í‹¸ (ì„ íƒ)\n",
    "# =====================\n",
    "\n",
    "def kb_self_check(max_show: int = 5):\n",
    "    \"\"\"ì¤‘ë³µ ìž„ë² ë”© ê·¸ë£¹/ìƒ˜í”Œ í‘œì‹œ\"\"\"\n",
    "    import hashlib\n",
    "    groups = {}\n",
    "    arr = np.ascontiguousarray(kb_embeddings)\n",
    "    for i, row in enumerate(arr):\n",
    "        h = hashlib.sha256(row.view(np.uint8)).hexdigest()\n",
    "        groups.setdefault(h, []).append(i)\n",
    "    dup_groups = {h:idxs for h,idxs in groups.items() if len(idxs) > 1}\n",
    "    print(f\"[SELF-CHECK] ì¤‘ë³µ ìž„ë² ë”© ê·¸ë£¹ ìˆ˜: {len(dup_groups)}\")\n",
    "    for h, idxs in list(dup_groups.items())[:max_show]:\n",
    "        names = [kb_terms[i] for i in idxs]\n",
    "        cats  = [kb_categories[i] for i in idxs]\n",
    "        print(f\"  - size={len(idxs)} | terms={names[:5]} | cats={cats[:5]}\")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# 8. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì˜ˆì‹œ)\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- [Test Run: GCP OCR + API Parser + RAG + NLI] ---\")\n",
    "\n",
    "    # (ì„ íƒ) KB ì¤‘ë³µ ì²´í¬\n",
    "    try:\n",
    "        kb_self_check()\n",
    "    except Exception as e:\n",
    "        print(f\"[SELF-CHECK] ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    # ì˜ˆì‹œ ì´ë¯¸ì§€ ê²½ë¡œ\n",
    "    test_image = os.environ.get(\"ALLER_GUARD_TEST_IMAGE\", r\"C:\\\\Users\\\\MYNOTE\\\\AllerGuard\\\\Data\\\\ê¹€ê´‘ë¬´_118.jpg\")\n",
    "    if not os.path.exists(test_image):\n",
    "        print(f\"âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {test_image}\")\n",
    "    test_input = {\"image_path\": test_image}\n",
    "\n",
    "    try:\n",
    "        final_state = app.invoke(test_input, {\"recursion_limit\": 1000})\n",
    "        print(\"\\nìµœì¢… ë°˜í™˜ JSON:\")\n",
    "        print(final_state.get('final_output_json', ''))\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
