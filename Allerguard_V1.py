# --- 0. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import os
import pandas as pd
import numpy as np  # .npy íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•´ í•„ìš”
import io
import json
import re  # ğŸ‘ˆ ì •ê·œì‹(Regex) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from typing import List, Set, TypedDict

# ë¨¸ì‹ ëŸ¬ë‹ ë° ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline # (ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ìœ ì§€)
from google.cloud import vision
from google.oauth2 import service_account  # ğŸ‘ˆ ì§ì ‘ ì¸ì¦ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬
from langgraph.graph import StateGraph, END
from dotenv import load_dotenv # ğŸ‘ˆ .env íŒŒì¼ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸

# for llm library
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()


print("--- ğŸš€ ì•Œë ˆë¥´ê¸° ë¶„ì„ ì„œë¹„ìŠ¤ (GCP Vision API + RAG + LLM Fallback) ì‹œì‘ ---")
print("ì‚¬ì „ ë¹Œë“œëœ ì„ë² ë”© ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

# --- 0a. í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡ ì •ì˜ ---
ALLERGENS_STD_SET = set([
    "ì•Œë¥˜", "ìš°ìœ ", "ë©”ë°€", "ë•…ì½©", "ëŒ€ë‘", "ë°€", "ì£", "í˜¸ë‘",
    "ê²Œ", "ìƒˆìš°", "ì˜¤ì§•ì–´", "ê³ ë“±ì–´", "ì¡°ê°œë¥˜", "ë³µìˆ­ì•„", "í† ë§ˆí† ",
    "ë‹­ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì‡ ê³ ê¸°", "ì•„í™©ì‚°ë¥˜"
])
print(f"âœ… í‘œì¤€ ì•Œë ˆë¥´ê¸° ì¹´í…Œê³ ë¦¬ {len(ALLERGENS_STD_SET)}ê°œ ë¡œë“œ ì™„ë£Œ.")

# --- 0b. ë¹„-ì„±ë¶„ í‚¤ì›Œë“œ í•„í„° ëª©ë¡ (Node 2 ìˆ˜ì •ìš©) ---
IGNORE_KEYWORDS = set([
    'ì—´ëŸ‰', 'íƒ„ìˆ˜í™”ë¬¼', 'ë‹¨ë°±ì§ˆ', 'ì§€ë°©', 'ë‹¹ë¥˜', 'ë‚˜íŠ¸ë¥¨', 'ì½œë ˆìŠ¤í…Œë¡¤',
    'í¬í™”ì§€ë°©', 'íŠ¸ëœìŠ¤ì§€ë°©', 'ë‚´ìš©ëŸ‰', 'I', 'II' # (ë¹ˆ ë¬¸ìì—´ '' ì œê±°ëœ ìƒíƒœ)
])
print(f"âœ… ë¹„-ì„±ë¶„ í•„í„° í‚¤ì›Œë“œ {len(IGNORE_KEYWORDS)}ê°œ ë¡œë“œ ì™„ë£Œ.")


template_for_extract = """
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ ë‚´ìš©ì„ í™•ì¸í•´ì„œ ì›ì¬ë£Œëª…ê³¼ í˜¼ì…ê°€ëŠ¥ í˜¹ì€ ê°™ì€ì œì¡°ì‹œì„¤ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¬ë£Œë¥¼ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.
ì›ì¬ë£Œëª… ë‚´ì—ì„œ ì¤‘ë³µë˜ëŠ” í•­ëª©ì€ ì œê±°í•˜ê³  í•´ë‹¹ë˜ëŠ” ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'ì—†ìŒ' ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”
í˜¼ì…ê°€ëŠ¥ í˜¹ì€ ê°™ì€ì œì¡°ì‹œì„¤ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¬ë£Œì¤‘ ì¤‘ë³µë˜ëŠ” í•­ëª©ì€ ì œê±°í•˜ê³  í•´ë‹¹ë˜ëŠ” ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'ì—†ìŒ' ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”
ì¬ë£Œí•œê°œë‹¹ í•œì¤„ì— ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ìŒ JSON í¬ë§· ì— ë§ê²Œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

----
ì›ë¬¸ í…ìŠ¤íŠ¸:
{raw_text}
----

----
[JSON í¬ë§·]
{{
  "ì›ì¬ë£Œëª…": [],
  "í˜¼ì…ê°€ëŠ¥": []
}}

í˜¼ì…ê°€ëŠ¥,ê°™ì€ì œì¡°ì‹œì„¤:
----

#Answer:
"""

template_for_allergen = """
ì£¼ì–´ì§„ ì›ë¬¸ í…ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ì‹ì¬ë£Œ ì„±ë¶„ì— ì•ŒëŸ¬ì§€ ìœ ë°œ ê°€ëŠ¥ì„±ë¶„ ìˆìœ¼ë©´ 'ìˆìŒ', ì—†ë‹¤ë©´ 'ì—†ìŒ' ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”
'ìˆìŒ','ì—†ìŒ' ì´ì™¸ì˜ ë‹µë³€ì€ í•˜ì§€ë§ˆì„¸ìš”

----
ì›ë¬¸ í…ìŠ¤íŠ¸:
{raw_text}
----

#Answer:
"""

def text_parser_by_llm(raw_text):
    res = chain_for_extract.invoke({"raw_text":raw_text})
    print(f"text_parser_by_llm result =>\n{res.content}")

    ingredient_queue = []
    found_allergens_set = set()


    # json.loads() í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë¬¸ìì—´ì„ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    json_data = json.loads(res.content)    
    #json_data = json.loads(test_ing)     # for testing
    
    
    # ë”•ì…”ë„ˆë¦¬ì˜ í‚¤-ê°’ ìŒì„ ë°˜ë³µí•©ë‹ˆë‹¤.
    for key, value in json_data.items():
        # 'ì—†ìŒ'ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ì¬ë£Œë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
        filtered_ingredients = [ingredient for ingredient in value if ingredient != "ì—†ìŒ"]

        # ë§Œì•½ í•„í„°ë§ëœ ì¬ë£Œ ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šë‹¤ë©´ ì¶œë ¥í•©ë‹ˆë‹¤.
        if filtered_ingredients:
            print(f"* {key} : ")
            for ingredient in filtered_ingredients:
                if ingredient not in IGNORE_KEYWORDS:
                    ingredient_queue.append(ingredient) 
                
                if ingredient in ALLERGENS_STD_SET:
                    print(f"  -> '{ingredient}'ì€(ëŠ”) í‘œì¤€ ì•Œë ˆë¥´ê¸°ì´ë¯€ë¡œ final_setì— ì§ì ‘ ì¶”ê°€.")
                    found_allergens_set.add(ingredient) 
                
    
    return ingredient_queue, found_allergens_set


# --- 1. ê¸€ë¡œë²Œ ì„¤ì •: ëª¨ë¸ ë¡œë“œ ë° RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ---
try:
    # 1a. RAG ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì¿¼ë¦¬ ì„ë² ë”©ìš©)
    EMBEDDING_MODEL_NAME = 'distiluse-base-multilingual-cased-v1'
    print(f"'{EMBEDDING_MODEL_NAME}' ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print("âœ… ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    # 1b. Zero-Shot NLI ëª¨ë¸ ë¡œë“œ (Fallback ì „ìš©) - [T5ì—ì„œ êµì²´ë¨]
    print("Zero-Shot NLI ëª¨ë¸ ë¡œë“œ ì¤‘ (Fallback ì „ìš©)...")
    NLI_MODEL_NAME = "klue/roberta-base"
    
    # ì´ì „ ë¡œê·¸ì—ì„œ CUDA ì‚¬ìš©ì´ í™•ì¸ë˜ì—ˆìœ¼ë¯€ë¡œ device=0 (GPU) ì„¤ì •
    nli_pipeline = pipeline("zero-shot-classification", model=NLI_MODEL_NAME, device=0) 
    print(f"âœ… Zero-Shot NLI ëª¨ë¸ ({NLI_MODEL_NAME}) ë¡œë“œ ì™„ë£Œ.")
    
    # NLI Fallbackì´ ì‚¬ìš©í•  í›„ë³´ ë ˆì´ë¸” ëª©ë¡ (ê¸€ë¡œë²Œ ìºì‹œ)
    ALLERGEN_CANDIDATES = list(ALLERGENS_STD_SET) + ["ê´€ë ¨ ì—†ìŒ"]
    print(f"âœ… NLI Fallback í›„ë³´ ë ˆì´ë¸” {len(ALLERGEN_CANDIDATES)}ê°œ ì¤€ë¹„ ì™„ë£Œ.")

    # 1c. GCP Vision API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì§ì ‘ ê²½ë¡œ ì§€ì • ë°©ì‹)
    print("GCP Vision API í´ë¼ì´ì–¸íŠ¸ (ì§ì ‘ ê²½ë¡œ ì§€ì •) ì´ˆê¸°í™” ì¤‘...")
    KEY_JSON_PATH = os.getenv("GCP_KEY_JSON_PATH")
    try:
        credentials = service_account.Credentials.from_service_account_file(KEY_JSON_PATH)
        vision_client = vision.ImageAnnotatorClient(credentials=credentials)
        print(f"âœ… GCP Vision í´ë¼ì´ì–¸íŠ¸ (ì§ì ‘ ê²½ë¡œ) ì¤€ë¹„ ì™„ë£Œ.")
        
    except FileNotFoundError:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì§€ì •ëœ í‚¤ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   ì‹œë„í•œ ê²½ë¡œ: {KEY_JSON_PATH}")
        raise 
    
    # 1d. ì‚¬ì „ ê³„ì‚°ëœ ë²¡í„° ìºì‹œ ë¡œë“œ
    print("ì‚¬ì „ ê³„ì‚°ëœ RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ì¤‘...")
    kb_embeddings = np.load("kb_embeddings.npy") 
    
    with open("kb_categories.json", "r", encoding="utf-8") as f:
            kb_categories = json.load(f) 
    
    print(f"âœ… RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ì™„ë£Œ ({len(kb_categories)}ê°œ í•­ëª©)")
    
    
    prompt_for_extract = PromptTemplate.from_template(template_for_extract)
    print("prompt_for_extract=",prompt_for_extract)


    llm = ChatOpenAI(
        temperature=0,
        model_name="gpt-4.1",  # ëª¨ë¸ëª…
    )

    # chain ìƒì„±
    chain_for_extract = prompt_for_extract | llm    
    
    print(f"âœ… LLM chain ìƒì„± ì™„ë£Œ)")

except Exception as e:
    print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ê¸€ë¡œë²Œ ì„¤ì • ì‹¤íŒ¨: {e}")
    exit()


# RAG ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  ìœ ì‚¬ë„ ì„ê³„ê°’ (íŠœë‹ëœ ê°’ìœ¼ë¡œ ê°€ì •, ì˜ˆ: 0.85)
# (ë§Œì•½ 0.9ë¡œ ìœ ì§€í•˜ê³  NLIê°€ 'ìœ ì²­ë‹¨ë°±ë¶„ë§'ì„ ì¡ë„ë¡ í•˜ë ¤ë©´ 0.9ë¡œ ì„¤ì •í•˜ì„¸ìš”)
RAG_CONFIDENCE_THRESHOLD = 0.85
print(f"â„¹ï¸ RAG ì‹ ë¢°ë„ ì„ê³„ê°’: {RAG_CONFIDENCE_THRESHOLD}")

# NLI Fallback ê²°ê³¼ê°€ ìœ íš¨í•˜ë‹¤ê³  ì¸ì •í•  ìµœì†Œ ì ìˆ˜
NLI_FALLBACK_THRESHOLD = 0.5  
print(f"â„¹ï¸ NLI Fallback ì‹ ë¢°ë„ ì„ê³„ê°’: {NLI_FALLBACK_THRESHOLD}")

# --- 2. LangGraph ìƒíƒœ ì •ì˜ ---
class AllergyGraphState(TypedDict):
    image_path: str
    raw_ocr_text: str
    ingredients_to_check: List[str]
    current_ingredient: str
    rag_result: dict
    final_allergens: Set[str]
    final_output_json: str
    # ì¶”ê°€ë¶€ë¶„
    final_error_msg: List[str]  # ì—ëŸ¬ë©”ì‹œì§€ìš©
    text_parser: str

# --- 3. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---

def append_error_msg(state: AllergyGraphState, new_error_message):
    existing_errors = state.get("final_error_msg", [])
    
    existing_errors.append(new_error_message)
    
    return existing_errors

def call_gcp_vision_api(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 1 (Entry Point): GCP Vision API í˜¸ì¶œ
    """
    print(f"\n--- (Node 1: call_gcp_vision_api) ---")
    img_path = state['image_path']
    print(f"GCP Vision API í˜¸ì¶œ... (ì´ë¯¸ì§€: {img_path})")
    try:
        with io.open(img_path, 'rb') as image_file:
            content = image_file.read()
        image = vision.Image(content=content)
        
        response = vision_client.text_detection(image=image)
        if response.error.message:
            raise Exception(f"GCP API Error: {response.error.message}")

        raw_text = response.full_text_annotation.text
        print(f"âœ… GCP OCR ì„±ê³µ. (ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)}=ì¶”ì¶¡í…ìŠ¤íŠ¸[{raw_text}])")
        return {**state, "raw_ocr_text": raw_text}
    
    except Exception as e:
        print(f"âŒ GCP Vision API ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {**state, "raw_ocr_text": ""}
    
    
def text_parser_by_regex(raw_text):
    clean_text = raw_text.replace("\n", " ")

    ingredient_queue = []
    found_allergens_set = set()

    match1 = re.search(r"ì›ì¬ë£Œëª…[ :](.*?)(â€¢|\||ì˜ì–‘ì •ë³´|ì˜ì–‘ì„±ë¶„|$)", clean_text)
    
    if match1:
        ingredient_blob = match1.group(1).strip()
        raw_ingredients_list = [item.strip() for item in ingredient_blob.split(',') if item.strip()]
        cleaned_ingredients_raw = [name.split('(')[0].strip() for name in raw_ingredients_list if name.strip()]
        
        cleaned_ingredients_filtered = []
        for item in cleaned_ingredients_raw:
            is_noise = False
            for keyword in IGNORE_KEYWORDS:
                if item.startswith(keyword):
                    is_noise = True
                    print(f"  -> í•„í„°ë§ë¨: '{item}' (ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ '{keyword}'ë¡œ ì‹œì‘í•˜ë¯€ë¡œ ì œì™¸)")
                    break 
            
            if not is_noise:
                cleaned_ingredients_filtered.append(item)
        
        ingredient_queue.extend(cleaned_ingredients_filtered)
        print(f"âœ… Regex íŒŒì„œ: 'ì›ì¬ë£Œëª…' ì„¹ì…˜ì—ì„œ {len(cleaned_ingredients_filtered)}ê°œ ì„±ë¶„ ì¶”ì¶œ: {cleaned_ingredients_filtered}")
    
    else:
        print("â„¹ï¸ Regex íŒŒì„œ: 'ì›ì¬ë£Œëª…' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨.")

    match2 = re.search(r"â€¢?\s*([\w,]+)\s+í•¨ìœ ", clean_text)
    if match2:
        contains_blob = match2.group(1) 
        contains_list = [item.strip() for item in contains_blob.split(',') if item.strip()]
        print(f"âœ… Regex íŒŒì„œ: '...í•¨ìœ ' ì„¹ì…˜ì—ì„œ {len(contains_list)}ê°œ í•­ëª© ì¶”ì¶œ: {contains_list}")
        
        for item in contains_list:
            if item not in IGNORE_KEYWORDS:
                ingredient_queue.append(item) 
            
            if item in ALLERGENS_STD_SET:
                print(f"  -> '{item}'ì€(ëŠ”) í‘œì¤€ ì•Œë ˆë¥´ê¸°ì´ë¯€ë¡œ final_setì— ì§ì ‘ ì¶”ê°€.")
                found_allergens_set.add(item) 
    else:
        print("â„¹ï¸ Regex íŒŒì„œ: '...í•¨ìœ ' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨.")
    
    return ingredient_queue, found_allergens_set


def parse_text_from_raw(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 2 (Regex íŒŒì„œ ë…¸ë“œ)
    (startswith í•„í„° ë¡œì§ì´ ì ìš©ëœ ìµœì¢… ìˆ˜ì • ë²„ì „)
    """
    text_parser = state.get('text_parser', None)
        
    print(f"\n--- (Node 2: parse_text_from_raw) ==> [{text_parser}] ---")
    raw_text = state['raw_ocr_text']
    if not raw_text or not raw_text.strip():
        error_message = "â„¹ï¸ OCR í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ íŒŒì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤."
        print(error_message)
        return {**state, 
                "ingredients_to_check": [], 
                "final_allergens": set(),
                "final_error_msg":append_error_msg(state, error_message),
                }

    params = (raw_text, )
    
    if not text_parser:
        print(f"\n--- (Node 2: parse_text_from_raw) [Regex Parser] ---")
        ingredient_queue, found_allergens_set = text_parser_by_regex(*params)
    elif text_parser in globals():
        print(f"\n--- (Node 2: parse_text_from_raw) [{text_parser}] ---")
        ingredient_queue, found_allergens_set = globals()[text_parser](*params)
    else:
        error_message = f"ì„ íƒí•œ text_parser={text_parser}ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        print(error_message)
        return {**state, 
                "ingredients_to_check": [], 
                "final_allergens": set(),
                "final_error_msg":append_error_msg(state, error_message),
                }


    final_queue = sorted(list(set(ingredient_queue)))
    print(f"==> ìµœì¢… RAG ê²€ì‚¬ í (ì¤‘ë³µì œê±°, {len(final_queue)}ê°œ): {final_queue}")
    
    return {
        **state,
        "ingredients_to_check": final_queue,      
        "final_allergens": found_allergens_set 
    }


def prepare_next_ingredient(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 3 (ë£¨í”„ ì»¨íŠ¸ë¡¤ëŸ¬)
    """
    print(f"\n--- (Node 3: prepare_next_ingredient) ---")
    queue = state['ingredients_to_check']
    next_ingredient = queue.pop(0) 
    print(f"ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: '{next_ingredient}' (ë‚¨ì€ í•­ëª©: {len(queue)}ê°œ)")
    return {
        **state,
        "current_ingredient": next_ingredient,
        "ingredients_to_check": queue
    }

def rag_search(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 4 (í•µì‹¬ RAG ê²€ìƒ‰ ë…¸ë“œ)
    """
    print(f"--- (Node 4: rag_search) ---")
    ingredient = state['current_ingredient']
    
    query_embedding = embedding_model.encode([ingredient])
    similarities = cosine_similarity(query_embedding, kb_embeddings)
    
    best_match_index = np.argmax(similarities[0])
    confidence_score = float(similarities[0][best_match_index])
    
    found_allergen = kb_categories[best_match_index] 
    
    print(f"RAG ê²€ìƒ‰: '{ingredient}' (ìœ ì‚¬ë„: {confidence_score:.4f}) -> ë§¤í•‘: '{found_allergen}'")
    
    rag_result_data = {
        "confidence": confidence_score,
        "found_allergen": found_allergen
    }
    return {**state, "rag_result": rag_result_data}


# ==============================================================================
# === ğŸ’¥ [êµì²´ëœ ë…¸ë“œ 5] (Zero-Shot NLI íŒŒì´í”„ë¼ì¸ ë²„ì „) ğŸ’¥ ===
# ==============================================================================
def llm_fallback(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 5 (LLM Fallback ë…¸ë“œ) - [NLI Zero-Shot ë²„ì „]
    
    RAGê°€ ì‹¤íŒ¨í•œ í•­ëª©ì„ Zero-Shot Classification íŒŒì´í”„ë¼ì¸(NLI ëª¨ë¸)ìœ¼ë¡œ ë„˜ê¹ë‹ˆë‹¤.
    ì…ë ¥ ì„±ë¶„ì„ ëª¨ë“  ì•Œë ˆë¥´ê¸° í›„ë³´ ë ˆì´ë¸”ê³¼ ë¹„êµí•˜ì—¬ ìµœê³  ì ìˆ˜(entailment)ë¥¼ ë°›ì€ í•­ëª©ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"--- (Node 5: llm_fallback) [NLI Zero-Shot] ---")
    ingredient = state['current_ingredient']
    print(f"NLI Fallback: '{ingredient}' ë¶„ë¥˜ ìš”ì²­... (í›„ë³´: {len(ALLERGEN_CANDIDATES)}ê°œ)")

    try:
        # NLI íŒŒì´í”„ë¼ì¸ í˜¸ì¶œ (ê¸€ë¡œë²Œ íŒŒì´í”„ë¼ì¸ 'nli_pipeline' ë° í›„ë³´ ë¦¬ìŠ¤íŠ¸ 'ALLERGEN_CANDIDATES' ì¬ì‚¬ìš©)
        response = nli_pipeline(ingredient, ALLERGEN_CANDIDATES) 
        
        # ê°€ì¥ ì ìˆ˜ê°€ ë†’ì€ ë ˆì´ë¸”ê³¼ ì ìˆ˜ë¥¼ ì¶”ì¶œ
        top_label = response['labels'][0]
        top_score = response['scores'][0]
        
        print(f"NLI ì‘ë‹µ: Label='{top_label}', Score={top_score:.4f}")

        # ìµœê³  ì ìˆ˜ ë ˆì´ë¸”ì´ í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡(SET)ì— ìˆëŠ”ì§€ í™•ì¸
        if top_label in ALLERGENS_STD_SET: 
            # í•´ë‹¹ ì ìˆ˜ê°€ ìš°ë¦¬ê°€ ì„¤ì •í•œ NLI ì„ê³„ê°’(ì˜ˆ: 0.5)ë³´ë‹¤ ë†’ì€ì§€ í™•ì¸
            if top_score >= NLI_FALLBACK_THRESHOLD:
                 print(f"  -> ìœ íš¨í•œ ë¶„ë¥˜: '{top_label}' (Score: {top_score}, ì„ê³„ê°’ {NLI_FALLBACK_THRESHOLD} í†µê³¼).")
                 return {**state, "rag_result": {"confidence": top_score, "found_allergen": top_label}}
            else:
                 # ì•Œë ˆë¥´ê¸°ì´ê¸´ í•˜ì§€ë§Œ, ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ì•„ì„œ ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ
                 print(f"  -> ì ìˆ˜ê°€ ë‚®ìŒ ({top_score} < {NLI_FALLBACK_THRESHOLD}). 'ì—†ìŒ'ìœ¼ë¡œ ì²˜ë¦¬.")
                 return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì—†ìŒ"}}
        else:
            # ìµœê³  ì ìˆ˜ ë ˆì´ë¸”ì´ "ê´€ë ¨ ì—†ìŒ"ì´ê±°ë‚˜, (í˜¹ì‹œ ëª¨ë¥¼) ë‹¤ë¥¸ ì“°ë ˆê¸° ê°’ì¸ ê²½ìš°
            print(f"  -> ìµœê³  ì ìˆ˜ ë ˆì´ë¸”ì´ '{top_label}'ì´ë¯€ë¡œ 'ì—†ìŒ'ìœ¼ë¡œ ì²˜ë¦¬.")
            return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì—†ìŒ"}}
            
    except Exception as e:
        print(f"âŒ NLI Fallback ì¤‘ ì˜¤ë¥˜: {e}")
        return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì˜¤ë¥˜"}}
    
# ==============================================================================
# === ğŸ’¥ [ìƒˆë¡œ ì¶”ê°€ëœ ë…¸ë“œ 5b] (ì›¹ ê²€ìƒ‰ ë° ì§€ì‹ ë² ì´ìŠ¤ í™•ì¥) ğŸ’¥ ===
# ==============================================================================
from googleapiclient.discovery import build
def search_and_update_kb(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 5b: [ì‹ ê·œ ì•Œë ˆë¥´ê¸° íƒì§€] ê²€ìƒ‰ ì—”ì§„ì„ í™œìš©í•´ ì‹ ê·œ ì„±ë¶„ì„ ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ê³  KBë¥¼ í™•ì¥í•©ë‹ˆë‹¤. (LLM ë¯¸ì‚¬ìš©)
    """
    print(f"\n--- (Node 5b: find_new_allergens_and_update_kb) [New Allergen Detection] ---")
    ingredient = state['current_ingredient']
    RAG_KNOWLEDGE_BASE_CSV = "domestic_allergy_rag_knowledge_1000.csv"
    
    # â¬‡ï¸â¬‡ï¸â¬‡ï¸ [ì¤‘ìš”] ì—¬ê¸°ì— ì§ì ‘ ë°œê¸‰ë°›ìœ¼ì‹  API í‚¤ì™€ ê²€ìƒ‰ ì—”ì§„ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”! â¬‡ï¸â¬‡ï¸â¬‡ï¸
    API_KEY = os.getenv("GOOGLE_API_KEY")
    SEARCH_ENGINE_ID = os.getenv("GOOGLE_CSE_ID")

    print(f"'{ingredient}' ì„±ë¶„ì˜ ìƒìœ„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")

    # âœ¨ [ê°œì„  1] KBì—ì„œ ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ë¯¸ë¦¬ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    try:
        df = pd.read_csv(RAG_KNOWLEDGE_BASE_CSV)
        if ingredient in df['term'].values:
            print(f" Â -> '{ingredient}'ì€(ëŠ”) ì´ë¯¸ ì§€ì‹ ë² ì´ìŠ¤ì— ì¡´ì¬í•©ë‹ˆë‹¤. íƒì§€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return state
        
        # ì¤‘ë³µì„ ì œê±°í•œ ì „ì²´ ì¹´í…Œê³ ë¦¬ ëª©ë¡ í™•ë³´
        existing_categories = df['category'].unique().tolist()

    except FileNotFoundError:
        print(f" Â -> ì§€ì‹ ë² ì´ìŠ¤ íŒŒì¼ '{RAG_KNOWLEDGE_BASE_CSV}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ì–´, ë¶„ë¥˜ë¥¼ ì§„í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return state

    # âœ¨ [ê°œì„  2] LLM ëŒ€ì‹ , ê° ì¹´í…Œê³ ë¦¬ì™€ ì¡°í•©í•˜ì—¬ ì—°ê´€ì„±ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    found_category = None
    service = build("customsearch", "v1", developerKey=API_KEY)

    for category in existing_categories:
        try:
            # ì¢€ ë” ì •í™•í•œ ì—°ê´€ì„±ì„ ì°¾ê¸° ìœ„í•´ 'ì›ë£Œ', 'ìœ ë˜' ë“±ì˜ í‚¤ì›Œë“œë¥¼ í•¨ê»˜ ê²€ìƒ‰
            search_query = f"'{ingredient}' '{category}' ì›ë£Œ ìœ ë˜"
            print(f" Â -> '{category}' ì¹´í…Œê³ ë¦¬ì™€ì˜ ì—°ê´€ì„±ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤... (ì¿¼ë¦¬: {search_query})")
            
            response = service.cse().list(q=search_query, cx=SEARCH_ENGINE_ID, num=1).execute()
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ì´ ìˆë‹¤ê³  íŒë‹¨
            if response.get('items'):
                print(f" Â -> ë¶„ì„ ê²°ê³¼: '{ingredient}'ì€(ëŠ”) '{category}' ì¹´í…Œê³ ë¦¬ì™€ ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
                found_category = category
                break # ê°€ì¥ ë¨¼ì € ì°¾ì•„ë‚¸ ì¹´í…Œê³ ë¦¬ë¡œ í™•ì •í•˜ê³  ë£¨í”„ ì¢…ë£Œ
        
        except Exception as e:
            print(f" Â -> ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì¹´í…Œê³ ë¦¬: {category}): {e}")
            continue # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ì— ì‹¤íŒ¨í•´ë„ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ë¡œ ê³„ì† ì§„í–‰

    # âœ¨ [ê°œì„  3] ì°¾ì•„ë‚¸ ì¹´í…Œê³ ë¦¬ê°€ ìˆì„ ê²½ìš°ì—ë§Œ KBë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    if found_category:
        description = f"{ingredient}ì€(ëŠ”) {found_category}ì— í•´ë‹¹í•˜ëŠ” ì„±ë¶„ì…ë‹ˆë‹¤."
        
        try:
            new_entry_df = pd.DataFrame([{
                "term": ingredient,
                "category": found_category,
                "description": description
            }])
            new_entry_df.to_csv(RAG_KNOWLEDGE_BASE_CSV, mode='a', header=False, index=False, encoding='utf-8-sig')
            print(f"âœ… ì§€ì‹ ë² ì´ìŠ¤ '{RAG_KNOWLEDGE_BASE_CSV}'ì— '{ingredient}' -> '{found_category}' ì •ë³´ ì¶”ê°€ ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ì— ì“°ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        print(f" Â -> ë¶„ì„ ê²°ê³¼: '{ingredient}'ì„(ë¥¼) ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
    return state


# --- 4. LangGraph ì—£ì§€(Edge) í•¨ìˆ˜ ì •ì˜ ---

# (ê¸°ì¡´ route_rag_result í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ ë‘¡ë‹ˆë‹¤)

# ==============================================================================
# === ğŸ’¥ [ìƒˆë¡œ ì¶”ê°€ëœ ì¡°ê±´ë¶€ ì—£ì§€] (Fallback ë¼ìš°í„°) ğŸ’¥ ===
# ==============================================================================
def route_fallback_result(state: AllergyGraphState) -> str:
    """(ì¡°ê±´ë¶€ ì—£ì§€ 3: Fallback ë¼ìš°í„°)
    NLI Fallbackì˜ ê²°ê³¼ì— ë”°ë¼ ë¶„ê¸°í•©ë‹ˆë‹¤.
    - ê²°ê³¼ê°€ ìœ íš¨í•œ ì•Œë ˆë¥´ê¸°ì¸ ê²½ìš°: 'update_final_list'ë¡œ ì´ë™í•˜ì—¬ ìµœì¢… ëª©ë¡ì— ì¶”ê°€
    - ê²°ê³¼ê°€ 'ì—†ìŒ' ë˜ëŠ” 'ì˜¤ë¥˜'ì¸ ê²½ìš°: 'search_and_update_kb'ë¡œ ì´ë™í•˜ì—¬ ì›¹ ê²€ìƒ‰ ì‹œë„
    """
    print(f"--- (Edge: route_fallback_result?) ---")
    fallback_allergen = state['rag_result']['found_allergen']
    
    if fallback_allergen in ALLERGENS_STD_SET:
        print(f"  -> [Fallback ì„±ê³µ]. 'update_final_list'ë¡œ ì´ë™.")
        return "allergen_found"
    else: # 'ì—†ìŒ', 'ì˜¤ë¥˜' ë“±ì˜ ê²½ìš°
        print(f"  -> [Fallback ê²°ê³¼ ë¶ˆí™•ì‹¤]. 'search_and_update_kb'ë¡œ ì´ë™í•˜ì—¬ ì›¹ ê²€ìƒ‰.")
        return "perform_web_search"



def update_final_list(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 6 (ê²°ê³¼ ì·¨í•© ë…¸ë“œ)
    """
    print(f"--- (Node 6: update_final_list) ---")
    result_allergen = state['rag_result']['found_allergen']
    
    if result_allergen in ALLERGENS_STD_SET:
        current_set = state['final_allergens']
        print(f"âœ… ìœ íš¨í•œ ì•Œë ˆë¥´ê¸° ë°œê²¬: '{result_allergen}'. ìµœì¢… ëª©ë¡ì— ì¶”ê°€.")
        current_set.add(result_allergen)
        return {**state, "final_allergens": current_set}
    else:
        print(f"â„¹ï¸ '{result_allergen}'ì€(ëŠ”) í‘œì¤€ ì•Œë ˆë¥´ê¸° í•­ëª©ì´ ì•„ë‹ˆë¯€ë¡œ ë¬´ì‹œí•©ë‹ˆë‹¤.")
        return state 

def finalize_processing(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 7 (ì¢…ë£Œ ë…¸ë“œ)
    """
    print(f"\n--- (Node 7: finalize_processing) ---")
    final_set = state['final_allergens']
    
    final_list = sorted(list(final_set))
    final_json = json.dumps(final_list, ensure_ascii=False)
    
    print(f"ğŸ‰ ëª¨ë“  ì„±ë¶„ ê²€ì‚¬ ì™„ë£Œ. ìµœì¢… ê²°ê³¼: {final_json}")
    return {**state, "final_output_json": final_json}


# --- 4. LangGraph ì—£ì§€(Edge) í•¨ìˆ˜ ì •ì˜ ---

def route_rag_result(state: AllergyGraphState) -> str:
    """(ì¡°ê±´ë¶€ ì—£ì§€ 1: RAG ë¼ìš°í„°)
    """
    print(f"--- (Edge: route_rag_result?) ---")
    confidence = state['rag_result']['confidence']
    allergen = state['rag_result']['found_allergen']
    
    if confidence >= RAG_CONFIDENCE_THRESHOLD and allergen in ALLERGENS_STD_SET:
        print(f"  -> [RAG ì„±ê³µ]. 'update_final_list'ë¡œ ì´ë™.")
        return "rag_success"
    else:
        print(f"  -> [RAG ì‹¤íŒ¨/ë¶ˆí™•ì‹¤]. 'llm_fallback'ìœ¼ë¡œ ì´ë™.")
        return "needs_llm_fallback"

def check_remaining_ingredients(state: AllergyGraphState) -> str:
    """(ì¡°ê±´ë¶€ ì—£ì§€ 2: ë£¨í”„ ì œì–´)
    """
    print(f"--- (Edge: check_remaining_ingredients?) ---")
    
    if state["ingredients_to_check"] and len(state["ingredients_to_check"]) > 0:
        print(f"  -> [í•­ëª© ë‚¨ìŒ]. 'prepare_next_ingredient'ë¡œ ë£¨í”„.")
        return "has_more_ingredients"
    else:
        print("  -> [í•­ëª© ì—†ìŒ]. 'finalize_processing'ë¡œ ì´ë™.")
        return "all_ingredients_done"
    
def decide_after_parsing(state: AllergyGraphState) -> str:
    """
    âœ… (ìƒˆë¡œìš´ ì¡°ê±´ë¶€ ì—£ì§€) íŒŒì‹± ì§í›„, ì²˜ë¦¬í•  ì„±ë¶„ì´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    - ì„±ë¶„ì´ ìˆìœ¼ë©´: 'prepare_next_ingredient'ë¡œ ì´ë™í•˜ì—¬ ë£¨í”„ ì‹œì‘
    - ì„±ë¶„ì´ ì—†ìœ¼ë©´: 'finalize_processing'ìœ¼ë¡œ ë°”ë¡œ ì´ë™í•˜ì—¬ ì¢…ë£Œ
    """
    print(f"--- (Edge: decide_after_parsing?) ---")
    if state["ingredients_to_check"] and len(state["ingredients_to_check"]) > 0:
        print(f" Â -> [ì„±ë¶„ ëª©ë¡ ìˆìŒ]. ì²˜ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
        return "process_ingredients"
    else:
        print(" Â -> [ì„±ë¶„ ëª©ë¡ ì—†ìŒ]. ì²˜ë¦¬ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return "skip_to_end"

# --- 5. ê·¸ë˜í”„ ë¹Œë“œ ë° ì»´íŒŒì¼ ---

print("\n--- LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘ ---")

workflow = StateGraph(AllergyGraphState)

# --- 5. ê·¸ë˜í”„ ë¹Œë“œ ë° ì»´íŒŒì¼ (ì •ë¦¬ëœ ìµœì¢… ë²„ì „) ---

print("\n--- LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘ ---")

workflow = StateGraph(AllergyGraphState)

# 1. ëª¨ë“  ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ë¨¼ì € ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("call_gcp_vision_api", call_gcp_vision_api)
workflow.add_node("parse_text_from_raw", parse_text_from_raw)
workflow.add_node("prepare_next_ingredient", prepare_next_ingredient)
workflow.add_node("rag_search", rag_search)
workflow.add_node("llm_fallback", llm_fallback)
workflow.add_node("search_and_update_kb", search_and_update_kb)
workflow.add_node("update_final_list", update_final_list) 
workflow.add_node("finalize_processing", finalize_processing)

# 2. ì§„ì…ì (Entry Point)ì„ ì„¤ì •í•©ë‹ˆë‹¤.
workflow.set_entry_point("call_gcp_vision_api")

# 3. ê° ë…¸ë“œ ê°„ì˜ ì—£ì§€(íë¦„)ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
workflow.add_edge("call_gcp_vision_api", "parse_text_from_raw")
# ë¬´ì¡°ê±´ ì ì¸ ì—£ì§€ ì‚­ì œ
# workflow.add_edge("parse_text_from_raw", "prepare_next_ingredient")

workflow.add_conditional_edges(
    "parse_text_from_raw",
    decide_after_parsing, # ë°©ê¸ˆ ì¶”ê°€í•œ í•¨ìˆ˜ ì‚¬ìš©
    {
        "process_ingredients": "prepare_next_ingredient", # ì„±ë¶„ì´ ìˆìœ¼ë©´ ë‹¤ìŒ ì„±ë¶„ ì¤€ë¹„ë¡œ
        "skip_to_end": "finalize_processing"             # ì„±ë¶„ì´ ì—†ìœ¼ë©´ ë°”ë¡œ ì¢…ë£Œë¡œ
    }
)

workflow.add_edge("prepare_next_ingredient", "rag_search")

# 4. RAG ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "rag_search",
    route_rag_result,
    {"rag_success": "update_final_list", "needs_llm_fallback": "llm_fallback"}
)

# 5. LLM Fallback ê²°ê³¼ì— ë”°ë¥¸ ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "llm_fallback",
    route_fallback_result,
    {"allergen_found": "update_final_list", "perform_web_search": "search_and_update_kb"}
)

# 6. ì›¹ ê²€ìƒ‰ ë…¸ë“œ ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì·¨í•© ë…¸ë“œë¡œ ì—°ê²°í•©ë‹ˆë‹¤.
workflow.add_edge("search_and_update_kb", "update_final_list")

# 7. ë©”ì¸ ë£¨í”„ë¥¼ ìœ„í•œ ì¡°ê±´ë¶€ ë¶„ê¸°ë¥¼ ì—°ê²°í•©ë‹ˆë‹¤. (ëª¨ë“  ì¬ë£Œë¥¼ ë‹¤ ê²€ì‚¬í–ˆëŠ”ì§€ í™•ì¸)
workflow.add_conditional_edges(
    "update_final_list",
    check_remaining_ingredients,
    {"has_more_ingredients": "prepare_next_ingredient", "all_ingredients_done": "finalize_processing"}
)

# 8. ìµœì¢… ë…¸ë“œë¥¼ ê·¸ë˜í”„ì˜ ë(END)ê³¼ ì—°ê²°í•©ë‹ˆë‹¤.
workflow.add_edge("finalize_processing", END)

# 9. ê·¸ë˜í”„ë¥¼ ìµœì¢… ì»´íŒŒì¼í•©ë‹ˆë‹¤.
app = workflow.compile()
print("--- âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ ---")

# --- 9. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---
print("\n\n--- [Test Run: GCP API + Regex íŒŒì„œ + NLI Fallback ê¸°ë°˜ ì‹¤í–‰] ---")


# from langchain_teddynote.graphs import visualize_graph

# # ê·¸ë˜í”„ ì‹œê°í™”
# visualize_graph(app)

# (í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ íŒŒì¼ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤)
# my_test_image_file = "image.jpg" # ğŸ‘ˆ 'image.jpg'ëŠ” OCR ë¡œê·¸ë¥¼ ì œê³µí•œ ê·¸ ì´ë¯¸ì§€ íŒŒì¼ ê°€ì •

# if my_test_image_file:
#     test_input = {"image_path": my_test_image_file}
#     print(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘: {my_test_image_file}\n")

#     print("\n--- [Test Run: ìµœì¢… ê²°ê³¼ (invoke)] ---")
#     final_state = app.invoke(test_input, {"recursion_limit": 100}) 
#     print("\nìµœì¢… ë°˜í™˜ JSON:")
#     print(final_state['final_output_json'])

# else:

#     print("\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê±´ë„ˆëœ€: 'my_test_image_file' ë³€ìˆ˜ì— ì´ë¯¸ì§€ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
