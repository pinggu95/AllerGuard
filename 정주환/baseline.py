# --- 0. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ---
import os
import pandas as pd
import numpy as np  # .npy íŒŒì¼ ë¡œë“œë¥¼ ìœ„í•´ í•„ìš”
import io
import json
import re  # ğŸ‘ˆ ì •ê·œì‹(Regex) ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from typing import List, Set, TypedDict

# ë¨¸ì‹ ëŸ¬ë‹ ë° ëª¨ë¸ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from google.cloud import vision
from google.oauth2 import service_account  # ğŸ‘ˆ ì§ì ‘ ì¸ì¦ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

# LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬
from langgraph.graph import StateGraph, END

print("--- ğŸš€ ì•Œë ˆë¥´ê¸° ë¶„ì„ ì„œë¹„ìŠ¤ (GCP Vision API + RAG + LLM Fallback) ì‹œì‘ ---")
print("ì‚¬ì „ ë¹Œë“œëœ ì„ë² ë”© ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

# --- 0a. í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡ ì •ì˜ ---
ALLERGENS_STD_SET = set([
    "ì•Œë¥˜", "ìš°ìœ ", "ë©”ë°€", "ë•…ì½©", "ëŒ€ë‘", "ë°€", "ì£", "í˜¸ë‘",
    "ê²Œ", "ìƒˆìš°", "ì˜¤ì§•ì–´", "ê³ ë“±ì–´", "ì¡°ê°œë¥˜", "ë³µìˆ­ì•„", "í† ë§ˆí† ",
    "ë‹­ê³ ê¸°", "ë¼ì§€ê³ ê¸°", "ì‡ ê³ ê¸°", "ì•„í™©ì‚°ë¥˜"
])
print(f"âœ… í‘œì¤€ ì•Œë ˆë¥´ê¸° ì¹´í…Œê³ ë¦¬ {len(ALLERGENS_STD_SET)}ê°œ ë¡œë“œ ì™„ë£Œ.")

# --- 0b. ë¹„-ì„±ë¶„ í‚¤ì›Œë“œ í•„í„° ëª©ë¡ (Node 2 ìˆ˜ì •ìš©) ---
# íŒŒì„œê°€ ì‹¤ìˆ˜ë¡œ ì¶”ì¶œí•œ ì˜ì–‘ì •ë³´ ë˜ëŠ” OCR ë…¸ì´ì¦ˆë¥¼ RAG íì—ì„œ ì œì™¸í•˜ê¸° ìœ„í•œ í•„í„°
IGNORE_KEYWORDS = set([
    'ì—´ëŸ‰', 'íƒ„ìˆ˜í™”ë¬¼', 'ë‹¨ë°±ì§ˆ', 'ì§€ë°©', 'ë‹¹ë¥˜', 'ë‚˜íŠ¸ë¥¨', 'ì½œë ˆìŠ¤í…Œë¡¤',
    'í¬í™”ì§€ë°©', 'íŠ¸ëœìŠ¤ì§€ë°©', 'ë‚´ìš©ëŸ‰', 'I', 'II' # <-- ë¹ˆ ë¬¸ìì—´ '' ì œê±°ë¨
])
print(f"âœ… ë¹„-ì„±ë¶„ í•„í„° í‚¤ì›Œë“œ {len(IGNORE_KEYWORDS)}ê°œ ë¡œë“œ ì™„ë£Œ.")


# --- 1. ê¸€ë¡œë²Œ ì„¤ì •: ëª¨ë¸ ë¡œë“œ ë° RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ---
# (ì•± ì‹¤í–‰ ì‹œ ë‹¨ 1íšŒ ìˆ˜í–‰. ëª¨ë“  ëª¨ë¸ê³¼ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•©ë‹ˆë‹¤.)
try:
    # 1a. RAG ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì¿¼ë¦¬ ì„ë² ë”©ìš©)
    EMBEDDING_MODEL_NAME = 'distiluse-base-multilingual-cased-v1'
    print(f"'{EMBEDDING_MODEL_NAME}' ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    print("âœ… ì¿¼ë¦¬ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    # 1b. T5 ëª¨ë¸ ë¡œë“œ (Fallback ìš©ë„ë¡œë§Œ ì‚¬ìš©ë¨)
    print("T5 ëª¨ë¸ ë¡œë“œ ì¤‘ (Fallback ì „ìš©)...")
    t5_model_id = "paust/pko-t5-small"
    t5_tok = AutoTokenizer.from_pretrained(t5_model_id)
    t5_mdl = AutoModelForSeq2SeqLM.from_pretrained(t5_model_id)
    t5_pipeline = pipeline("text2text-generation", model=t5_mdl, tokenizer=t5_tok, max_new_tokens=64)
    print("âœ… T5 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")

    # 1c. GCP Vision API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì§ì ‘ ê²½ë¡œ ì§€ì • ë°©ì‹)
    print("GCP Vision API í´ë¼ì´ì–¸íŠ¸ (ì§ì ‘ ê²½ë¡œ ì§€ì •) ì´ˆê¸°í™” ì¤‘...")
    
    # ì‚¬ìš©ìë‹˜ì˜ PCì— ìˆëŠ” í‚¤ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œ (í•œê¸€ ê²½ë¡œ í¬í•¨)
    KEY_JSON_PATH = r"C:\Users\ì •ì£¼í™˜\Desktop\keyfolder\nlp-study-467306-563e76afdbca.json"

    try:
        # 1. ì§€ì •ëœ ê²½ë¡œì—ì„œ ì„œë¹„ìŠ¤ ê³„ì • íŒŒì¼ë¡œ 'ì¸ì¦ì •ë³´(credentials)' ê°ì²´ë¥¼ ì§ì ‘ ìƒì„±
        credentials = service_account.Credentials.from_service_account_file(KEY_JSON_PATH)
        
        # 2. í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œ ì´ 'ì¸ì¦ì •ë³´'ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì£¼ì…(inject)
        vision_client = vision.ImageAnnotatorClient(credentials=credentials)
        
        print(f"âœ… GCP Vision í´ë¼ì´ì–¸íŠ¸ (ì§ì ‘ ê²½ë¡œ) ì¤€ë¹„ ì™„ë£Œ.")
        
    except FileNotFoundError:
        print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ì§€ì •ëœ í‚¤ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   ì‹œë„í•œ ê²½ë¡œ: {KEY_JSON_PATH}")
        raise # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•± ì¤‘ì§€
    
    # 1d. ì‚¬ì „ ê³„ì‚°ëœ ë²¡í„° ìºì‹œ ë¡œë“œ
    print("ì‚¬ì „ ê³„ì‚°ëœ RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ì¤‘...")
    kb_embeddings = np.load("kb_embeddings.npy") # Numpy ë°°ì—´ì„ íŒŒì¼ì—ì„œ ë°”ë¡œ ë¡œë“œ
    
    with open("kb_categories.json", "r", encoding="utf-8") as f:
            kb_categories = json.load(f) # ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ë¥¼ JSONì—ì„œ ë¡œë“œ
    
    print(f"âœ… RAG ì§€ì‹ ë² ì´ìŠ¤ ìºì‹œ ë¡œë“œ ì™„ë£Œ ({len(kb_categories)}ê°œ í•­ëª©)")

except Exception as e:
    print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ê¸€ë¡œë²Œ ì„¤ì • ì‹¤íŒ¨: {e}")
    # ì„¤ì •ì´ ì‹¤íŒ¨í•˜ë©´ ì•±ì„ ì‹¤í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¢…ë£Œ
    exit()


# RAG ê²€ìƒ‰ ì‹œ ì‚¬ìš©í•  ìœ ì‚¬ë„ ì„ê³„ê°’
RAG_CONFIDENCE_THRESHOLD = 0.85

# --- 2. LangGraph ìƒíƒœ ì •ì˜ ---
class AllergyGraphState(TypedDict):
    """
    ê·¸ë˜í”„ ì „ì²´ë¥¼ ìˆœíšŒí•˜ëŠ” ì¤‘ì•™ ìƒíƒœ ì €ì¥ì†Œ(State)ì…ë‹ˆë‹¤.
    ëª¨ë“  ë…¸ë“œëŠ” ì´ Stateë¥¼ ì½ê³ , ìì‹ ì˜ ì‘ì—… ê²°ê³¼ë¥¼ ì´ Stateì— ë‹¤ì‹œ ì”ë‹ˆë‹¤.
    """
    image_path: str                # ê·¸ë˜í”„ ìµœì´ˆ ì…ë ¥ (ì´ë¯¸ì§€ ê²½ë¡œ)
    raw_ocr_text: str              # GCP OCRì´ ë°˜í™˜í•œ ì›ë³¸ í…ìŠ¤íŠ¸
    ingredients_to_check: List[str]  # íŒŒì‹±ëœ í›„, ê²€ì‚¬ë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì„±ë¶„ ëª©ë¡ (í)
    current_ingredient: str        # í˜„ì¬ ë£¨í”„ì—ì„œ ê²€ì‚¬ ì¤‘ì¸ ë‹¨ì¼ ì„±ë¶„
    rag_result: dict               # RAG ë˜ëŠ” LLM ë…¸ë“œì˜ ì²˜ë¦¬ ê²°ê³¼
    final_allergens: Set[str]      # ìµœì¢… ë°œê²¬ëœ í‘œì¤€ ì•Œë ˆë¥´ê¸° (ì¤‘ë³µ ì œê±°ìš© Set)
    final_output_json: str         # ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ë  ìµœì¢… JSON ë¬¸ìì—´


# --- 3. LangGraph ë…¸ë“œ í•¨ìˆ˜ ì •ì˜ ---

def call_gcp_vision_api(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 1 (Entry Point): GCP Vision API í˜¸ì¶œ
    
    Stateì˜ 'image_path'ë¥¼ ë°›ì•„ GCP Vision APIë¥¼ í˜¸ì¶œí•˜ê³ ,
    ì¶”ì¶œëœ ì „ì²´ í…ìŠ¤íŠ¸ ë¸”ë¡ì„ 'raw_ocr_text' ìƒíƒœë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    """
    print(f"\n--- (Node 1: call_gcp_vision_api) ---")
    img_path = state['image_path']
    print(f"GCP Vision API í˜¸ì¶œ... (ì´ë¯¸ì§€: {img_path})")
    try:
        # ì´ë¯¸ì§€ íŒŒì¼ì„ ë°”ì´ë„ˆë¦¬ë¡œ ì½ê¸°
        with io.open(img_path, 'rb') as image_file:
            content = image_file.read()
        image = vision.Image(content=content)
        
        # í…ìŠ¤íŠ¸ ê°ì§€(text_detection) API í˜¸ì¶œ
        response = vision_client.text_detection(image=image)
        if response.error.message:
            raise Exception(f"GCP API Error: {response.error.message}")

        # ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ ë¸”ë¡ìœ¼ë¡œ ê°€ì ¸ì˜´
        raw_text = response.full_text_annotation.text
        print(f"âœ… GCP OCR ì„±ê³µ. (ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_text)})")
        return {**state, "raw_ocr_text": raw_text}
    
    except Exception as e:
        print(f"âŒ GCP Vision API ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {**state, "raw_ocr_text": ""}


def parse_text_from_raw(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 2 (Regex íŒŒì„œ ë…¸ë“œ)
    
    'raw_ocr_text'ë¥¼ ì…ë ¥ë°›ì•„ ì •ê·œì‹(Regex)ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    1. 'ì›ì¬ë£Œëª…:' ì„¹ì…˜ì—ì„œ ì„±ë¶„ ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì˜ˆ: 'ë°€ê°€ë£¨', 'ì¹˜ì¦ˆë¶„ë§')
    2. '...í•¨ìœ ' ì„¹ì…˜ì—ì„œ ê²½ê³  ëª©ë¡ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. (ì˜ˆ: 'ë°€', 'ìš°ìœ ')
    
    'í•¨ìœ ' ëª©ë¡ì˜ í‘œì¤€ ì•Œë ˆë¥´ê¸°ëŠ” 'final_allergens' Setì— ë¯¸ë¦¬ ì¶”ê°€í•˜ê³ ,
    ë‘ ëª©ë¡ì˜ ëª¨ë“  ì„±ë¶„(ì¤‘ë³µ ì œê±°)ì„ 'ingredients_to_check' íë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"\n--- (Node 2: parse_text_from_raw) [Regex Parser] ---")
    raw_text = state['raw_ocr_text']
    if not raw_text or not raw_text.strip():
        print("â„¹ï¸ OCR í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì–´ íŒŒì‹±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {**state, "ingredients_to_check": [], "final_allergens": set()}

    # ì •ê·œì‹ ì²˜ë¦¬ë¥¼ ì‰½ê²Œ í•˜ê¸° ìœ„í•´ ëª¨ë“  ê°œí–‰ë¬¸ì(\n)ë¥¼ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    clean_text = raw_text.replace("\n", " ")

    ingredient_queue = []      # RAGê°€ ê²€ì‚¬í•  ëª¨ë“  ì„±ë¶„ í›„ë³´ í
    found_allergens_set = set() # ìµœì¢… ê²°ê³¼ë¥¼ ëˆ„ì í•  Set ì´ˆê¸°í™”

    # 1. "ì›ì¬ë£Œëª…:" ì„¹ì…˜ ì¶”ì¶œ (ì˜ì–‘ì •ë³´ ì„¹ì…˜ ì „ê¹Œì§€ë§Œ ì½ë„ë¡ Regex ìˆ˜ì •ë¨)
    match1 = re.search(r"ì›ì¬ë£Œëª…[ :](.*?)(â€¢|\||ì˜ì–‘ì •ë³´|ì˜ì–‘ì„±ë¶„|$)", clean_text)
    
    if match1:
        ingredient_blob = match1.group(1).strip() # "ë°€ê°€ë£¨(ë°€:ë¯¸êµ­ì‚°), ê°€ê³µìœ ì§€..."
        raw_ingredients_list = [item.strip() for item in ingredient_blob.split(',') if item.strip()]
        
        # ì„±ë¶„ ì´ë¦„ë§Œ ì •ë¦¬ (ì˜ˆ: "ë°€ê°€ë£¨(ë°€:ë¯¸êµ­ì‚°)" -> "ë°€ê°€ë£¨")
        cleaned_ingredients_raw = [name.split('(')[0].strip() for name in raw_ingredients_list if name.strip()]
        
        # 'startswith' í•„í„°ë§ ë¡œì§ (ë…¸ì´ì¦ˆ ì²­í¬ ì œê±°)
        cleaned_ingredients_filtered = []
        for item in cleaned_ingredients_raw:
            is_noise = False
            for keyword in IGNORE_KEYWORDS:
                if item.startswith(keyword):  # IGNORE_KEYWORDS í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ë©´
                    is_noise = True
                    print(f"  -> í•„í„°ë§ë¨: '{item}' (ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ '{keyword}'ë¡œ ì‹œì‘í•˜ë¯€ë¡œ ì œì™¸)")
                    break  # ë…¸ì´ì¦ˆ í™•ì¸ ì‹œ ë‚´ë¶€ ë£¨í”„ íƒˆì¶œ
            
            if not is_noise:
                cleaned_ingredients_filtered.append(item) # ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ í•­ëª©ë§Œ ì¶”ê°€
        
        ingredient_queue.extend(cleaned_ingredients_filtered)
        print(f"âœ… Regex íŒŒì„œ: 'ì›ì¬ë£Œëª…' ì„¹ì…˜ì—ì„œ {len(cleaned_ingredients_filtered)}ê°œ ì„±ë¶„ ì¶”ì¶œ: {cleaned_ingredients_filtered}")
    
    else:
        print("â„¹ï¸ Regex íŒŒì„œ: 'ì›ì¬ë£Œëª…' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨.")

    # 2. "... í•¨ìœ " ì„¹ì…˜ì—ì„œ ëª¨ë“  ì•Œë ˆë¥´ê¸° ì§ì ‘ ì¶”ì¶œ
    match2 = re.search(r"â€¢?\s*([\w,]+)\s+í•¨ìœ ", clean_text)
    if match2:
        contains_blob = match2.group(1) # "ë°€,ìš°ìœ ,ëŒ€ë‘,ì‡ ê³ ê¸°"
        contains_list = [item.strip() for item in contains_blob.split(',') if item.strip()]
        print(f"âœ… Regex íŒŒì„œ: '...í•¨ìœ ' ì„¹ì…˜ì—ì„œ {len(contains_list)}ê°œ í•­ëª© ì¶”ì¶œ: {contains_list}")
        
        for item in contains_list:
            if item not in IGNORE_KEYWORDS: # (í•¨ìœ  ëª©ë¡ì—ë„ ì•ˆì „ í•„í„° ì ìš©)
                ingredient_queue.append(item) 
            
            if item in ALLERGENS_STD_SET: # í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡ì— ìˆë‹¤ë©´
                print(f"  -> '{item}'ì€(ëŠ”) í‘œì¤€ ì•Œë ˆë¥´ê¸°ì´ë¯€ë¡œ final_setì— ì§ì ‘ ì¶”ê°€.")
                found_allergens_set.add(item) # ìµœì¢… ëª©ë¡ì— ë¯¸ë¦¬ ì¶”ê°€
    else:
        print("â„¹ï¸ Regex íŒŒì„œ: '...í•¨ìœ ' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í•¨.")

    # 3. ìµœì¢… í ìƒì„± (ì¤‘ë³µ ì œê±°)
    final_queue = sorted(list(set(ingredient_queue)))
    print(f"==> ìµœì¢… RAG ê²€ì‚¬ í (ì¤‘ë³µì œê±°, {len(final_queue)}ê°œ): {final_queue}")
    
    return {
        **state,
        "ingredients_to_check": final_queue,      
        "final_allergens": found_allergens_set 
    }


def prepare_next_ingredient(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 3 (ë£¨í”„ ì»¨íŠ¸ë¡¤ëŸ¬)
    
    'ingredients_to_check' í(Queue)ì—ì„œ ì„±ë¶„ì„ í•˜ë‚˜ì”© êº¼ë‚´ì–´(pop)
    'current_ingredient' ìƒíƒœì— ì„¤ì •í•©ë‹ˆë‹¤.
    """
    print(f"\n--- (Node 3: prepare_next_ingredient) ---")
    queue = state['ingredients_to_check']
    next_ingredient = queue.pop(0) # íì˜ ë§¨ ì•ì—ì„œ í•˜ë‚˜ êº¼ëƒ„
    print(f"ë‹¤ìŒ ê²€ì‚¬ ëŒ€ìƒ: '{next_ingredient}' (ë‚¨ì€ í•­ëª©: {len(queue)}ê°œ)")
    return {
        **state,
        "current_ingredient": next_ingredient, # í˜„ì¬ ê²€ì‚¬í•  ëŒ€ìƒ ì„¤ì •
        "ingredients_to_check": queue          # í•˜ë‚˜ê°€ ì œê±°ëœ íë¡œ ìƒíƒœ ì—…ë°ì´íŠ¸
    }

def rag_search(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 4 (í•µì‹¬ RAG ê²€ìƒ‰ ë…¸ë“œ)
    
    'current_ingredient'ë¥¼ ì„ë² ë”©í•˜ê³ , ë©”ëª¨ë¦¬ì— ë¡œë“œëœ KB ë²¡í„° ì „ì²´ì™€ ë¹„êµí•©ë‹ˆë‹¤.
    ê°€ì¥ ìœ ì‚¬ë„(confidence)ê°€ ë†’ì€ í•­ëª©ì˜ ë§¤í•‘ëœ ì•Œë ˆë¥´ê¸° ê°’ì„ 'rag_result' ìƒíƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print(f"--- (Node 4: rag_search) ---")
    ingredient = state['current_ingredient']
    
    # 1. ì¿¼ë¦¬(ì„±ë¶„ 1ê°œ) ì„ë² ë”© ìƒì„± (ì‹¤ì‹œê°„)
    query_embedding = embedding_model.encode([ingredient])
    
    # 2. KB(702ê°œ) ë²¡í„° ì „ì²´ì™€ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = cosine_similarity(query_embedding, kb_embeddings)
    
    # 3. ìµœê³  ì ìˆ˜(argmax)ì˜ ì¸ë±ìŠ¤ íƒìƒ‰
    best_match_index = np.argmax(similarities[0])
    confidence_score = float(similarities[0][best_match_index])
    
    # 4. í•´ë‹¹ ì¸ë±ìŠ¤ì˜ ì•Œë ˆë¥´ê¸° ê°’ ë§¤í•‘ (kb_categories ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¡°íšŒ)
    found_allergen = kb_categories[best_match_index] 
    
    print(f"RAG ê²€ìƒ‰: '{ingredient}' (ìœ ì‚¬ë„: {confidence_score:.4f}) -> ë§¤í•‘: '{found_allergen}'")
    
    rag_result_data = {
        "confidence": confidence_score,
        "found_allergen": found_allergen
    }
    return {**state, "rag_result": rag_result_data}

def llm_fallback(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 5 (LLM Fallback ë…¸ë“œ)
    
    RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶ˆí™•ì‹¤í•  ë•Œ(ì‹ ë¢°ë„ ì„ê³„ê°’ ë¯¸ë§Œ) í˜¸ì¶œë©ë‹ˆë‹¤.
    ê¸€ë¡œë²Œ T5 ëª¨ë¸ì—ê²Œ 'ë¶„ë¥˜' ì§ˆë¬¸ì„ ë˜ì ¸ ì´ ì„±ë¶„ì´ ì–´ë–¤ í‘œì¤€ ì•Œë ˆë¥´ê¸°ì¸ì§€ êµì°¨ ê²€ì¦í•©ë‹ˆë‹¤.
    """
    print(f"--- (Node 5: llm_fallback) ---")
    ingredient = state['current_ingredient']
    print(f"LLM Fallback: T5 ëª¨ë¸ì—ê²Œ '{ingredient}' ë¶„ë¥˜ ìš”ì²­...")

    # T5 ëª¨ë¸ì„ 'ë¶„ë¥˜ê¸°'ë¡œ í™œìš©í•˜ëŠ” í”„ë¡¬í”„íŠ¸
    prompt = f"""
ë‹¤ìŒ ì„±ë¶„ì€ ì–´ë–¤ í‘œì¤€ ì•Œë ˆë¥´ê¸° ë¶„ë¥˜ì— ì†í•©ë‹ˆê¹Œ? ì„±ë¶„: "{ingredient}", ë¶„ë¥˜ ëª©ë¡: {', '.join(list(ALLERGENS_STD_SET))}. ì§€ì‹œ: ëª©ë¡ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”. ëª©ë¡ì— ì—†ìœ¼ë©´ "ì—†ìŒ"ì´ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
ì •ë‹µ: """
    
    try:
        # ê¸€ë¡œë²Œ T5 íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©
        response = t5_pipeline(prompt)[0]["generated_text"].strip()
        print(f"T5 ì‘ë‹µ: '{response}'")
        
        if response in ALLERGENS_STD_SET: # T5 ì‘ë‹µì´ í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡ì— ìˆë‹¤ë©´
            return {**state, "rag_result": {"confidence": 1.0, "found_allergen": response}}
        else: # "ì—†ìŒ" ë˜ëŠ” ê¸°íƒ€ ì“°ë ˆê¸° ê°’ì„ ë°˜í™˜í•˜ë©´ "ì—†ìŒ"ìœ¼ë¡œ í†µì¼
            return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì—†ìŒ"}}
            
    except Exception as e:
        print(f"âŒ T5 Fallback ì¤‘ ì˜¤ë¥˜: {e}")
        return {**state, "rag_result": {"confidence": 1.0, "found_allergen": "ì˜¤ë¥˜"}}

def update_final_list(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 6 (ê²°ê³¼ ì·¨í•© ë…¸ë“œ)
    
    RAG(ë…¸ë“œ 4) ë˜ëŠ” LLM(ë…¸ë“œ 5)ì˜ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë°›ì•„,
    ê·¸ ê²°ê³¼ê°€ 'í‘œì¤€ ì•Œë ˆë¥´ê¸° ëª©ë¡(ALLERGENS_STD_SET)'ì— í¬í•¨ëœ ìœ íš¨í•œ í•­ëª©ì¼ ê²½ìš°ì—ë§Œ
    'final_allergens' Setì— ì¶”ê°€(ëˆ„ì )í•©ë‹ˆë‹¤.
    """
    print(f"--- (Node 6: update_final_list) ---")
    result_allergen = state['rag_result']['found_allergen']
    
    if result_allergen in ALLERGENS_STD_SET:
        current_set = state['final_allergens']
        # 'í•¨ìœ ' ëª©ë¡(ë…¸ë“œ 2)ì—ì„œ ì´ë¯¸ ì¶”ê°€ë˜ì—ˆì„ ìˆ˜ë„ ìˆì§€ë§Œ, Setì´ë¯€ë¡œ ì¤‘ë³µì€ ìë™ ì²˜ë¦¬ë¨
        print(f"âœ… ìœ íš¨í•œ ì•Œë ˆë¥´ê¸° ë°œê²¬: '{result_allergen}'. ìµœì¢… ëª©ë¡ì— ì¶”ê°€.")
        current_set.add(result_allergen)
        return {**state, "final_allergens": current_set}
    else:
        # ê²°ê³¼ê°€ "ì—†ìŒ", "ì˜¤ë¥˜" ë˜ëŠ” KBì˜ "ê¸°íƒ€" ë“± í‘œì¤€ ëª©ë¡ì— ì—†ìœ¼ë©´ ë¬´ì‹œ
        print(f"â„¹ï¸ '{result_allergen}'ì€(ëŠ”) í‘œì¤€ ì•Œë ˆë¥´ê¸° í•­ëª©ì´ ì•„ë‹ˆë¯€ë¡œ ë¬´ì‹œí•©ë‹ˆë‹¤.")
        return state # Setì— ë³€ê²½ì´ ì—†ìœ¼ë¯€ë¡œ stateë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜

def finalize_processing(state: AllergyGraphState) -> AllergyGraphState:
    """
    âœ… ë…¸ë“œ 7 (ì¢…ë£Œ ë…¸ë“œ)
    
    ëª¨ë“  ë£¨í”„ê°€ ëë‚œ í›„ í˜¸ì¶œë©ë‹ˆë‹¤.
    ìµœì¢… ëˆ„ì ëœ 'final_allergens' Setì„ API ì‘ë‹µì— ì í•©í•œ 'ì •ë ¬ëœ JSON ë¦¬ìŠ¤íŠ¸'ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    print(f"\n--- (Node 7: finalize_processing) ---")
    final_set = state['final_allergens']
    
    # Set(ìˆœì„œ ì—†ìŒ)ì„ Listë¡œ ë³€í™˜í•˜ê³  ì•ŒíŒŒë²³ìˆœìœ¼ë¡œ ì •ë ¬
    final_list = sorted(list(final_set))
    
    # JSON ë¬¸ìì—´ë¡œ ë³€í™˜
    final_json = json.dumps(final_list, ensure_ascii=False)
    
    print(f"ğŸ‰ ëª¨ë“  ì„±ë¶„ ê²€ì‚¬ ì™„ë£Œ. ìµœì¢… ê²°ê³¼: {final_json}")
    return {**state, "final_output_json": final_json}


# --- 4. LangGraph ì—£ì§€(Edge) í•¨ìˆ˜ ì •ì˜ ---

def route_rag_result(state: AllergyGraphState) -> str:
    """(ì¡°ê±´ë¶€ ì—£ì§€ 1: RAG ë¼ìš°í„°)
    RAG ê²€ìƒ‰(ë…¸ë“œ 4)ì˜ ì‹ ë¢°ë„ë¥¼ í™•ì¸í•˜ì—¬, ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
    - [ì„±ê³µ] ì‹ ë¢°ë„ê°€ ë†’ê³  ìœ íš¨í•œ ì•Œë ˆë¥´ê¸° -> 'update_final_list'ë¡œ ë°”ë¡œ ì´ë™
    - [ì‹¤íŒ¨/ë¶ˆí™•ì‹¤] ì‹ ë¢°ë„ê°€ ë‚®ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ -> 'llm_fallback'ìœ¼ë¡œ ì´ë™
    """
    print(f"--- (Edge: route_rag_result?) ---")
    confidence = state['rag_result']['confidence']
    allergen = state['rag_result']['found_allergen']
    
    if confidence >= RAG_CONFIDENCE_THRESHOLD and allergen in ALLERGENS_STD_SET:
        print(f"  -> [RAG ì„±ê³µ]. 'update_final_list'ë¡œ ì´ë™.")
        return "rag_success"
    else:
        print(f"  -> [RAG ì‹¤íŒ¨/ë¶ˆí™•ì‹¤]. 'llm_fallback'ìœ¼ë¡œ ì´ë™.")
        return "needs_llm_fallback"

def check_remaining_ingredients(state: AllergyGraphState) -> str:
    """(ì¡°ê±´ë¶€ ì—£ì§€ 2: ë£¨í”„ ì œì–´)
    ê²°ê³¼ ì·¨í•©(ë…¸ë“œ 6) í›„, 'ingredients_to_check' íì— ê²€ì‚¬í•  í•­ëª©ì´ ë” ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    - [ë‚¨ìŒ] íì— í•­ëª©ì´ ë‚¨ì•„ìˆìŒ -> 'prepare_next_ingredient'ë¡œ ëŒì•„ê°€ ë£¨í”„ ê³„ì†
    - [ì—†ìŒ] íê°€ ë¹„ì—ˆìŒ -> 'finalize_processing'ë¡œ ì´ë™í•˜ì—¬ ê·¸ë˜í”„ ì¢…ë£Œ
    """
    print(f"--- (Edge: check_remaining_ingredients?) ---")
    
    if state["ingredients_to_check"] and len(state["ingredients_to_check"]) > 0:
        print(f"  -> [í•­ëª© ë‚¨ìŒ]. 'prepare_next_ingredient'ë¡œ ë£¨í”„.")
        return "has_more_ingredients"
    else:
        print("  -> [í•­ëª© ì—†ìŒ]. 'finalize_processing'ë¡œ ì´ë™.")
        return "all_ingredients_done"

# --- 5. ê·¸ë˜í”„ ë¹Œë“œ ë° ì»´íŒŒì¼ ---

print("\n--- LangGraph ì›Œí¬í”Œë¡œìš° ë¹Œë“œ ì‹œì‘ ---")

workflow = StateGraph(AllergyGraphState)

# 1. ëª¨ë“  ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€
workflow.add_node("call_gcp_vision_api", call_gcp_vision_api)
workflow.add_node("parse_text_from_raw", parse_text_from_raw) # <--- ìµœì¢… ìˆ˜ì •ëœ í•¨ìˆ˜ê°€ ë“±ë¡ë¨
workflow.add_node("prepare_next_ingredient", prepare_next_ingredient)
workflow.add_node("rag_search", rag_search)
workflow.add_node("llm_fallback", llm_fallback)
workflow.add_node("update_final_list", update_final_list)
workflow.add_node("finalize_processing", finalize_processing)

# 2. ì—£ì§€ ì—°ê²° (íë¦„ ì •ì˜)
workflow.set_entry_point("call_gcp_vision_api")                # ì‹œì‘: GCP í˜¸ì¶œ
workflow.add_edge("call_gcp_vision_api", "parse_text_from_raw")    # GCP -> íŒŒì‹± (ìˆ˜ì •ëœ ë…¸ë“œ)
workflow.add_edge("parse_text_from_raw", "prepare_next_ingredient") # íŒŒì‹± -> ë£¨í”„ ì‹œì‘(ì²« ì„±ë¶„ ì¤€ë¹„)
workflow.add_edge("prepare_next_ingredient", "rag_search")          # ì„±ë¶„ ì¤€ë¹„ -> RAG ê²€ìƒ‰

# 3. RAG ë¼ìš°íŒ… (ì¡°ê±´ë¶€ ì—£ì§€ 1)
workflow.add_conditional_edges(
    "rag_search",
    route_rag_result,
    {"rag_success": "update_final_list", "needs_llm_fallback": "llm_fallback"}
)

# 4. Fallback ê²°ê³¼ë„ ì·¨í•© ë…¸ë“œë¡œ ì—°ê²°
workflow.add_edge("llm_fallback", "update_final_list") 

# 5. ë©”ì¸ ë£¨í”„ (ì¡°ê±´ë¶€ ì—£ì§€ 2)
workflow.add_conditional_edges(
    "update_final_list",
    check_remaining_ingredients,
    {"has_more_ingredients": "prepare_next_ingredient", "all_ingredients_done": "finalize_processing"}
)

# 6. ì¢…ë£Œ ë…¸ë“œ ì—°ê²°
workflow.add_edge("finalize_processing", END)

# 7. ì»´íŒŒì¼
app = workflow.compile()
print("--- âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼ ì™„ë£Œ ---")


# --- 6. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ---
print("\n\n--- [Test Run: GCP API + Regex íŒŒì„œ + ë¡œì»¬ ìºì‹œ ê¸°ë°˜ ì‹¤í–‰] ---")

# (í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ íŒŒì¼ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤)
my_test_image_file = "image.jpg" # ğŸ‘ˆ 'image.jpg'ëŠ” OCR ë¡œê·¸ë¥¼ ì œê³µí•œ ê·¸ ì´ë¯¸ì§€ íŒŒì¼ ê°€ì •

if my_test_image_file:
    test_input = {"image_path": my_test_image_file}
    print(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘: {my_test_image_file}\n")

    # (ì£¼ì„ í•´ì œí•˜ì—¬ ìŠ¤íŠ¸ë¦¼ ë¡œê·¸ ë³´ê¸°)
    # for step in app.stream(test_input, {"recursion_limit": 50}): 
    #     print(step)

    print("\n--- [Test Run: ìµœì¢… ê²°ê³¼ (invoke)] ---")
    # .invoke()ëŠ” ëª¨ë“  ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ê³  ìµœì¢… ìƒíƒœ(State)ë§Œ ë°˜í™˜í•¨
    # íì˜ ì•„ì´í…œ ê°œìˆ˜ê°€ 30ê°œë¥¼ ì´ˆê³¼í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¬ê·€ ì œí•œì„ ë„‰ë„‰í•˜ê²Œ 100ìœ¼ë¡œ ì„¤ì •
    final_state = app.invoke(test_input, {"recursion_limit": 100}) 
    print("\nìµœì¢… ë°˜í™˜ JSON:")
    print(final_state['final_output_json'])

else:
    print("\ní…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê±´ë„ˆëœ€: 'my_test_image_file' ë³€ìˆ˜ì— ì´ë¯¸ì§€ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")